{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "single-algorithm-introductory-example.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNfnBAP+9N3OJr+i+X5YuFB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielroth12/Multilevel-Monte-Carlo-learning/blob/main/single_algorithm_introductory_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BtTim4HwVYa",
        "outputId": "ce07207c-4b82-4bb0-f8e8-45a264b0619b"
      },
      "source": [
        "#Single-level algorithm using the Milstein scheme\n",
        "#For more detailed explanations of the training and model parameters\n",
        "#see Gerstner et al. \"Multilevel Monte Carlo learning.\" arXiv preprint arXiv:2102.08734 (2021).\n",
        "\n",
        "#Packages\n",
        "%tensorflow_version 1.x\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt \n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import time\n",
        "from tensorflow.python.ops import init_ops\n",
        "from tensorflow.contrib.layers.python.layers import initializers\n",
        "from tensorflow.python.training.moving_averages import assign_moving_average\n",
        "\n",
        "#Basic network framework according to Beck, Christian, et al. \"Solving the Kolmogorov PDE by means of deep learning.\" Journal of Scientific Computing 88.3 (2021): 1-28.\n",
        "def neural_net(x, xi_approx, neurons, is_training, name,\n",
        "                    mv_decay=0.9, dtype=tf.float32):\n",
        "  \n",
        "  def approx_test(): return xi_approx\n",
        "  def approx_learn(): return x\n",
        "  x = tf.cond(is_training, approx_learn,approx_test)\n",
        "\n",
        "  def _batch_normalization(_x):\n",
        "    beta = tf.get_variable('beta', [_x.get_shape()[-1]],\n",
        "                           dtype, init_ops.zeros_initializer())\n",
        "    gamma = tf.get_variable('gamma', [_x.get_shape()[-1]],\n",
        "                            dtype, init_ops.ones_initializer())\n",
        "    mv_mean = tf.get_variable('mv_mean', [_x.get_shape()[-1]],\n",
        "                              dtype, init_ops.zeros_initializer(),\n",
        "                              trainable=False)\n",
        "    mv_variance = tf.get_variable('mv_variance', [_x.get_shape()[-1]],\n",
        "                                  dtype, init_ops.ones_initializer(),\n",
        "                                  trainable=False)\n",
        "    mean, variance = tf.nn.moments(_x, [0], name='moments')\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_mean, mean,\n",
        "                                               mv_decay, True))\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_variance, variance,\n",
        "                                               mv_decay, False))\n",
        "    mean, variance = tf.cond(is_training,\n",
        "                             lambda: (mean, variance),\n",
        "                             lambda: (mv_mean, mv_variance))\n",
        "    return tf.nn.batch_normalization(_x, mean, variance,\n",
        "                                     beta, gamma, 1e-6)\n",
        "  \n",
        "  def _layer(_x, out_size, activation_fn):\n",
        "    w = tf.get_variable('weights',\n",
        "                        [_x.get_shape().as_list()[-1], out_size],\n",
        "                        dtype, initializers.xavier_initializer())\n",
        "    return activation_fn(_batch_normalization(tf.matmul(_x, w)))\n",
        "  \n",
        "  with tf.variable_scope(name):\n",
        "      x = _batch_normalization(x)\n",
        "      for i in range(len(neurons)):\n",
        "        with tf.variable_scope('layer_%i_' % (i + 1)):\n",
        "          x = _layer(x, neurons[i],\n",
        "            tf.nn.tanh if i < len(neurons)-1 else tf.identity)\n",
        "  return x\n",
        "\n",
        "#Basic network framework according to Beck, Christian, et al. \"Solving the Kolmogorov PDE by means of deep learning.\" Journal of Scientific Computing 88.3 (2021): 1-28.\n",
        "#Minor adjustments to file output and in lines 108-115 changed to exponential decay\n",
        "\n",
        "def train_and_test(xi, xi_approx, x_sde, phi, u_reference, neurons,\n",
        "          train_steps,mc_rounds, mc_freq, file_name,\n",
        "          dtype=tf.float32):\n",
        "  \n",
        "  def _approximate_errors():\n",
        "    lr, gs = sess.run([learning_rate, global_step])\n",
        "    l1_err, l2_err, li_err = 0., 0., 0.\n",
        "    rel_l1_err, rel_l2_err, rel_li_err = 0., 0., 0.\n",
        "    for _ in range(mc_rounds):\n",
        "      plot_xi, plot_approx, plot_ref, l1, l2, li, rl1, rl2, rli, appr, ref \\\n",
        "              = sess.run([xi_approx, u_approx,u_reference,err_l_1, err_l_2, err_l_inf,\n",
        "                          rel_err_l_1, rel_err_l_2, rel_err_l_inf, approx, reference],\n",
        "                         feed_dict={is_training: False})\n",
        "      l1_err, l2_err, li_err = (l1_err + l1, l2_err + l2,\n",
        "                                  np.maximum(li_err, li))\n",
        "      rel_l1_err, rel_l2_err, rel_li_err \\\n",
        "              = (rel_l1_err + rl1, rel_l2_err + rl2,\n",
        "                 np.maximum(rel_li_err, rli))\n",
        "    l1_err, l2_err = l1_err / mc_rounds, np.sqrt(l2_err / mc_rounds)\n",
        "    rel_l1_err, rel_l2_err \\\n",
        "              = rel_l1_err / mc_rounds, np.sqrt(rel_l2_err / mc_rounds)\n",
        "    t_mc = time.time()\n",
        "\n",
        "    file_out.write('%i, %f, %f, %f, %f \\n' % (gs, li_err, lr, t1_train - t0_train, t_mc - t1_train)) \n",
        "    file_out.flush()\n",
        "    \n",
        "\n",
        "  t0_train = time.time()\n",
        "  is_training = tf.placeholder(tf.bool, [])\n",
        "  u_approx = neural_net(xi, xi_approx, neurons, is_training, 'u_approx', dtype=dtype)  \n",
        "  loss = tf.reduce_mean(tf.squared_difference(u_approx, phi))\n",
        "  \n",
        "  approx=tf.reduce_mean(u_approx)\n",
        "  reference=tf.reduce_mean(u_reference)\n",
        "  err = tf.abs(u_approx - u_reference)\n",
        "  err_l_1 = tf.reduce_mean(err)\n",
        "  err_l_2 = tf.reduce_mean(err ** 2)\n",
        "  err_l_inf = tf.reduce_max(err)\n",
        "  rel_err = err / tf.maximum(u_reference, 1e-4)\n",
        "  rel_err_l_1 = tf.reduce_mean(rel_err)\n",
        "  rel_err_l_2 = tf.reduce_mean(rel_err ** 2)\n",
        "  rel_err_l_inf = tf.reduce_max(rel_err)\n",
        "\n",
        "  lr=0.01\n",
        "  step_rate = 40000\n",
        "  decay = 0.1\n",
        "\n",
        "  global_step = tf.Variable(0, trainable=False)\n",
        "  increment_global_step = tf.assign(global_step, global_step + 1)\n",
        "  learning_rate = tf.train.exponential_decay(lr, global_step, step_rate, decay, staircase=True)\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=0.01)\n",
        "\n",
        "\n",
        "  update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, 'u_approx')\n",
        "  with tf.control_dependencies(update_ops):\n",
        "    train_op = optimizer.minimize(loss, global_step)\n",
        "    \n",
        "        \n",
        "  file_out = open(file_name, 'w')\n",
        "  file_out.write('step, li_err, learning_rate, time_train, time_test \\n ')\n",
        "    \n",
        "  with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "        \n",
        "    for step in range(train_steps):\n",
        "      if step % mc_freq == 0:\n",
        "        print(step)\n",
        "        t1_train = time.time()\n",
        "        _approximate_errors()\n",
        "        t0_train = time.time()\n",
        "      sess.run(train_op, feed_dict={is_training:True})\n",
        "    t1_train = time.time()\n",
        "    _approximate_errors()\n",
        "        \n",
        "  file_out.close()\n",
        "  \n",
        "#Model and training parameter specification\n",
        "for i in range(1,2):\n",
        "  tf.reset_default_graph()\n",
        "  tf.random.set_random_seed(i)\n",
        "  with tf.Session()  as sess:\n",
        "    dtype = tf.float32\n",
        "\n",
        "    #Set network and training parameter\n",
        "    batch_size =125000\n",
        "    batch_size_approx=batch_size\n",
        "    N, d = 128, 5\n",
        "    neurons = [50, 50, 1]\n",
        "    train_steps = 150000\n",
        "    mc_rounds, mc_freq = 100, 5000\n",
        "    mc_samples_ref, mc_rounds_ref = 1, 1\n",
        "    N_l=1\n",
        "\n",
        "    #Define training and test interval\n",
        "    s_0_l=80.0\n",
        "    s_0_r=120.0\n",
        "    sigma_l=0.1\n",
        "    sigma_r=0.2\n",
        "    mu_l=0.02\n",
        "    mu_r=0.05\n",
        "    T_l=0.9\n",
        "    T_r=1.0\n",
        "    K_l=109.0\n",
        "    K_r=110.0\n",
        "    s_0_l_approx=80.4\n",
        "    s_0_r_approx=119.6\n",
        "    sigma_l_approx=0.11\n",
        "    sigma_r_approx=0.19\n",
        "    mu_l_approx=0.03\n",
        "    mu_r_approx=0.04\n",
        "    T_l_approx=0.91\n",
        "    T_r_approx=0.99\n",
        "    K_l_approx=109.1\n",
        "    K_r_approx=109.9\n",
        "    s0 = tf.random_uniform((batch_size,1), minval=s_0_l,\n",
        "                                 maxval=s_0_r, dtype=dtype)\n",
        "    sigma=tf.random_uniform((batch_size,1),\n",
        "                            minval=sigma_l,maxval=sigma_r, dtype=dtype)\n",
        "    mu=tf.random_uniform((batch_size,1),\n",
        "                            minval=mu_l,maxval=mu_r, dtype=dtype)\n",
        "    T=tf.random_uniform((batch_size,1),\n",
        "                            minval=T_l,maxval=T_r, dtype=dtype)\n",
        "    K=tf.random_uniform((batch_size,1),\n",
        "                      minval=K_l,maxval=K_r, dtype=dtype)\n",
        "    s0_approx = tf.random_uniform((batch_size_approx,  1 ), \n",
        "                            minval=s_0_l_approx,maxval=s_0_r_approx, dtype=dtype)\n",
        "    sigma_approx=tf.random_uniform((batch_size_approx,  1 ), \n",
        "                            minval=sigma_l_approx,maxval=sigma_r_approx, dtype=dtype)\n",
        "    mu_approx=tf.random_uniform((batch_size_approx,1),\n",
        "                            minval=mu_l_approx,maxval=mu_r_approx, dtype=dtype)\n",
        "    T_approx=tf.random_uniform((batch_size_approx,1),\n",
        "                            minval=T_l_approx,maxval=T_r_approx, dtype=dtype)\n",
        "    K_approx=tf.random_uniform((batch_size_approx,1),\n",
        "                            minval=K_l_approx,maxval=K_r_approx, dtype=dtype)\n",
        "    \n",
        "    xi=tf.reshape(tf.stack([s0,sigma,mu,T,K], axis=2), (batch_size,d))\n",
        "    xi_approx=tf.reshape(tf.stack([s0_approx,sigma_approx,mu_approx,T_approx,K_approx], axis=2), (batch_size_approx,d))\n",
        "    \n",
        "    #Closed solution as reference\n",
        "    tfd = tfp.distributions\n",
        "    dist = tfd.Normal(loc=tf.cast(0.,tf.float32), scale=tf.cast(1.,tf.float32))\n",
        "    d1=tf.math.divide(\n",
        "    (tf.log(tf.math.divide(s0_approx,K_approx))+(mu_approx + 0.5*sigma_approx**2)*T_approx) , (sigma_approx*tf.sqrt(T_approx)))\n",
        "    d2=tf.math.divide(\n",
        "    (tf.log(tf.math.divide(s0_approx,K_approx))+(mu_approx - 0.5*sigma_approx**2)*T_approx) , (sigma_approx*tf.sqrt(T_approx)))\n",
        "    u_reference= tf.multiply(s0_approx,(dist.cdf(d1)))-K_approx*tf.exp(-mu_approx*T_approx)*(dist.cdf(d2))\n",
        "\n",
        "  #European option\n",
        "  def phi(x,sigma,mu,T,K, axis=1):\n",
        "    payoffcoarse=tf.exp(-mu * T)* tf.maximum(x - K, 0.)\n",
        "    return payoffcoarse\n",
        "  #Milstein scheme\n",
        "  def sde_body(idx, s, sigma,mu,T,K, samples): \n",
        "    h=T/N\n",
        "    z=tf.random_normal(shape=(samples, batch_size,1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    s=s + mu *s * h +sigma * s *tf.sqrt(h)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(h)*z)**2-h)    \n",
        "    return tf.add(idx, 1), s, sigma,mu,T,K\n",
        "  #Monte Carlo loop                 \n",
        "  def mc_body(idx, p):\n",
        "    _, _x, _sigma,_mu,_T,_K = tf.while_loop(lambda _idx, s, sigma,mu,T,K: _idx < N,\n",
        "                          lambda _idx, s, sigma,mu,T,K: sde_body(_idx, s, sigma,mu,T,K,\n",
        "                                                   mc_samples_ref),\n",
        "                                                   loop_var_mc)\n",
        "    return idx + 1, p + tf.reduce_mean(phi(_x,_sigma,_mu,_T,_K, 2), axis=0)\n",
        "\n",
        "  loop_var_mc = (tf.constant(0),tf.ones((mc_samples_ref,batch_size, 1), dtype) * s0, tf.ones((mc_samples_ref,batch_size, 1), dtype) * sigma,tf.ones((mc_samples_ref,batch_size, 1), dtype) * mu,tf.ones((mc_samples_ref,batch_size, 1), dtype) * T,tf.ones((mc_samples_ref,batch_size, 1), dtype) * K)\n",
        "  _, u = tf.while_loop(lambda idx, p: idx < N_l, mc_body,(tf.constant(0), tf.zeros((batch_size, 1), dtype)))\n",
        "  u_mc_test = u / tf.cast(N_l, tf.float32)\n",
        "\n",
        "  #Start training and testing                       \n",
        "  train_and_test(xi, xi_approx, xi, u_mc_test, u_reference,neurons, train_steps,\n",
        "                          mc_rounds, mc_freq, 'single-introductory.csv', dtype)   \n",
        "\n",
        "                        \n",
        "                        \n",
        "                       "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "5000\n",
            "10000\n",
            "15000\n",
            "20000\n",
            "25000\n",
            "30000\n",
            "35000\n",
            "40000\n",
            "45000\n",
            "50000\n",
            "55000\n",
            "60000\n",
            "65000\n",
            "70000\n",
            "75000\n",
            "80000\n",
            "85000\n",
            "90000\n",
            "95000\n",
            "100000\n",
            "105000\n",
            "110000\n",
            "115000\n",
            "120000\n",
            "125000\n",
            "130000\n",
            "135000\n",
            "140000\n",
            "145000\n"
          ]
        }
      ]
    }
  ]
}