{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "github-multi.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNk4b1OYZQcyYra1otS5tGH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielroth12/Multilevel-Monte-Carlo-learning/blob/main/github_multi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2RhdjiTtuwl"
      },
      "source": [
        "\n",
        "%tensorflow_version 1.x\n",
        "#@title\n",
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once in a notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "## Authenticate and create the PyDrive client.\n",
        "## This only needs to be done once in a notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        "\n",
        "#Netz nach kolmogorov\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import time\n",
        "from tensorflow.python.ops import init_ops\n",
        "from tensorflow.contrib.layers.python.layers import initializers\n",
        "from tensorflow.python.training.moving_averages import assign_moving_average\n",
        "def neural_net(x, xi_approx, neurons, is_training, name,\n",
        "                    mv_decay=0.9, dtype=tf.float32):\n",
        "  \n",
        "  \n",
        "  def approx_test(): return xi_approx\n",
        "  def approx_learn(): return x\n",
        "  x = tf.cond(is_training, approx_learn,approx_test)\n",
        "\n",
        "  def _batch_normalization(_x):\n",
        "    beta = tf.get_variable('beta', [_x.get_shape()[-1]],\n",
        "                           dtype, init_ops.zeros_initializer())\n",
        "    gamma = tf.get_variable('gamma', [_x.get_shape()[-1]],\n",
        "                            dtype, init_ops.ones_initializer())\n",
        "    mv_mean = tf.get_variable('mv_mean', [_x.get_shape()[-1]],\n",
        "                              dtype, init_ops.zeros_initializer(),\n",
        "                              trainable=False)\n",
        "    mv_variance = tf.get_variable('mv_variance', [_x.get_shape()[-1]],\n",
        "                                  dtype, init_ops.ones_initializer(),\n",
        "                                  trainable=False)\n",
        "    mean, variance = tf.nn.moments(_x, [0], name='moments')\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_mean, mean,\n",
        "                                               mv_decay, True))\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_variance, variance,\n",
        "                                               mv_decay, False))\n",
        "    mean, variance = tf.cond(is_training,\n",
        "                             lambda: (mean, variance),\n",
        "                             lambda: (mv_mean, mv_variance))\n",
        "    return tf.nn.batch_normalization(_x, mean, variance,\n",
        "                                     beta, gamma, 1e-6)\n",
        "  \n",
        "  def _layer(_x, out_size, activation_fn):\n",
        "    w = tf.get_variable('weights',\n",
        "                        [_x.get_shape().as_list()[-1], out_size],\n",
        "                        dtype, initializers.xavier_initializer())\n",
        "    return activation_fn(_batch_normalization(tf.matmul(_x, w)))\n",
        "  \n",
        "  with tf.variable_scope(name):\n",
        "      x = _batch_normalization(x)\n",
        "      #xnormal=x\n",
        "      for i in range(len(neurons)):\n",
        "        with tf.variable_scope('layer_%i_' % (i + 1)):\n",
        "          x = _layer(x, neurons[i],\n",
        "            tf.nn.tanh if i < len(neurons)-1 else tf.identity)\n",
        "  return x\n",
        "\n",
        "def neural_net_p1_p0(x, xi_approx, neurons, is_training, name,\n",
        "                    mv_decay=0.9, dtype=tf.float32):\n",
        "  \n",
        "  \n",
        "  def approx_test(): return xi_approx\n",
        "  def approx_learn(): return x\n",
        "  x = tf.cond(is_training, approx_learn,approx_test)\n",
        "\n",
        "  def _batch_normalization(_x):\n",
        "    beta = tf.get_variable('beta2', [_x.get_shape()[-1]],\n",
        "                           dtype, init_ops.zeros_initializer())\n",
        "    gamma = tf.get_variable('gamma2', [_x.get_shape()[-1]],\n",
        "                            dtype, init_ops.ones_initializer())\n",
        "    mv_mean = tf.get_variable('mv_mean2', [_x.get_shape()[-1]],\n",
        "                              dtype, init_ops.zeros_initializer(),\n",
        "                              trainable=False)\n",
        "    mv_variance = tf.get_variable('mv_variance2', [_x.get_shape()[-1]],\n",
        "                                  dtype, init_ops.ones_initializer(),\n",
        "                                  trainable=False)\n",
        "    mean, variance = tf.nn.moments(_x, [0], name='moments2')\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_mean, mean,\n",
        "                                               mv_decay, True))\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_variance, variance,\n",
        "                                               mv_decay, False))\n",
        "    mean, variance = tf.cond(is_training,\n",
        "                             lambda: (mean, variance),\n",
        "                             lambda: (mv_mean, mv_variance))\n",
        "    return tf.nn.batch_normalization(_x, mean, variance,\n",
        "                                     beta, gamma, 1e-6)\n",
        "  \n",
        "  def _layer(_x, out_size, activation_fn):\n",
        "    w = tf.get_variable('weights2',\n",
        "                        [_x.get_shape().as_list()[-1], out_size],\n",
        "                        dtype, initializers.xavier_initializer())\n",
        "    return activation_fn(_batch_normalization(tf.matmul(_x, w)))\n",
        "  \n",
        "  with tf.variable_scope(name):\n",
        "      x = _batch_normalization(x)\n",
        "      #xnormal=x\n",
        "      for i in range(len(neurons)):\n",
        "        with tf.variable_scope('layer2_%i_' % (i + 1)):\n",
        "          x = _layer(x, neurons[i],\n",
        "            tf.nn.tanh if i < len(neurons)-1 else tf.identity)\n",
        "  return x  \n",
        "\n",
        "def neural_net_p2_p1(x, xi_approx, neurons, is_training, name,\n",
        "                    mv_decay=0.9, dtype=tf.float32):\n",
        "  \n",
        "  \n",
        "  def approx_test(): return xi_approx\n",
        "  def approx_learn(): return x\n",
        "  x = tf.cond(is_training, approx_learn,approx_test)\n",
        "\n",
        "  def _batch_normalization(_x):\n",
        "    beta = tf.get_variable('beta3', [_x.get_shape()[-1]],\n",
        "                           dtype, init_ops.zeros_initializer())\n",
        "    gamma = tf.get_variable('gamma3', [_x.get_shape()[-1]],\n",
        "                            dtype, init_ops.ones_initializer())\n",
        "    mv_mean = tf.get_variable('mv_mean3', [_x.get_shape()[-1]],\n",
        "                              dtype, init_ops.zeros_initializer(),\n",
        "                              trainable=False)\n",
        "    mv_variance = tf.get_variable('mv_variance3', [_x.get_shape()[-1]],\n",
        "                                  dtype, init_ops.ones_initializer(),\n",
        "                                  trainable=False)\n",
        "    mean, variance = tf.nn.moments(_x, [0], name='moments3')\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_mean, mean,\n",
        "                                               mv_decay, True))\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_variance, variance,\n",
        "                                               mv_decay, False))\n",
        "    mean, variance = tf.cond(is_training,\n",
        "                             lambda: (mean, variance),\n",
        "                             lambda: (mv_mean, mv_variance))\n",
        "    return tf.nn.batch_normalization(_x, mean, variance,\n",
        "                                     beta, gamma, 1e-6)\n",
        "  \n",
        "  def _layer(_x, out_size, activation_fn):\n",
        "    w = tf.get_variable('weights3',\n",
        "                        [_x.get_shape().as_list()[-1], out_size],\n",
        "                        dtype, initializers.xavier_initializer())\n",
        "    return activation_fn(_batch_normalization(tf.matmul(_x, w)))\n",
        "  \n",
        "  with tf.variable_scope(name):\n",
        "      x = _batch_normalization(x)\n",
        "      #xnormal=x\n",
        "      for i in range(len(neurons)):\n",
        "        with tf.variable_scope('layer3_%i_' % (i + 1)):\n",
        "          x = _layer(x, neurons[i],\n",
        "            tf.nn.tanh if i < len(neurons)-1 else tf.identity)\n",
        "  return x   \n",
        "\n",
        "def neural_net_p3_p2(x, xi_approx, neurons, is_training, name,\n",
        "                    mv_decay=0.9, dtype=tf.float32):\n",
        "  \n",
        "  \n",
        "  def approx_test(): return xi_approx\n",
        "  def approx_learn(): return x\n",
        "  x = tf.cond(is_training, approx_learn,approx_test)\n",
        "\n",
        "  def _batch_normalization(_x):\n",
        "    beta = tf.get_variable('beta4', [_x.get_shape()[-1]],\n",
        "                           dtype, init_ops.zeros_initializer())\n",
        "    gamma = tf.get_variable('gamma4', [_x.get_shape()[-1]],\n",
        "                            dtype, init_ops.ones_initializer())\n",
        "    mv_mean = tf.get_variable('mv_mean4', [_x.get_shape()[-1]],\n",
        "                              dtype, init_ops.zeros_initializer(),\n",
        "                              trainable=False)\n",
        "    mv_variance = tf.get_variable('mv_variance4', [_x.get_shape()[-1]],\n",
        "                                  dtype, init_ops.ones_initializer(),\n",
        "                                  trainable=False)\n",
        "    mean, variance = tf.nn.moments(_x, [0], name='moments4')\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_mean, mean,\n",
        "                                               mv_decay, True))\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_variance, variance,\n",
        "                                               mv_decay, False))\n",
        "    mean, variance = tf.cond(is_training,\n",
        "                             lambda: (mean, variance),\n",
        "                             lambda: (mv_mean, mv_variance))\n",
        "    return tf.nn.batch_normalization(_x, mean, variance,\n",
        "                                     beta, gamma, 1e-6)\n",
        "  \n",
        "  def _layer(_x, out_size, activation_fn):\n",
        "    w = tf.get_variable('weights4',\n",
        "                        [_x.get_shape().as_list()[-1], out_size],\n",
        "                        dtype, initializers.xavier_initializer())\n",
        "    return activation_fn(_batch_normalization(tf.matmul(_x, w)))\n",
        "  \n",
        "  with tf.variable_scope(name):\n",
        "      x = _batch_normalization(x)\n",
        "      #xnormal=x\n",
        "      for i in range(len(neurons)):\n",
        "        with tf.variable_scope('layer4_%i_' % (i + 1)):\n",
        "          x = _layer(x, neurons[i],\n",
        "            tf.nn.tanh if i < len(neurons)-1 else tf.identity)\n",
        "  return x     \n",
        "\n",
        "def neural_net_p4_p3(x, xi_approx, neurons, is_training, name,\n",
        "                    mv_decay=0.9, dtype=tf.float32):\n",
        "  \n",
        "  \n",
        "  def approx_test(): return xi_approx\n",
        "  def approx_learn(): return x\n",
        "  x = tf.cond(is_training, approx_learn,approx_test)\n",
        "\n",
        "  def _batch_normalization(_x):\n",
        "    beta = tf.get_variable('beta5', [_x.get_shape()[-1]],\n",
        "                           dtype, init_ops.zeros_initializer())\n",
        "    gamma = tf.get_variable('gamma5', [_x.get_shape()[-1]],\n",
        "                            dtype, init_ops.ones_initializer())\n",
        "    mv_mean = tf.get_variable('mv_mean5', [_x.get_shape()[-1]],\n",
        "                              dtype, init_ops.zeros_initializer(),\n",
        "                              trainable=False)\n",
        "    mv_variance = tf.get_variable('mv_variance5', [_x.get_shape()[-1]],\n",
        "                                  dtype, init_ops.ones_initializer(),\n",
        "                                  trainable=False)\n",
        "    mean, variance = tf.nn.moments(_x, [0], name='moments5')\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_mean, mean,\n",
        "                                               mv_decay, True))\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_variance, variance,\n",
        "                                               mv_decay, False))\n",
        "    mean, variance = tf.cond(is_training,\n",
        "                             lambda: (mean, variance),\n",
        "                             lambda: (mv_mean, mv_variance))\n",
        "    return tf.nn.batch_normalization(_x, mean, variance,\n",
        "                                     beta, gamma, 1e-6)\n",
        "  \n",
        "  def _layer(_x, out_size, activation_fn):\n",
        "    w = tf.get_variable('weights5',\n",
        "                        [_x.get_shape().as_list()[-1], out_size],\n",
        "                        dtype, initializers.xavier_initializer())\n",
        "    return activation_fn(_batch_normalization(tf.matmul(_x, w)))\n",
        "  \n",
        "  with tf.variable_scope(name):\n",
        "      x = _batch_normalization(x)\n",
        "      #xnormal=x\n",
        "      for i in range(len(neurons)):\n",
        "        with tf.variable_scope('layer5_%i_' % (i + 1)):\n",
        "          x = _layer(x, neurons[i],\n",
        "            tf.nn.tanh if i < len(neurons)-1 else tf.identity)\n",
        "  return x       \n",
        "\n",
        "def neural_net_p5_p4(x, xi_approx, neurons, is_training, name,\n",
        "                    mv_decay=0.9, dtype=tf.float32):\n",
        "  \n",
        "  \n",
        "  def approx_test(): return xi_approx\n",
        "  def approx_learn(): return x\n",
        "  x = tf.cond(is_training, approx_learn,approx_test)\n",
        "\n",
        "  def _batch_normalization(_x):\n",
        "    beta = tf.get_variable('beta6', [_x.get_shape()[-1]],\n",
        "                           dtype, init_ops.zeros_initializer())\n",
        "    gamma = tf.get_variable('gamma6', [_x.get_shape()[-1]],\n",
        "                            dtype, init_ops.ones_initializer())\n",
        "    mv_mean = tf.get_variable('mv_mean6', [_x.get_shape()[-1]],\n",
        "                              dtype, init_ops.zeros_initializer(),\n",
        "                              trainable=False)\n",
        "    mv_variance = tf.get_variable('mv_variance6', [_x.get_shape()[-1]],\n",
        "                                  dtype, init_ops.ones_initializer(),\n",
        "                                  trainable=False)\n",
        "    mean, variance = tf.nn.moments(_x, [0], name='moments6')\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_mean, mean,\n",
        "                                               mv_decay, True))\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_variance, variance,\n",
        "                                               mv_decay, False))\n",
        "    mean, variance = tf.cond(is_training,\n",
        "                             lambda: (mean, variance),\n",
        "                             lambda: (mv_mean, mv_variance))\n",
        "    return tf.nn.batch_normalization(_x, mean, variance,\n",
        "                                     beta, gamma, 1e-6)\n",
        "  \n",
        "  def _layer(_x, out_size, activation_fn):\n",
        "    w = tf.get_variable('weights6',\n",
        "                        [_x.get_shape().as_list()[-1], out_size],\n",
        "                        dtype, initializers.xavier_initializer())\n",
        "    return activation_fn(_batch_normalization(tf.matmul(_x, w)))\n",
        "  \n",
        "  with tf.variable_scope(name):\n",
        "      x = _batch_normalization(x)\n",
        "      #xnormal=x\n",
        "      for i in range(len(neurons)):\n",
        "        with tf.variable_scope('layer6_%i_' % (i + 1)):\n",
        "          x = _layer(x, neurons[i],\n",
        "            tf.nn.tanh if i < len(neurons)-1 else tf.identity)\n",
        "  return x    \n",
        "\n",
        "def neural_net_p6_p5(x, xi_approx, neurons, is_training, name,\n",
        "                    mv_decay=0.9, dtype=tf.float32):\n",
        "  \n",
        "  \n",
        "  def approx_test(): return xi_approx\n",
        "  def approx_learn(): return x\n",
        "  x = tf.cond(is_training, approx_learn,approx_test)\n",
        "\n",
        "  def _batch_normalization(_x):\n",
        "    beta = tf.get_variable('beta7', [_x.get_shape()[-1]],\n",
        "                           dtype, init_ops.zeros_initializer())\n",
        "    gamma = tf.get_variable('gamma7', [_x.get_shape()[-1]],\n",
        "                            dtype, init_ops.ones_initializer())\n",
        "    mv_mean = tf.get_variable('mv_mean7', [_x.get_shape()[-1]],\n",
        "                              dtype, init_ops.zeros_initializer(),\n",
        "                              trainable=False)\n",
        "    mv_variance = tf.get_variable('mv_variance7', [_x.get_shape()[-1]],\n",
        "                                  dtype, init_ops.ones_initializer(),\n",
        "                                  trainable=False)\n",
        "    mean, variance = tf.nn.moments(_x, [0], name='moments7')\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_mean, mean,\n",
        "                                               mv_decay, True))\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_variance, variance,\n",
        "                                               mv_decay, False))\n",
        "    mean, variance = tf.cond(is_training,\n",
        "                             lambda: (mean, variance),\n",
        "                             lambda: (mv_mean, mv_variance))\n",
        "    return tf.nn.batch_normalization(_x, mean, variance,\n",
        "                                     beta, gamma, 1e-6)\n",
        "  \n",
        "  def _layer(_x, out_size, activation_fn):\n",
        "    w = tf.get_variable('weights7',\n",
        "                        [_x.get_shape().as_list()[-1], out_size],\n",
        "                        dtype, initializers.xavier_initializer())\n",
        "    return activation_fn(_batch_normalization(tf.matmul(_x, w)))\n",
        "  \n",
        "  with tf.variable_scope(name):\n",
        "      x = _batch_normalization(x)\n",
        "      #xnormal=x\n",
        "      for i in range(len(neurons)):\n",
        "        with tf.variable_scope('layer7_%i_' % (i + 1)):\n",
        "          x = _layer(x, neurons[i],\n",
        "            tf.nn.tanh if i < len(neurons)-1 else tf.identity)\n",
        "  return x         \n",
        "\n",
        "def neural_net_p7_p6(x, xi_approx, neurons, is_training, name,\n",
        "                    mv_decay=0.9, dtype=tf.float32):\n",
        "  \n",
        "  \n",
        "  def approx_test(): return xi_approx\n",
        "  def approx_learn(): return x\n",
        "  x = tf.cond(is_training, approx_learn,approx_test)\n",
        "\n",
        "  def _batch_normalization(_x):\n",
        "    beta = tf.get_variable('beta8', [_x.get_shape()[-1]],\n",
        "                           dtype, init_ops.zeros_initializer())\n",
        "    gamma = tf.get_variable('gamma8', [_x.get_shape()[-1]],\n",
        "                            dtype, init_ops.ones_initializer())\n",
        "    mv_mean = tf.get_variable('mv_mean8', [_x.get_shape()[-1]],\n",
        "                              dtype, init_ops.zeros_initializer(),\n",
        "                              trainable=False)\n",
        "    mv_variance = tf.get_variable('mv_variance8', [_x.get_shape()[-1]],\n",
        "                                  dtype, init_ops.ones_initializer(),\n",
        "                                  trainable=False)\n",
        "    mean, variance = tf.nn.moments(_x, [0], name='moments8')\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_mean, mean,\n",
        "                                               mv_decay, True))\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_variance, variance,\n",
        "                                               mv_decay, False))\n",
        "    mean, variance = tf.cond(is_training,\n",
        "                             lambda: (mean, variance),\n",
        "                             lambda: (mv_mean, mv_variance))\n",
        "    return tf.nn.batch_normalization(_x, mean, variance,\n",
        "                                     beta, gamma, 1e-6)\n",
        "  \n",
        "  def _layer(_x, out_size, activation_fn):\n",
        "    w = tf.get_variable('weights8',\n",
        "                        [_x.get_shape().as_list()[-1], out_size],\n",
        "                        dtype, initializers.xavier_initializer())\n",
        "    return activation_fn(_batch_normalization(tf.matmul(_x, w)))\n",
        "  \n",
        "  with tf.variable_scope(name):\n",
        "      x = _batch_normalization(x)\n",
        "      #xnormal=x\n",
        "      for i in range(len(neurons)):\n",
        "        with tf.variable_scope('layer8_%i_' % (i + 1)):\n",
        "          x = _layer(x, neurons[i],\n",
        "            tf.nn.tanh if i < len(neurons)-1 else tf.identity)\n",
        "  return x         \n",
        "\n",
        "def neural_net_p8_p7(x, xi_approx, neurons, is_training, name,\n",
        "                    mv_decay=0.9, dtype=tf.float32):\n",
        "  \n",
        "  \n",
        "  def approx_test(): return xi_approx\n",
        "  def approx_learn(): return x\n",
        "  x = tf.cond(is_training, approx_learn,approx_test)\n",
        "\n",
        "  def _batch_normalization(_x):\n",
        "    beta = tf.get_variable('beta9', [_x.get_shape()[-1]],\n",
        "                           dtype, init_ops.zeros_initializer())\n",
        "    gamma = tf.get_variable('gamma9', [_x.get_shape()[-1]],\n",
        "                            dtype, init_ops.ones_initializer())\n",
        "    mv_mean = tf.get_variable('mv_mean9', [_x.get_shape()[-1]],\n",
        "                              dtype, init_ops.zeros_initializer(),\n",
        "                              trainable=False)\n",
        "    mv_variance = tf.get_variable('mv_variance9', [_x.get_shape()[-1]],\n",
        "                                  dtype, init_ops.ones_initializer(),\n",
        "                                  trainable=False)\n",
        "    mean, variance = tf.nn.moments(_x, [0], name='moments9')\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_mean, mean,\n",
        "                                               mv_decay, True))\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_variance, variance,\n",
        "                                               mv_decay, False))\n",
        "    mean, variance = tf.cond(is_training,\n",
        "                             lambda: (mean, variance),\n",
        "                             lambda: (mv_mean, mv_variance))\n",
        "    return tf.nn.batch_normalization(_x, mean, variance,\n",
        "                                     beta, gamma, 1e-6)\n",
        "  \n",
        "  def _layer(_x, out_size, activation_fn):\n",
        "    w = tf.get_variable('weights9',\n",
        "                        [_x.get_shape().as_list()[-1], out_size],\n",
        "                        dtype, initializers.xavier_initializer())\n",
        "    return activation_fn(_batch_normalization(tf.matmul(_x, w)))\n",
        "  \n",
        "  with tf.variable_scope(name):\n",
        "      x = _batch_normalization(x)\n",
        "      #xnormal=x\n",
        "      for i in range(len(neurons)):\n",
        "        with tf.variable_scope('layer9_%i_' % (i + 1)):\n",
        "          x = _layer(x, neurons[i],\n",
        "            tf.nn.tanh if i < len(neurons)-1 else tf.identity)\n",
        "  return x       \n",
        "\n",
        "def kolmogorov_train_and_test(xi, xi_approx, xi_p1_p0, xi_p8_p7, xi_p7_p6, xi_p6_p5, xi_p5_p4, xi_p4_p3, xi_p3_p2, xi_p2_p1, x_sde, phi_p0, phi_p8_p7, phi_p7_p6, phi_p6_p5, phi_p5_p4, phi_p4_p3, phi_p3_p2, phi_p2_p1, phi_p1_p0, u_reference, u_reference_p0,u_reference_p8_p7,u_reference_p7_p6,u_reference_p6_p5,u_reference_p5_p4,u_reference_p4_p3,u_reference_p3_p2,u_reference_p2_p1,u_reference_p1_p0, neurons,\n",
        "          lr_boundaries, lr_values, train_steps,\n",
        "          mc_rounds, mc_freq, file_name,\n",
        "          dtype=tf.float32):\n",
        "\n",
        "  def _approximate_errors():\n",
        "    lr, gs, lr10,lr21,lr32,lr43,lr54,lr65 = sess.run([learning_rate, global_step,learning_rate_p1_p0,learning_rate_p2_p1,learning_rate_p3_p2,learning_rate_p4_p3,learning_rate_p5_p4,learning_rate_p6_p5])\n",
        "    li_err, li_err_p1_p0, li_err_kombination, li_err_p2_p1, li_err_p3_p2,li_err_p4_p3,li_err_p5_p4,li_err_p6_p5,li_err_p7_p6,li_err_p8_p7 = 0., 0., 0.,0.,0.,0.,0.,0.,0.,0.\n",
        "    #rel_l1_err, rel_l2_err, rel_li_err = 0., 0., 0.\n",
        "    for _ in range(mc_rounds):\n",
        "      li, appr, ref, li_p8_p7, li_p7_p6, li_p6_p5, li_p5_p4, li_p4_p3, li_p3_p2, li_p2_p1,li_p1_p0, li_kombination = sess.run([err_l_inf,  approx, reference, err_l_p8_p7, err_l_p7_p6, err_l_p6_p5, err_l_p5_p4, err_l_p4_p3, err_l_p3_p2, err_l_p2_p1,err_l_p1_p0, err_l_kombination],\n",
        "                                              feed_dict={is_training: False})\n",
        "       #= sess.run([],\n",
        "       #                                       feed_dict={is_training: False})\n",
        "      li_err, li_err_p1_p0, li_err_kombination, li_err_p2_p1, li_err_p3_p2, li_err_p4_p3, li_err_p5_p4, li_err_p6_p5, li_err_p7_p6, li_err_p8_p7 = (np.maximum(li_err, li),np.maximum(li_err_p1_p0, li_p1_p0)\n",
        "          ,np.maximum(li_kombination, li_err_kombination),np.maximum(li_err_p2_p1, li_p2_p1),np.maximum(li_err_p3_p2, li_p3_p2),np.maximum(li_err_p4_p3, li_p4_p3),np.maximum(li_err_p5_p4, li_p5_p4),np.maximum(li_err_p6_p5, li_p6_p5),np.maximum(li_err_p7_p6, li_p7_p6),np.maximum(li_err_p8_p7, li_p8_p7))\n",
        "    #l1_err, l2_err = l1_err / mc_rounds, tf.sqrt(l2_err / mc_rounds)\n",
        "    #rel_l1_err, rel_l2_err \\\n",
        "    #          = rel_l1_err / mc_rounds, tf.sqrt(rel_l2_err / mc_rounds)\n",
        "    t_mc = time.time()\n",
        "    file_out.write('%i, %f, %f,%f, %f, %f, %f, '\n",
        "        '%f, %f, %f, %f, %f, %f, %f, %f, %f, %f, %f, %f, %f, %f, %f \\n' % (gs, li_err, li_err_p1_p0, li_err_p2_p1, li_err_p3_p2, li_err_p4_p3, li_err_p5_p4, li_err_p6_p5, li_err_p7_p6, li_err_p8_p7, li_err_kombination, lr,\n",
        "                      t1_train - t0_train, t_mc - t1_train, appr, ref, lr10,lr21,lr32,lr43,lr54,lr65))\n",
        "    file_out.flush()\n",
        "    \n",
        "\n",
        "  t0_train = time.time()\n",
        "  is_training = tf.placeholder(tf.bool, [])\n",
        "  u_approx = neural_net(xi, xi_approx, neurons, is_training, 'u_approx', dtype=dtype)\n",
        "  u_approx_p1_p0 = neural_net_p1_p0(xi_p1_p0, xi_approx, neurons, is_training, 'u_approx_p1_p0', dtype=dtype)\n",
        "  u_approx_p2_p1 = neural_net_p2_p1(xi_p2_p1, xi_approx, neurons, is_training, 'u_approx_p2_p1', dtype=dtype)\n",
        "  u_approx_p3_p2 = neural_net_p3_p2(xi_p3_p2, xi_approx, neurons, is_training, 'u_approx_p3_p2', dtype=dtype)\n",
        "  u_approx_p4_p3 = neural_net_p4_p3(xi_p4_p3, xi_approx, neurons, is_training, 'u_approx_p4_p3', dtype=dtype)  \n",
        "  u_approx_p5_p4 = neural_net_p5_p4(xi_p5_p4, xi_approx, neurons, is_training, 'u_approx_p5_p4', dtype=dtype)  \n",
        "  u_approx_p6_p5 = neural_net_p6_p5(xi_p6_p5, xi_approx, neurons, is_training, 'u_approx_p6_p5', dtype=dtype)  \n",
        "  u_approx_p7_p6 = neural_net_p7_p6(xi_p7_p6, xi_approx, neurons, is_training, 'u_approx_p7_p6', dtype=dtype)  \n",
        "  u_approx_p8_p7 = neural_net_p8_p7(xi_p8_p7, xi_approx, neurons, is_training, 'u_approx_p8_p7', dtype=dtype)  \n",
        "  #u_approx2 = neural_net_test(xifehler, neurons, is_training, 'u_approx', dtype=dtype)\n",
        "  \n",
        "  \n",
        "  \n",
        "  loss_p0 = tf.reduce_mean(tf.squared_difference(u_approx, phi_p0))\n",
        "  loss_p1_p0 = tf.reduce_mean(tf.squared_difference(u_approx_p1_p0, phi_p1_p0))\n",
        "  loss_p2_p1 = tf.reduce_mean(tf.squared_difference(u_approx_p2_p1, phi_p2_p1))\n",
        "  loss_p3_p2 = tf.reduce_mean(tf.squared_difference(u_approx_p3_p2, phi_p3_p2))\n",
        "  loss_p4_p3 = tf.reduce_mean(tf.squared_difference(u_approx_p4_p3, phi_p4_p3))\n",
        "  loss_p5_p4 = tf.reduce_mean(tf.squared_difference(u_approx_p5_p4, phi_p5_p4))\n",
        "  loss_p6_p5 = tf.reduce_mean(tf.squared_difference(u_approx_p6_p5, phi_p6_p5))\n",
        "  loss_p7_p6 = tf.reduce_mean(tf.squared_difference(u_approx_p7_p6, phi_p7_p6))\n",
        "  loss_p8_p7 = tf.reduce_mean(tf.squared_difference(u_approx_p8_p7, phi_p8_p7))\n",
        "  \n",
        "  approx=tf.reduce_mean(u_approx)\n",
        "  approx_p1_p0=tf.reduce_mean(u_approx_p1_p0)\n",
        "  #approx_delta=tf.reduce_mean((u_approx_delta-u_approx)/h)\n",
        "  #approx_delta=tf.reduce_mean(u_approx_delta)\n",
        "  reference=tf.reduce_mean(u_reference_p0)\n",
        "  #reference_delta=tf.reduce_mean(delta_reference)\n",
        "  err = tf.abs(u_approx - u_reference_p0)\n",
        "  err_p1_p0 = tf.abs(u_approx_p1_p0 - u_reference_p1_p0)\n",
        "  err_p2_p1 = tf.abs(u_approx_p2_p1 - u_reference_p2_p1)\n",
        "  err_p3_p2 = tf.abs(u_approx_p3_p2 - u_reference_p3_p2)\n",
        "  err_p4_p3 = tf.abs(u_approx_p4_p3 - u_reference_p4_p3)\n",
        "  err_p5_p4 = tf.abs(u_approx_p5_p4 - u_reference_p5_p4)\n",
        "  err_p6_p5 = tf.abs(u_approx_p6_p5 - u_reference_p6_p5)\n",
        "  err_p7_p6 = tf.abs(u_approx_p7_p6 - u_reference_p7_p6)\n",
        "  err_p8_p7 = tf.abs(u_approx_p8_p7 - u_reference_p8_p7)\n",
        "\n",
        "  err_kombination=tf.abs(u_approx   + u_approx_p7_p6 + u_approx_p6_p5 + u_approx_p5_p4 + u_approx_p4_p3 + u_approx_p3_p2 + u_approx_p2_p1 + u_approx_p1_p0 - u_reference)\n",
        "  #err_delta=tf.abs(u_approx_delta - delta_reference)\n",
        "  #err_l_1 = tf.reduce_mean(err)\n",
        "  #err_l_2 = tf.reduce_mean(err ** 2)\n",
        "  err_l_inf = tf.reduce_max(err)\n",
        "  err_l_p2_p1 = tf.reduce_max(err_p2_p1)\n",
        "  err_l_p3_p2 = tf.reduce_max(err_p3_p2)\n",
        "  err_l_p4_p3 = tf.reduce_max(err_p4_p3)\n",
        "  err_l_p5_p4 = tf.reduce_max(err_p5_p4)\n",
        "  err_l_p6_p5 = tf.reduce_max(err_p6_p5)\n",
        "  err_l_p7_p6 = tf.reduce_max(err_p7_p6)\n",
        "  err_l_p8_p7 = tf.reduce_max(err_p8_p7)\n",
        "  #err_l_inf_delta = tf.reduce_max(err_delta)\n",
        "  #rel_err = err / tf.maximum(u_reference, 1e-4)\n",
        "  #rel_err_l_1 = tf.reduce_mean(rel_err)\n",
        "  #rel_err_l_2 = tf.reduce_mean(rel_err ** 2)\n",
        "  err_l_p1_p0 = tf.reduce_max(err_p1_p0)\n",
        "  err_l_kombination = tf.reduce_max(err_kombination)\n",
        "  #with tf.Session()  as sess:\n",
        "    #rel_err_l_inf_arg = tf.argmax(rel_err)\n",
        "    #K=xi[rel_err_l_inf_arg]\n",
        "    #K=tf.gather(xi_approx,rel_err_l_inf_arg)\n",
        "    #rel_err_l_inf_arg=K\n",
        "    #rel_err_l_inf_arg=variance\n",
        "\n",
        "\n",
        "    \n",
        "#  global_step = tf.get_variable('global_step', [], tf.int32,tf.constant_initializer(0),trainable=False)\n",
        "#  learning_rate = tf.train.piecewise_constant(global_step,lr_boundaries,lr_values)\n",
        "#  optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "  \n",
        "  #global_step_p1_p0 = tf.get_variable('global_step_p1_p0', [], tf.int32,tf.constant_initializer(0),trainable=False)\n",
        "  #learning_rate_p1_p0 = tf.train.piecewise_constant(global_step_p1_p0,lr_boundaries_p1_p0,lr_values_p1_p0)\n",
        "  #optimizer_p1_p0 = tf.train.AdamOptimizer(learning_rate_p1_p0)\n",
        "\n",
        "  #global_step_p2_p1 = tf.get_variable('global_step_p2_p1', [], tf.int32,tf.constant_initializer(0),trainable=False)\n",
        "  #learning_rate_p2_p1 = tf.train.piecewise_constant(global_step_p2_p1,lr_boundaries_p2_p1,lr_values_p2_p1)\n",
        "  #optimizer_p2_p1 = tf.train.AdamOptimizer(learning_rate_p2_p1)\n",
        "\n",
        "  #global_step_p3_p2 = tf.get_variable('global_step_p3_p2', [], tf.int32,tf.constant_initializer(0),trainable=False)\n",
        "  #learning_rate_p3_p2 = tf.train.piecewise_constant(global_step_p3_p2,lr_boundaries_p3_p2,lr_values_p3_p2)\n",
        "  #optimizer_p3_p2 = tf.train.AdamOptimizer(learning_rate_p3_p2)\n",
        "\n",
        "  #global_step_p4_p3 = tf.get_variable('global_step_p4_p3', [], tf.int32,tf.constant_initializer(0),trainable=False)\n",
        "  #learning_rate_p4_p3 = tf.train.piecewise_constant(global_step_p4_p3,lr_boundaries_p4_p3,lr_values_p4_p3)\n",
        "  #optimizer_p4_p3 = tf.train.AdamOptimizer(learning_rate_p4_p3)\n",
        "\n",
        "  #global_step_p5_p4 = tf.get_variable('global_step_p5_p4', [], tf.int32,tf.constant_initializer(0),trainable=False)\n",
        "  #learning_rate_p5_p4 = tf.train.piecewise_constant(global_step_p5_p4,lr_boundaries_p5_p4,lr_values_p5_p4)\n",
        "  #optimizer_p5_p4 = tf.train.AdamOptimizer(learning_rate_p5_p4)\n",
        "\n",
        "  #global_step_p6_p5 = tf.get_variable('global_step_p6_p5', [], tf.int32,tf.constant_initializer(0),trainable=False)\n",
        "  #learning_rate_p6_p5 = tf.train.piecewise_constant(global_step_p6_p5,lr_boundaries_p6_p5,lr_values_p6_p5)\n",
        "  #optimizer_p6_p5 = tf.train.AdamOptimizer(learning_rate_p6_p5)\n",
        "\n",
        "  #global_step_p7_p6 = tf.get_variable('global_step_p7_p6', [], tf.int32,tf.constant_initializer(0),trainable=False)\n",
        "  #learning_rate_p7_p6 = tf.train.piecewise_constant(global_step_p7_p6,lr_boundaries_p7_p6,lr_values_p7_p6)\n",
        "  #optimizer_p7_p6 = tf.train.AdamOptimizer(learning_rate_p4_p3)\n",
        "\n",
        "  #wird gar nicht gebraucht\n",
        "  global_step_p8_p7 = tf.get_variable('global_step_p8_p7', [], tf.int32,tf.constant_initializer(0),trainable=False)\n",
        "  learning_rate_p8_p7 = tf.train.piecewise_constant(global_step_p8_p7,lr_boundaries_p8_p7,lr_values_p8_p7)\n",
        "  optimizer_p8_p7 = tf.train.AdamOptimizer(learning_rate_p8_p7)\n",
        "\n",
        "#neuer versuch lr\n",
        "  #Variance^gamma als Startlernrate\n",
        "  #\n",
        "  gamma=0.3\n",
        "  #\n",
        "#p0\n",
        "  #std = tf.math.sqrt(tf.math.reduce_variance(phi_p0))\n",
        "  #print(std)\n",
        "  std=tf.math.pow(tf.math.reduce_variance(phi_p0), tf.constant(gamma, dtype=tf.float32), name=None)\n",
        "  #print(std)\n",
        "  lr = 0.01#1/std\n",
        "  step_rate = 40000\n",
        "  decay = 0.1\n",
        "  global_step = tf.Variable(0, trainable=False)\n",
        "  increment_global_step = tf.assign(global_step, global_step + 1)\n",
        "  learning_rate = tf.train.exponential_decay(lr, global_step, step_rate, decay, staircase=True)\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "\n",
        "#p1_p0\n",
        "  std_p1_p0 = 1# tf.math.pow(tf.math.reduce_variance(phi_p1_p0), tf.constant(gamma, dtype=tf.float32), name=None)\n",
        "  \n",
        "  lr_p1_p0 = lr#1/std_p1_p0\n",
        "  step_rate_p1_p0 = 40000\n",
        "  decay_p1_p0 = 0.1\n",
        "  global_step_p1_p0 = tf.Variable(0, trainable=False)\n",
        "  increment_global_step = tf.assign(global_step_p1_p0, global_step_p1_p0 + 1)\n",
        "  learning_rate_p1_p0 = tf.train.exponential_decay(lr_p1_p0, global_step_p1_p0, step_rate_p1_p0, decay_p1_p0, staircase=True)\n",
        "  optimizer_p1_p0 = tf.train.AdamOptimizer(learning_rate_p1_p0)\n",
        "\n",
        "#p2_p1\n",
        "  std_p2_p1 = tf.math.pow(tf.math.reduce_variance(phi_p2_p1), tf.constant(gamma, dtype=tf.float32), name=None)\n",
        "  lr_p2_p1 = lr#1/std_p2_p1\n",
        "  step_rate_p2_p1 =40000\n",
        "  decay_p2_p1 = 0.1\n",
        "  global_step_p2_p1 = tf.Variable(0, trainable=False)\n",
        "  increment_global_step = tf.assign(global_step_p2_p1, global_step_p2_p1 + 1)\n",
        "  learning_rate_p2_p1 = tf.train.exponential_decay(lr_p2_p1, global_step_p2_p1, step_rate_p2_p1, decay_p2_p1, staircase=True)\n",
        "  optimizer_p2_p1 = tf.train.AdamOptimizer(learning_rate_p2_p1)\n",
        "#p3_p2\n",
        "  std_p3_p2 = tf.math.pow(tf.math.reduce_variance(phi_p3_p2), tf.constant(gamma, dtype=tf.float32), name=None)\n",
        "  lr_p3_p2 = lr#1/std_p3_p2\n",
        "  step_rate_p3_p2 = 40000\n",
        "  decay_p3_p2 = 0.1\n",
        "  global_step_p3_p2 = tf.Variable(0, trainable=False)\n",
        "  increment_global_step = tf.assign(global_step_p3_p2, global_step_p3_p2 + 1)\n",
        "  learning_rate_p3_p2 = tf.train.exponential_decay(lr_p3_p2, global_step_p3_p2, step_rate_p3_p2, decay_p3_p2, staircase=True)\n",
        "  optimizer_p3_p2 = tf.train.AdamOptimizer(learning_rate_p3_p2)\n",
        "#p4_p3\n",
        "  std_p4_p3 = tf.math.pow(tf.math.reduce_variance(phi_p4_p3), tf.constant(gamma, dtype=tf.float32), name=None)\n",
        "  lr_p4_p3 = lr#1/std_p4_p3\n",
        "  step_rate_p4_p3 = 40000\n",
        "  decay_p4_p3 = 0.1\n",
        "  global_step_p4_p3 = tf.Variable(0, trainable=False)\n",
        "  increment_global_step = tf.assign(global_step_p4_p3, global_step_p4_p3 + 1)\n",
        "  learning_rate_p4_p3 = tf.train.exponential_decay(lr_p4_p3, global_step_p4_p3, step_rate_p4_p3, decay_p4_p3, staircase=True)\n",
        "  optimizer_p4_p3 = tf.train.AdamOptimizer(learning_rate_p4_p3)\n",
        "#p5_p4\n",
        "  std_p5_p4 = tf.math.pow(tf.math.reduce_variance(phi_p5_p4), tf.constant(gamma, dtype=tf.float32), name=None)\n",
        "  lr_p5_p4 = lr#1/std_p5_p4\n",
        "  step_rate_p5_p4 = 40000\n",
        "  decay_p5_p4 = 0.1\n",
        "  global_step_p5_p4 = tf.Variable(0, trainable=False)\n",
        "  increment_global_step = tf.assign(global_step_p5_p4, global_step_p5_p4 + 1)\n",
        "  learning_rate_p5_p4 = tf.train.exponential_decay(lr_p5_p4, global_step_p5_p4, step_rate_p5_p4, decay_p5_p4, staircase=True)\n",
        "  optimizer_p5_p4 = tf.train.AdamOptimizer(learning_rate_p5_p4)\n",
        "\n",
        "#p6_p5\n",
        "  std_p6_p5 = tf.math.pow(tf.math.reduce_variance(phi_p6_p5), tf.constant(gamma, dtype=tf.float32), name=None)\n",
        "  lr_p6_p5 = lr#1/std_p6_p5\n",
        "  step_rate_p6_p5 = 40000\n",
        "  decay_p6_p5 = 0.1\n",
        "  global_step_p6_p5 = tf.Variable(0, trainable=False)\n",
        "  increment_global_step = tf.assign(global_step_p6_p5, global_step_p6_p5 + 1)\n",
        "  learning_rate_p6_p5 = tf.train.exponential_decay(lr_p6_p5, global_step_p6_p5, step_rate_p6_p5, decay_p6_p5, staircase=True)\n",
        "  optimizer_p6_p5 = tf.train.AdamOptimizer(learning_rate_p6_p5)\n",
        "\n",
        "#p7_p6\n",
        "  std_p7_p6 = tf.math.pow(tf.math.reduce_variance(phi_p7_p6), tf.constant(gamma, dtype=tf.float32), name=None)\n",
        "  lr_p7_p6 = 0.1#lr#1/std_p7_p6\n",
        "  step_rate_p7_p6 =40000\n",
        "  decay_p7_p6 = 0.1\n",
        "  global_step_p7_p6 = tf.Variable(0, trainable=False)\n",
        "  increment_global_step = tf.assign(global_step_p7_p6, global_step_p7_p6 + 1)\n",
        "  learning_rate_p7_p6 = tf.train.exponential_decay(lr_p7_p6, global_step_p7_p6, step_rate_p7_p6, decay_p7_p6, staircase=True)\n",
        "  optimizer_p7_p6 = tf.train.AdamOptimizer(learning_rate_p7_p6)\n",
        "  #learning_rate=tf.constant(optimizer._lr)\n",
        "  #print(learning_rate)\n",
        "\n",
        "  update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, 'u_approx')\n",
        "  with tf.control_dependencies(update_ops):\n",
        "    train_op = optimizer.minimize(loss_p0, global_step)\n",
        "  \n",
        "  update_ops_p1_p0 = tf.get_collection(tf.GraphKeys.UPDATE_OPS, 'u_approx_p1_p0')\n",
        "  with tf.control_dependencies(update_ops_p1_p0):\n",
        "    train_op_p1_p0 = optimizer_p1_p0.minimize(loss_p1_p0, global_step_p1_p0)  \n",
        "\n",
        "  update_ops_p2_p1 = tf.get_collection(tf.GraphKeys.UPDATE_OPS, 'u_approx_p2_p1')\n",
        "  with tf.control_dependencies(update_ops_p2_p1):\n",
        "    train_op_p2_p1 = optimizer_p2_p1.minimize(loss_p2_p1, global_step_p2_p1)   \n",
        "    \n",
        "  update_ops_p3_p2 = tf.get_collection(tf.GraphKeys.UPDATE_OPS, 'u_approx_p3_p2')\n",
        "  with tf.control_dependencies(update_ops_p3_p2):\n",
        "    train_op_p3_p2 = optimizer_p3_p2.minimize(loss_p3_p2, global_step_p3_p2)   \n",
        "\n",
        "  update_ops_p4_p3 = tf.get_collection(tf.GraphKeys.UPDATE_OPS, 'u_approx_p4_p3')\n",
        "  with tf.control_dependencies(update_ops_p4_p3):\n",
        "    train_op_p4_p3 = optimizer_p4_p3.minimize(loss_p4_p3, global_step_p4_p3)    \n",
        "\n",
        "  update_ops_p5_p4 = tf.get_collection(tf.GraphKeys.UPDATE_OPS, 'u_approx_p5_p4')\n",
        "  with tf.control_dependencies(update_ops_p5_p4):\n",
        "    train_op_p5_p4 = optimizer_p5_p4.minimize(loss_p5_p4, global_step_p5_p4) \n",
        "\n",
        "  update_ops_p6_p5 = tf.get_collection(tf.GraphKeys.UPDATE_OPS, 'u_approx_p6_p5')\n",
        "  with tf.control_dependencies(update_ops_p6_p5):\n",
        "    train_op_p6_p5 = optimizer_p6_p5.minimize(loss_p6_p5, global_step_p6_p5) \n",
        "\n",
        "  update_ops_p7_p6 = tf.get_collection(tf.GraphKeys.UPDATE_OPS, 'u_approx_p7_p6')\n",
        "  with tf.control_dependencies(update_ops_p7_p6):\n",
        "    train_op_p7_p6 = optimizer_p7_p6.minimize(loss_p7_p6, global_step_p7_p6) \n",
        "\n",
        "  update_ops_p8_p7 = tf.get_collection(tf.GraphKeys.UPDATE_OPS, 'u_approx_p8_p7')\n",
        "  with tf.control_dependencies(update_ops_p8_p7):\n",
        "    train_op_p8_p7 = optimizer_p8_p7.minimize(loss_p8_p7, global_step_p8_p7) \n",
        "        \n",
        "  file_out = open(file_name, 'w')\n",
        "  file_out.write('step, p0, p1_p0, p2_p1, p3_p2, p4_p3, p5_p4, p6_p5, p7_p6, p8_p7,kombination, learning_rate p0, time_train, time_mc ,approx, reference, lr10,lr21,lr32,lr43,lr54,lr65 \\n ')\n",
        "\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for step in range(2,train_steps):\n",
        "      if step % mc_freq == 0:\n",
        "        print(step)\n",
        "        t1_train = time.time()\n",
        "        _approximate_errors()\n",
        "        t0_train = time.time()\n",
        "      sess.run([train_op,train_op_p1_p0,train_op_p2_p1,train_op_p3_p2,train_op_p4_p3,train_op_p5_p4,train_op_p6_p5,train_op_p7_p6], feed_dict={is_training:True})\n",
        "    t1_train = time.time()\n",
        "    _approximate_errors()     \n",
        "  file_out.close()\n",
        "  \n",
        "  \n",
        "\n",
        "#Heston Modell (3.6 kolmogorov)\n",
        "for i in range(1,2):\n",
        " print(i)\n",
        " tf.reset_default_graph()\n",
        " with tf.Session()  as sess:\n",
        "  dtype = tf.float32\n",
        "  batch_size      =75000 #batch P0\n",
        "  batch_size_p1_p0=1817   #bath P1-P0\n",
        "  batch_size_p2_p1=690\n",
        "  batch_size_p3_p2=264\n",
        "  batch_size_p4_p3=93\n",
        "  batch_size_p5_p4=33\n",
        "  batch_size_p6_p5=12\n",
        "  batch_size_p7_p6=5\n",
        "  batch_size_p8_p7=0     \n",
        "  batch_size_approx=2000000\n",
        "\n",
        "  N, d = 1, 5\n",
        "  N_p0=N\n",
        "  N_p1_p0=N \n",
        "  N_p2_p1=N*2\n",
        "  N_p3_p2=N*4\n",
        "  N_p4_p3=N*8\n",
        "  N_p5_p4=N*16\n",
        "  N_p6_p5=N*32\n",
        "  N_p7_p6=N*64\n",
        "  N_p8_p7=N*128\n",
        "  #beta = tf.constant([0.15], dtype=dtype)\n",
        "\n",
        "  #r=0.05\n",
        "  #B=120.\n",
        "  #K=110.\n",
        "  #h_fin=0.0000001\n",
        "  #mu=0.05\n",
        "  #sigma=0.2\n",
        "  neurons = [50, 50, 1]\n",
        "  train_steps = 50000\n",
        "  Ksteps_p8_p7=0#10000#1800\n",
        "  Ksteps_p7_p6=150000#\n",
        "  Ksteps_p6_p5=150000#150000\n",
        "  Ksteps_p5_p4=150000#150000\n",
        "  Ksteps_p4_p3=150000#150000\n",
        "  Ksteps_p3_p2=150000#150000\n",
        "  Ksteps_p2_p1=150000#150000\n",
        "  Ksteps_p1_p0=150000#150000\n",
        "  Ksteps_p0=   train_steps#120000      \n",
        "  mc_rounds, mc_freq = 1, 10000\n",
        "  #mc_samples_ref, mc_rounds_ref = 1024, 1024\n",
        "  mc_samples_ref_p0=1#1530596\n",
        "  mc_samples_ref_p1_p0=1#5000\n",
        "  mc_samples_ref_p2_p1=1#1000\n",
        "  mc_samples_ref_p3_p2=1#500\n",
        "  mc_samples_ref_p4_p3=1#500\n",
        "  mc_samples_ref_p5_p4=1#100\n",
        "  mc_samples_ref_p6_p5=1#100\n",
        "  mc_samples_ref, mc_rounds_ref_p0, mc_rounds_ref_p1_p0 = 1, 1000000,1000000\n",
        "  lr_boundaries = [501,2001,4001,6001,10001,30001]\n",
        "  lr_values = [1.0,0.1,0.01,0.001,0.0001,0.00005,0.00001]\n",
        "\n",
        "  lr_boundaries_p1_p0 = [1001,2001,2500]\n",
        "  lr_values_p1_p0 = [1.0,0.1, 0.01, 0.001]\n",
        "  lr_boundaries_p2_p1 = [1001,2001,2500]\n",
        "  lr_values_p2_p1 = [1.0,0.1, 0.01, 0.0001]\n",
        "  lr_boundaries_p3_p2 = [1001,2001,2500]\n",
        "  lr_values_p3_p2 = [1.0, 0.1, 0.01, 0.001]\n",
        "  lr_boundaries_p4_p3 = [1001,2001,2500]\n",
        "  lr_values_p4_p3 = [1.0, 0.1, 0.01, 0.001]\n",
        "  lr_boundaries_p5_p4 = [1001,2001,2500]\n",
        "  lr_values_p5_p4 = [1.0, 0.1, 0.01, 0.001]\n",
        "  lr_boundaries_p6_p5 = [1001,2001,2500]\n",
        "  lr_values_p6_p5 = [1.0, 0.1, 0.01, 0.001]\n",
        "  lr_boundaries_p7_p6 = [1001,2001,2500]\n",
        "  lr_values_p7_p6 = [1.0, 0.1, 0.01, 0.001]\n",
        "  lr_boundaries_p8_p7 = [1001,2001,2500]\n",
        "  lr_values_p8_p7 = [1.0, 0.1, 0.01, 0.001]\n",
        "\n",
        "  s_0_l=80.0\n",
        "  s_0_r=120.0\n",
        "  sigma_l=0.1\n",
        "  sigma_r=0.2\n",
        "  mu_l=0.02\n",
        "  mu_r=0.05\n",
        "  T_l=0.9\n",
        "  T_r=1.0\n",
        "  K_l=109.0\n",
        "  K_r=110.0\n",
        "  s_0_l_approx=80.4\n",
        "  s_0_r_approx=119.6\n",
        "  sigma_l_approx=0.11\n",
        "  sigma_r_approx=0.19\n",
        "  mu_l_approx=0.03\n",
        "  mu_r_approx=0.04\n",
        "  T_l_approx=0.91\n",
        "  T_r_approx=0.99\n",
        "  K_l_approx=109.1\n",
        "  K_r_approx=109.9\n",
        "\n",
        "\n",
        "  s0 = tf.random_uniform((batch_size,1), minval=s_0_l,\n",
        "                                 maxval=s_0_r, dtype=dtype)\n",
        "  sigma=tf.random_uniform((batch_size,1),\n",
        "                            minval=sigma_l,maxval=sigma_r, dtype=dtype)\n",
        "  mu=tf.random_uniform((batch_size,1),\n",
        "                            minval=mu_l,maxval=mu_r, dtype=dtype)\n",
        "  T=tf.random_uniform((batch_size,1),\n",
        "                            minval=T_l,maxval=T_r, dtype=dtype)\n",
        "  K=tf.random_uniform((batch_size,1),\n",
        "                      minval=K_l,maxval=K_r, dtype=dtype)\n",
        "  s0_p1_p0 = tf.stack((tf.random_uniform((batch_size_p1_p0,1), minval=s_0_l,\n",
        "                                 maxval=s_0_r, dtype=dtype)))\n",
        "  sigma_p1_p0=tf.random_uniform((batch_size_p1_p0,1),\n",
        "                            minval=sigma_l,maxval=sigma_r, dtype=dtype)\n",
        "  mu_p1_p0=tf.random_uniform((batch_size_p1_p0,1),\n",
        "                            minval=mu_l,maxval=mu_r, dtype=dtype)\n",
        "  T_p1_p0=tf.random_uniform((batch_size_p1_p0,1),\n",
        "                            minval=T_l,maxval=T_r, dtype=dtype)\n",
        "  K_p1_p0=tf.random_uniform((batch_size_p1_p0,1),\n",
        "                      minval=K_l,maxval=K_r, dtype=dtype)\n",
        "  s0_p2_p1 = tf.stack((tf.random_uniform((batch_size_p2_p1,1), minval=s_0_l,\n",
        "                                 maxval=s_0_r, dtype=dtype)))\n",
        "  sigma_p2_p1=tf.random_uniform((batch_size_p2_p1,1),\n",
        "                            minval=sigma_l,maxval=sigma_r, dtype=dtype)\n",
        "  mu_p2_p1=tf.random_uniform((batch_size_p2_p1,1),\n",
        "                            minval=mu_l,maxval=mu_r, dtype=dtype)\n",
        "  T_p2_p1=tf.random_uniform((batch_size_p2_p1,1),\n",
        "                            minval=T_l,maxval=T_r, dtype=dtype)\n",
        "  K_p2_p1=tf.random_uniform((batch_size_p2_p1,1),\n",
        "                      minval=K_l,maxval=K_r, dtype=dtype)\n",
        "  s0_p3_p2 = tf.stack((tf.random_uniform((batch_size_p3_p2,1), minval=s_0_l,\n",
        "                                 maxval=s_0_r, dtype=dtype)))\n",
        "  sigma_p3_p2=tf.random_uniform((batch_size_p3_p2,1),\n",
        "                            minval=sigma_l,maxval=sigma_r, dtype=dtype)\n",
        "  mu_p3_p2=tf.random_uniform((batch_size_p3_p2,1),\n",
        "                            minval=mu_l,maxval=mu_r, dtype=dtype)\n",
        "  T_p3_p2=tf.random_uniform((batch_size_p3_p2,1),\n",
        "                            minval=T_l,maxval=T_r, dtype=dtype)\n",
        "  K_p3_p2=tf.random_uniform((batch_size_p3_p2,1),\n",
        "                      minval=K_l,maxval=K_r, dtype=dtype)\n",
        "  s0_p4_p3 = tf.stack((tf.random_uniform((batch_size_p4_p3,1), minval=s_0_l,\n",
        "                                 maxval=s_0_r, dtype=dtype)))\n",
        "  sigma_p4_p3=tf.random_uniform((batch_size_p4_p3,1),\n",
        "                            minval=sigma_l,maxval=sigma_r, dtype=dtype)\n",
        "  mu_p4_p3=tf.random_uniform((batch_size_p4_p3,1),\n",
        "                            minval=mu_l,maxval=mu_r, dtype=dtype)\n",
        "  T_p4_p3=tf.random_uniform((batch_size_p4_p3,1),\n",
        "                            minval=T_l,maxval=T_r, dtype=dtype)\n",
        "  K_p4_p3=tf.random_uniform((batch_size_p4_p3,1),\n",
        "                      minval=K_l,maxval=K_r, dtype=dtype)\n",
        "  s0_p5_p4 = tf.stack((tf.random_uniform((batch_size_p5_p4,1), minval=s_0_l,\n",
        "                                 maxval=s_0_r, dtype=dtype)))\n",
        "  sigma_p5_p4=tf.random_uniform((batch_size_p5_p4,1),\n",
        "                            minval=sigma_l,maxval=sigma_r, dtype=dtype)\n",
        "  mu_p5_p4=tf.random_uniform((batch_size_p5_p4,1),\n",
        "                            minval=mu_l,maxval=mu_r, dtype=dtype)\n",
        "  T_p5_p4=tf.random_uniform((batch_size_p5_p4,1),\n",
        "                            minval=T_l,maxval=T_r, dtype=dtype)\n",
        "  K_p5_p4=tf.random_uniform((batch_size_p5_p4,1),\n",
        "                      minval=K_l,maxval=K_r, dtype=dtype)\n",
        "  s0_p6_p5 = tf.stack((tf.random_uniform((batch_size_p6_p5,1), minval=s_0_l,\n",
        "                                 maxval=s_0_r, dtype=dtype)))\n",
        "  sigma_p6_p5=tf.random_uniform((batch_size_p6_p5,1),\n",
        "                            minval=sigma_l,maxval=sigma_r, dtype=dtype)\n",
        "  mu_p6_p5=tf.random_uniform((batch_size_p6_p5,1),\n",
        "                            minval=mu_l,maxval=mu_r, dtype=dtype)\n",
        "  T_p6_p5=tf.random_uniform((batch_size_p6_p5,1),\n",
        "                            minval=T_l,maxval=T_r, dtype=dtype)\n",
        "  K_p6_p5=tf.random_uniform((batch_size_p6_p5,1),\n",
        "                      minval=K_l,maxval=K_r, dtype=dtype)\n",
        "  s0_p7_p6 = tf.stack((tf.random_uniform((batch_size_p7_p6 ,1), minval=s_0_l,\n",
        "                                 maxval=s_0_r, dtype=dtype)))\n",
        "  sigma_p7_p6 =tf.random_uniform((batch_size_p7_p6 ,1),\n",
        "                            minval=sigma_l,maxval=sigma_r, dtype=dtype)\n",
        "  mu_p7_p6 =tf.random_uniform((batch_size_p7_p6 ,1),\n",
        "                            minval=mu_l,maxval=mu_r, dtype=dtype)\n",
        "  T_p7_p6 =tf.random_uniform((batch_size_p7_p6 ,1),\n",
        "                            minval=T_l,maxval=T_r, dtype=dtype)\n",
        "  K_p7_p6 =tf.random_uniform((batch_size_p7_p6 ,1),\n",
        "                      minval=K_l,maxval=K_r, dtype=dtype)\n",
        "  s0_p8_p7 = tf.stack((tf.random_uniform((batch_size_p8_p7,1), minval=s_0_l,\n",
        "                                 maxval=s_0_r, dtype=dtype)))\n",
        "  sigma_p8_p7=tf.random_uniform((batch_size_p8_p7,1),\n",
        "                            minval=sigma_l,maxval=sigma_r, dtype=dtype)\n",
        "  mu_p8_p7=tf.random_uniform((batch_size_p8_p7,1),\n",
        "                            minval=mu_l,maxval=mu_r, dtype=dtype)\n",
        "  T_p8_p7=tf.random_uniform((batch_size_p8_p7,1),\n",
        "                            minval=T_l,maxval=T_r, dtype=dtype)\n",
        "  K_p8_p7=tf.random_uniform((batch_size_p8_p7,1),\n",
        "                      minval=K_l,maxval=K_r, dtype=dtype)\n",
        "\n",
        "  xi=tf.reshape(tf.stack([s0,sigma,mu,T,K], axis=2), (batch_size,d))\n",
        "  xi_p1_p0=tf.reshape(tf.stack([s0_p1_p0,sigma_p1_p0,mu_p1_p0,T_p1_p0,K_p1_p0], axis=2), (batch_size_p1_p0,d))\n",
        "  xi_p2_p1=tf.reshape(tf.stack([s0_p2_p1,sigma_p2_p1,mu_p2_p1,T_p2_p1,K_p2_p1], axis=2), (batch_size_p2_p1,d))\n",
        "  xi_p3_p2=tf.reshape(tf.stack([s0_p3_p2,sigma_p3_p2,mu_p3_p2,T_p3_p2,K_p3_p2], axis=2), (batch_size_p3_p2,d))\n",
        "  xi_p4_p3=tf.reshape(tf.stack([s0_p4_p3,sigma_p4_p3,mu_p4_p3,T_p4_p3,K_p4_p3], axis=2), (batch_size_p4_p3,d))\n",
        "  xi_p5_p4=tf.reshape(tf.stack([s0_p5_p4,sigma_p5_p4,mu_p5_p4,T_p5_p4,K_p5_p4], axis=2), (batch_size_p5_p4,d))\n",
        "  xi_p6_p5=tf.reshape(tf.stack([s0_p6_p5,sigma_p6_p5,mu_p6_p5,T_p6_p5,K_p6_p5], axis=2), (batch_size_p6_p5,d))\n",
        "  xi_p7_p6=tf.reshape(tf.stack([s0_p7_p6,sigma_p7_p6,mu_p7_p6,T_p7_p6,K_p7_p6], axis=2), (batch_size_p7_p6,d))\n",
        "  xi_p8_p7=tf.reshape(tf.stack([s0_p8_p7,sigma_p8_p7,mu_p8_p7,T_p8_p7,K_p8_p7], axis=2), (batch_size_p8_p7,d)) \n",
        "\n",
        "  s0_approx = tf.random_uniform((batch_size_approx,  1 ), \n",
        "                            minval=s_0_l_approx,maxval=s_0_r_approx, dtype=dtype)\n",
        "  sigma_approx=tf.random_uniform((batch_size_approx,  1 ), \n",
        "                            minval=sigma_l_approx,maxval=sigma_r_approx, dtype=dtype)\n",
        "  mu_approx=tf.random_uniform((batch_size_approx,1),\n",
        "                            minval=mu_l_approx,maxval=mu_r_approx, dtype=dtype)\n",
        "  T_approx=tf.random_uniform((batch_size_approx,1),\n",
        "                            minval=T_l_approx,maxval=T_r_approx, dtype=dtype)\n",
        "  K_approx=tf.random_uniform((batch_size_approx,1),\n",
        "                            minval=K_l_approx,maxval=K_r_approx, dtype=dtype)\n",
        "  xi_approx=tf.reshape(tf.stack([s0_approx,sigma_approx,mu_approx,T_approx,K_approx], axis=2), (batch_size_approx,d))\n",
        "  #xi_approx=xi\n",
        "  tfd = tfp.distributions\n",
        "  dist = tfd.Normal(loc=tf.cast(0.,tf.float32), scale=tf.cast(1.,tf.float32))\n",
        "  d1=tf.math.divide(\n",
        "  (tf.log(tf.math.divide(s0_approx,K_approx))+(mu_approx + 0.5*sigma_approx**2)*T_approx) , (sigma_approx*tf.sqrt(T_approx)))\n",
        "  d2=tf.math.divide(\n",
        "  (tf.log(tf.math.divide(s0_approx,K_approx))+(mu_approx - 0.5*sigma_approx**2)*T_approx) , (sigma_approx*tf.sqrt(T_approx)))\n",
        "  u_reference= tf.multiply(s0_approx,(dist.cdf(d1)))-K_approx*tf.exp(-mu_approx*T_approx)*(dist.cdf(d2))\n",
        "\n",
        "\n",
        "  #h = T / N\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#  h_p1_p0=h_p1_p0_coarse\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "#\n",
        " def sde_body_p1(idx, s,sigma,mu,T,K, samples): #Milstein Scheme\n",
        "    z = tf.random_normal(shape=(samples, batch_size_p1_p0, 1),\n",
        "                         stddev=1., dtype=dtype)\n",
        "    s=s + mu *s * h_p1_p0 +sigma * s *tf.sqrt(h_p1_p0)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(h_p1_p0)*z)**2-h_p1_p0)  \n",
        "    return tf.add(idx, 1), s, sigma,mu,T,K\n",
        "\n",
        " def mc_body_p1(idx, p):\n",
        "    _, _x, sigma,mu,T,K = tf.while_loop(lambda _idx, s, sigma,mu,T,K: _idx < N,\n",
        "                          lambda _idx, s, sigma,mu,T,K: sde_body_p1(_idx, s,sigma,mu,T,K,\n",
        "                                                   mc_samples_ref),\n",
        "                                                   loop_var_mc_p1)\n",
        "#    return idx + 1, p + tf.reduce_mean(phi(_xfine, 2), axis=0) -tf.reduce_mean(phi(_xcoarse, 2), axis=0) \n",
        "    return idx + 1, p + tf.reduce_mean(phi(_x,sigma,mu,T,K, 2), axis=0)\n",
        "\n",
        "#E[P_1-P_0] ein Pfad\n",
        "#\n",
        " def sde_body_p1_p0(idx, s, sfine,sigma,mu,T,K, samples): #Milstein Scheme\n",
        "    z1 = tf.random_normal(shape=(samples, batch_size_p1_p0, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z2 = tf.random_normal(shape=(samples, batch_size_p1_p0, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z=(z1+z2)/tf.sqrt(2.)\n",
        "    #Milstein\n",
        "    #feiner Pfad\n",
        "    h_p1_p0_coarse= T / (N_p1_p0)\n",
        "    h_p1_p0_fine= T / (N_p1_p0*2)\n",
        "    hfine=h_p1_p0_fine\n",
        "    hcoarse=h_p1_p0_coarse\n",
        "\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z1 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z1)**2-hfine)\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z2 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z2)**2-hfine)\n",
        "    #grober Pfad\n",
        "    s=s + mu *s * hcoarse +sigma * s *tf.sqrt(hcoarse)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(hcoarse)*z)**2-hcoarse)    \n",
        "    return tf.add(idx, 1), s, sfine, sigma,mu,T,K   \n",
        "\n",
        " def mc_body_p1_p0(idx, p):\n",
        "    _, _xcoarse, _xfine, sigma,mu,T,K = tf.while_loop(lambda _idx, s, xfine, sigma,mu,T,K: _idx < N_p1_p0,\n",
        "                          lambda _idx, s, xfine, sigma,mu,T,K: sde_body_p1_p0(_idx, s, xfine,sigma,mu,T,K,\n",
        "                                                   mc_samples_ref),\n",
        "                                                   loop_var_mc_p1_p0)\n",
        "    return idx + 1, p + tf.reduce_mean(phi(_xfine,sigma,mu,T,K, 2)-phi(_xcoarse,sigma,mu,T,K, 2), axis=0) \n",
        "#    return idx + 1, p + tf.reduce_mean(phi(_xcoarse, 2), axis=0)    \n",
        "\n",
        "#E[P_2-P_1] ein Pfad\n",
        "#\n",
        " def sde_body_p2_p1(idx, s, sfine,sigma,mu,T,K, samples): #Milstein Scheme\n",
        "    z1 = tf.random_normal(shape=(samples, batch_size_p2_p1, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z2 = tf.random_normal(shape=(samples, batch_size_p2_p1, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z=(z1+z2)/tf.sqrt(2.)\n",
        "    \n",
        "    h_p2_p1_coarse=T / (N_p2_p1)\n",
        "    h_p2_p1_fine=T / (N_p2_p1*2)\n",
        "    hfine=h_p2_p1_fine\n",
        "    hcoarse=h_p2_p1_coarse\n",
        "\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z1 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z1)**2-hfine)\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z2 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z2)**2-hfine)\n",
        "    #grober Pfad\n",
        "    s=s + mu *s * hcoarse +sigma * s *tf.sqrt(hcoarse)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(hcoarse)*z)**2-hcoarse)    \n",
        "    return tf.add(idx, 1), s, sfine , sigma,mu,T,K\n",
        "\n",
        " def mc_body_p2_p1(idx, p):\n",
        "    _, _xcoarse, _xfine, sigma,mu,T,K = tf.while_loop(lambda _idx, s, xfine, sigma,mu,T,K: _idx < N_p2_p1,\n",
        "                          lambda _idx, s, xfine, sigma,mu,T,K: sde_body_p2_p1(_idx, s, xfine,sigma,mu,T,K,\n",
        "                                                   mc_samples_ref),\n",
        "                                                   loop_var_mc_p2_p1)\n",
        "    return idx + 1, p + tf.reduce_mean(phi(_xfine,sigma,mu,T,K, 2), axis=0) -tf.reduce_mean(phi(_xcoarse,sigma,mu,T,K, 2), axis=0) \n",
        "#    return idx + 1, p + tf.reduce_mean(phi(_xcoarse, 2), axis=0)    \n",
        "\n",
        "#E[P_3-P_2] ein Pfad\n",
        "#\n",
        " def sde_body_p3_p2(idx, s, sfine,sigma,mu,T,K, samples): #Milstein Scheme\n",
        "    z1 = tf.random_normal(shape=(samples, batch_size_p3_p2, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z2 = tf.random_normal(shape=(samples, batch_size_p3_p2, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z=(z1+z2)/tf.sqrt(2.)\n",
        "    h_p3_p2_coarse=T / (N_p3_p2)\n",
        "    h_p3_p2_fine=T / (N_p3_p2*2)\n",
        "    hfine=h_p3_p2_fine\n",
        "    hcoarse=h_p3_p2_coarse\n",
        "\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z1 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z1)**2-hfine)\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z2 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z2)**2-hfine)\n",
        "    #grober Pfad\n",
        "    s=s + mu *s * hcoarse +sigma * s *tf.sqrt(hcoarse)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(hcoarse)*z)**2-hcoarse)    \n",
        "    return tf.add(idx, 1), s, sfine  , sigma,mu,T,K\n",
        "\n",
        " def sde_body_p4_p3(idx, s, sfine,sigma,mu,T,K, samples): #Milstein Scheme\n",
        "    z1 = tf.random_normal(shape=(samples, batch_size_p4_p3, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z2 = tf.random_normal(shape=(samples, batch_size_p4_p3, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z=(z1+z2)/tf.sqrt(2.)\n",
        "    h_p4_p3_coarse=T / (N_p4_p3)\n",
        "    h_p4_p3_fine=T / (N_p4_p3*2)\n",
        "    hfine=h_p4_p3_fine\n",
        "    hcoarse=h_p4_p3_coarse\n",
        "\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z1 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z1)**2-hfine)\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z2 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z2)**2-hfine)\n",
        "    #grober Pfad\n",
        "    s=s + mu *s * hcoarse +sigma * s *tf.sqrt(hcoarse)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(hcoarse)*z)**2-hcoarse)    \n",
        "    return tf.add(idx, 1), s, sfine  , sigma,mu,T,K\n",
        "\n",
        " def sde_body_p5_p4(idx, s, sfine,sigma,mu,T,K, samples): #Milstein Scheme\n",
        "    z1 = tf.random_normal(shape=(samples, batch_size_p5_p4, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z2 = tf.random_normal(shape=(samples, batch_size_p5_p4, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z=(z1+z2)/tf.sqrt(2.)\n",
        "    h_p5_p4_coarse=T / (N_p5_p4)\n",
        "    h_p5_p4_fine=T / (N_p5_p4*2)\n",
        "    hfine=h_p5_p4_fine\n",
        "    hcoarse=h_p5_p4_coarse\n",
        "\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z1 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z1)**2-hfine)\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z2 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z2)**2-hfine)\n",
        "    #grober Pfad\n",
        "    s=s + mu *s * hcoarse +sigma * s *tf.sqrt(hcoarse)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(hcoarse)*z)**2-hcoarse)    \n",
        "    return tf.add(idx, 1), s, sfine  , sigma,mu,T,K\n",
        "\n",
        " def sde_body_p6_p5(idx, s, sfine,sigma,mu,T,K, samples): #Milstein Scheme\n",
        "    z1 = tf.random_normal(shape=(samples, batch_size_p6_p5, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z2 = tf.random_normal(shape=(samples, batch_size_p6_p5, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z=(z1+z2)/tf.sqrt(2.)\n",
        "    h_p6_p5_coarse=T / (N_p6_p5)\n",
        "    h_p6_p5_fine=T / (N_p6_p5*2)\n",
        "    hfine=h_p6_p5_fine\n",
        "    hcoarse=h_p6_p5_coarse\n",
        "\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z1 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z1)**2-hfine)\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z2 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z2)**2-hfine)\n",
        "    #grober Pfad\n",
        "    s=s + mu *s * hcoarse +sigma * s *tf.sqrt(hcoarse)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(hcoarse)*z)**2-hcoarse)    \n",
        "    return tf.add(idx, 1), s, sfine  , sigma,mu,T,K\n",
        " def sde_body_p7_p6(idx, s, sfine,sigma,mu,T,K, samples): #Milstein Scheme\n",
        "    z1 = tf.random_normal(shape=(samples, batch_size_p7_p6, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z2 = tf.random_normal(shape=(samples, batch_size_p7_p6, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z=(z1+z2)/tf.sqrt(2.)\n",
        "    h_p7_p6_coarse=T / (N_p7_p6)\n",
        "    h_p7_p6_fine=T / (N_p7_p6*2)\n",
        "    hfine=h_p7_p6_fine\n",
        "    hcoarse=h_p7_p6_coarse\n",
        "\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z1 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z1)**2-hfine)\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z2 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z2)**2-hfine)\n",
        "    #grober Pfad\n",
        "    s=s + mu *s * hcoarse +sigma * s *tf.sqrt(hcoarse)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(hcoarse)*z)**2-hcoarse)    \n",
        "    return tf.add(idx, 1), s, sfine  , sigma,mu,T,K\n",
        " def sde_body_p8_p7(idx, s, sfine,sigma,mu,T,K, samples): #Milstein Scheme\n",
        "    z1 = tf.random_normal(shape=(samples, batch_size_p8_p7, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z2 = tf.random_normal(shape=(samples, batch_size_p8_p7, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z=(z1+z2)/tf.sqrt(2.)\n",
        "\n",
        "    h_p8_p7_coarse=T / (N_p8_p7)\n",
        "    h_p8_p7_fine=T / (N_p8_p7*2)\n",
        "    hfine=h_p8_p7_fine\n",
        "    hcoarse=h_p8_p7_coarse\n",
        "\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z1 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z1)**2-hfine)\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z2 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z2)**2-hfine)\n",
        "    #grober Pfad\n",
        "    s=s + mu *s * hcoarse +sigma * s *tf.sqrt(hcoarse)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(hcoarse)*z)**2-hcoarse)    \n",
        "    return tf.add(idx, 1), s, sfine  , sigma,mu,T,K\n",
        "\n",
        " def mc_body_p3_p2(idx, p):\n",
        "    _, _xcoarse, _xfine, sigma,mu,T,K = tf.while_loop(lambda _idx, s, xfine, sigma,mu,T,K: _idx < N_p3_p2,\n",
        "                          lambda _idx, s, xfine, sigma,mu,T,K: sde_body_p3_p2(_idx, s, xfine,sigma,mu,T,K,\n",
        "                                                   mc_samples_ref),\n",
        "                                                   loop_var_mc_p3_p2)\n",
        "    return idx + 1, p + tf.reduce_mean(phi(_xfine,sigma,mu,T,K, 2), axis=0) -tf.reduce_mean(phi(_xcoarse,sigma,mu,T,K, 2), axis=0) \n",
        "#    return idx + 1, p + tf.reduce_mean(phi(_xcoarse, 2), axis=0)   \n",
        "\n",
        " def mc_body_p4_p3(idx, p):\n",
        "    _, _xcoarse, _xfine, sigma,mu,T,K = tf.while_loop(lambda _idx, s, xfine, sigma,mu,T,K: _idx < N_p4_p3,\n",
        "                          lambda _idx, s, xfine, sigma,mu,T,K: sde_body_p4_p3(_idx, s, xfine,sigma,mu,T,K,\n",
        "                                                   mc_samples_ref),\n",
        "                                                   loop_var_mc_p4_p3)\n",
        "    return idx + 1, p + tf.reduce_mean(phi(_xfine,sigma,mu,T,K, 2), axis=0) -tf.reduce_mean(phi(_xcoarse,sigma,mu,T,K, 2), axis=0) \n",
        "#    return idx + 1, p + tf.reduce_mean(phi(_xcoarse, 2), axis=0)  \n",
        "\n",
        " def mc_body_p5_p4(idx, p):\n",
        "    _, _xcoarse, _xfine, sigma,mu,T,K = tf.while_loop(lambda _idx, s, xfine, sigma,mu,T,K: _idx < N_p5_p4,\n",
        "                          lambda _idx, s, xfine, sigma,mu,T,K: sde_body_p5_p4(_idx, s, xfine,sigma,mu,T,K,\n",
        "                                                   mc_samples_ref),\n",
        "                                                   loop_var_mc_p5_p4)\n",
        "    return idx + 1, p + tf.reduce_mean(phi(_xfine,sigma,mu,T,K, 2), axis=0) -tf.reduce_mean(phi(_xcoarse,sigma,mu,T,K, 2), axis=0) \n",
        "#    return idx + 1, p + tf.reduce_mean(phi(_xcoarse, 2), axis=0)  \n",
        "\n",
        " def mc_body_p6_p5(idx, p):\n",
        "    _, _xcoarse, _xfine, sigma,mu,T,K = tf.while_loop(lambda _idx, s, xfine, sigma,mu,T,K: _idx < N_p6_p5,\n",
        "                          lambda _idx, s, xfine, sigma,mu,T,K: sde_body_p6_p5(_idx, s, xfine,sigma,mu,T,K,\n",
        "                                                   mc_samples_ref),\n",
        "                                                   loop_var_mc_p6_p5)\n",
        "    return idx + 1, p + tf.reduce_mean(phi(_xfine,sigma,mu,T,K, 2), axis=0) -tf.reduce_mean(phi(_xcoarse,sigma,mu,T,K, 2), axis=0) \n",
        "#    return idx + 1, p + tf.reduce_mean(phi(_xcoarse, 2), axis=0)  \n",
        "\n",
        " def mc_body_p7_p6(idx, p):\n",
        "    _, _xcoarse, _xfine, sigma,mu,T,K = tf.while_loop(lambda _idx, s, xfine, sigma,mu,T,K: _idx < N_p7_p6,\n",
        "                          lambda _idx, s, xfine, sigma,mu,T,K: sde_body_p7_p6(_idx, s, xfine,sigma,mu,T,K,\n",
        "                                                   mc_samples_ref),\n",
        "                                                   loop_var_mc_p7_p6)\n",
        "    return idx + 1, p + tf.reduce_mean(phi(_xfine,sigma,mu,T,K, 2), axis=0) -tf.reduce_mean(phi(_xcoarse,sigma,mu,T,K, 2), axis=0) \n",
        "#    return idx + 1, p + tf.reduce_mean(phi(_xcoarse, 2), axis=0)  \n",
        "\n",
        " def mc_body_p8_p7(idx, p):\n",
        "    _, _xcoarse, _xfine, sigma,mu,T,K = tf.while_loop(lambda _idx, s, xfine, sigma,mu,T,K: _idx < N_p8_p7,\n",
        "                          lambda _idx, s, xfine, sigma,mu,T,K: sde_body_p8_p7(_idx, s, xfine,sigma,mu,T,K,\n",
        "                                                   mc_samples_ref),\n",
        "                                                   loop_var_mc_p8_p7)\n",
        "    return idx + 1, p + tf.reduce_mean(phi(_xfine,sigma,mu,T,K, 2), axis=0) -tf.reduce_mean(phi(_xcoarse,sigma,mu,T,K, 2), axis=0) \n",
        "#    return idx + 1, p + tf.reduce_mean(phi(_xcoarse, 2), axis=0)  \n",
        "#E[P_1-P_0]  Referenz (mehrere Pfade)\n",
        "#\n",
        "\n",
        "# def mc_body_ref_p0(idx2, p):\n",
        "#    _, _xcoarse, _xfine, sigma,mu,T,K = tf.while_loop(lambda _idx, s, xfine, sigma,mu,T,K: _idx < N_p0,\n",
        "#                          lambda _idx, s, xfine, sigma,mu,T,K: sde_body_ref_p0(_idx, s, xfine,sigma,mu,T,K,\n",
        "#                                                   mc_samples_ref_p0),\n",
        "#                                                   loop_var_mc_ref_p0)\n",
        "#    return idx2 + 1, p + tf.reduce_mean(phi(_xcoarse,sigma,mu,T,K, 2), axis=0) \n",
        "\n",
        " #def mc_body_p1_p0(idx, p):\n",
        " #   _, _xcoarse, _xfine, sigma,mu,T,K = tf.while_loop(lambda _idx, s, xfine, sigma,mu,T,K: _idx < N_p1_p0,\n",
        " #                         lambda _idx, s, xfine, sigma,mu,T,K: sde_body_p1_p0(_idx, s, xfine,sigma,mu,T,K,\n",
        " #                                                  mc_samples_ref),\n",
        " #                                                  loop_var_mc_p1_p0)\n",
        " #   return idx + 1, p + tf.reduce_mean(phi(_xfine,sigma,mu,T,K, 2)-phi(_xcoarse,sigma,mu,T,K, 2), axis=0) \n",
        "\n",
        " def mc_body_ref_p1_p0(idx, p):\n",
        "    _, _xcoarse, _xfine, sigma,mu,T,K = tf.while_loop(lambda _idx, s, xfine, sigma,mu,T,K: _idx < N_p1_p0,\n",
        "                          lambda _idx, s, xfine, sigma,mu,T,K: sde_body_ref_p1_p0(_idx, s, xfine,sigma,mu,T,K,\n",
        "                                                   mc_samples_ref),\n",
        "                                                   loop_var_mc_ref_p1_p0)\n",
        "    return idx + 1, p + tf.reduce_mean(phi(_xfine,sigma,mu,T,K, 2)-phi(_xcoarse,sigma,mu,T,K, 2), axis=0)  \n",
        "                            \n",
        " def sde_body_ref_p1_p0(idx, s, sfine,sigma,mu,T,K, samples): #Milstein Scheme\n",
        "    z1 = tf.random_normal(shape=(samples, batch_size_approx, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z2 = tf.random_normal(shape=(samples, batch_size_approx, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z=(z1+z2)/tf.sqrt(2.)\n",
        "    #Milstein\n",
        "    #feiner Pfad\n",
        "    h_p1_p0_coarse= T / (N_p1_p0)\n",
        "    h_p1_p0_fine= T / (N_p1_p0*2)\n",
        "    hfine=h_p1_p0_fine\n",
        "    hcoarse=h_p1_p0_coarse\n",
        "\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z1 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z1)**2-hfine)\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z2 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z2)**2-hfine)\n",
        "    #grober Pfad\n",
        "    s=s + mu *s * hcoarse +sigma * s *tf.sqrt(hcoarse)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(hcoarse)*z)**2-hcoarse)    \n",
        "    return tf.add(idx, 1), s, sfine, sigma,mu,T,K   \n",
        "\n",
        " def mc_body_ref_p2_p1(idx, p):\n",
        "    _, _xcoarse, _xfine, sigma,mu,T,K = tf.while_loop(lambda _idx, s, xfine, sigma,mu,T,K: _idx < N_p2_p1,\n",
        "                          lambda _idx, s, xfine, sigma,mu,T,K: sde_body_ref_p2_p1(_idx, s, xfine,sigma,mu,T,K,\n",
        "                                                   mc_samples_ref),\n",
        "                                                   loop_var_mc_ref_p2_p1)\n",
        "    return idx + 1, p + tf.reduce_mean(phi(_xfine,sigma,mu,T,K, 2)-phi(_xcoarse,sigma,mu,T,K, 2), axis=0)  \n",
        "                            \n",
        " def sde_body_ref_p2_p1(idx, s, sfine,sigma,mu,T,K, samples): #Milstein Scheme\n",
        "    z1 = tf.random_normal(shape=(samples, batch_size_approx, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z2 = tf.random_normal(shape=(samples, batch_size_approx, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z=(z1+z2)/tf.sqrt(2.)\n",
        "    #Milstein\n",
        "    #feiner Pfad\n",
        "    h_p2_p1_coarse= T / (N_p2_p1)\n",
        "    h_p2_p1_fine= T / (N_p2_p1*2)\n",
        "    hfine=h_p2_p1_fine\n",
        "    hcoarse=h_p2_p1_coarse\n",
        "\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z1 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z1)**2-hfine)\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z2 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z2)**2-hfine)\n",
        "    #grober Pfad\n",
        "    s=s + mu *s * hcoarse +sigma * s *tf.sqrt(hcoarse)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(hcoarse)*z)**2-hcoarse)    \n",
        "    return tf.add(idx, 1), s, sfine, sigma,mu,T,K   \n",
        "\n",
        " def mc_body_ref_p3_p2(idx, p):\n",
        "    _, _xcoarse, _xfine, sigma,mu,T,K = tf.while_loop(lambda _idx, s, xfine, sigma,mu,T,K: _idx < N_p3_p2,\n",
        "                          lambda _idx, s, xfine, sigma,mu,T,K: sde_body_ref_p3_p2(_idx, s, xfine,sigma,mu,T,K,\n",
        "                                                   mc_samples_ref),\n",
        "                                                   loop_var_mc_ref_p3_p2)\n",
        "    return idx + 1, p + tf.reduce_mean(phi(_xfine,sigma,mu,T,K, 2)-phi(_xcoarse,sigma,mu,T,K, 2), axis=0)  \n",
        "                            \n",
        " def sde_body_ref_p3_p2(idx, s, sfine,sigma,mu,T,K, samples): #Milstein Scheme\n",
        "    z1 = tf.random_normal(shape=(samples, batch_size_approx, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z2 = tf.random_normal(shape=(samples, batch_size_approx, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z=(z1+z2)/tf.sqrt(2.)\n",
        "    #Milstein\n",
        "    #feiner Pfad\n",
        "    h_p3_p2_coarse= T / (N_p3_p2)\n",
        "    h_p3_p2_fine= T / (N_p3_p2*2)\n",
        "    hfine=h_p3_p2_fine\n",
        "    hcoarse=h_p3_p2_coarse\n",
        "\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z1 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z1)**2-hfine)\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z2 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z2)**2-hfine)\n",
        "    #grober Pfad\n",
        "    s=s + mu *s * hcoarse +sigma * s *tf.sqrt(hcoarse)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(hcoarse)*z)**2-hcoarse)    \n",
        "    return tf.add(idx, 1), s, sfine, sigma,mu,T,K   \n",
        "\n",
        " def mc_body_ref_p4_p3(idx, p):\n",
        "    _, _xcoarse, _xfine, sigma,mu,T,K = tf.while_loop(lambda _idx, s, xfine, sigma,mu,T,K: _idx < N_p4_p3,\n",
        "                          lambda _idx, s, xfine, sigma,mu,T,K: sde_body_ref_p4_p3(_idx, s, xfine,sigma,mu,T,K,\n",
        "                                                   mc_samples_ref),\n",
        "                                                   loop_var_mc_ref_p4_p3)\n",
        "    return idx + 1, p + tf.reduce_mean(phi(_xfine,sigma,mu,T,K, 2)-phi(_xcoarse,sigma,mu,T,K, 2), axis=0)  \n",
        "                            \n",
        " def sde_body_ref_p4_p3(idx, s, sfine,sigma,mu,T,K, samples): #Milstein Scheme\n",
        "    z1 = tf.random_normal(shape=(samples, batch_size_approx, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z2 = tf.random_normal(shape=(samples, batch_size_approx, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z=(z1+z2)/tf.sqrt(2.)\n",
        "    #Milstein\n",
        "    #feiner Pfad\n",
        "    h_p4_p3_coarse= T / (N_p4_p3)\n",
        "    h_p4_p3_fine= T / (N_p4_p3*2)\n",
        "    hfine=h_p4_p3_fine\n",
        "    hcoarse=h_p4_p3_coarse\n",
        "\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z1 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z1)**2-hfine)\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z2 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z2)**2-hfine)\n",
        "    #grober Pfad\n",
        "    s=s + mu *s * hcoarse +sigma * s *tf.sqrt(hcoarse)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(hcoarse)*z)**2-hcoarse)    \n",
        "    return tf.add(idx, 1), s, sfine, sigma,mu,T,K   \n",
        "\n",
        " def mc_body_ref_p5_p4(idx, p):\n",
        "    _, _xcoarse, _xfine, sigma,mu,T,K = tf.while_loop(lambda _idx, s, xfine, sigma,mu,T,K: _idx < N_p5_p4,\n",
        "                          lambda _idx, s, xfine, sigma,mu,T,K: sde_body_ref_p5_p4(_idx, s, xfine,sigma,mu,T,K,\n",
        "                                                   mc_samples_ref),\n",
        "                                                   loop_var_mc_ref_p5_p4)\n",
        "    return idx + 1, p + tf.reduce_mean(phi(_xfine,sigma,mu,T,K, 2)-phi(_xcoarse,sigma,mu,T,K, 2), axis=0)  \n",
        "                            \n",
        " def sde_body_ref_p5_p4(idx, s, sfine,sigma,mu,T,K, samples): #Milstein Scheme\n",
        "    z1 = tf.random_normal(shape=(samples, batch_size_approx, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z2 = tf.random_normal(shape=(samples, batch_size_approx, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z=(z1+z2)/tf.sqrt(2.)\n",
        "    #Milstein\n",
        "    #feiner Pfad\n",
        "    h_p5_p4_coarse= T / (N_p5_p4)\n",
        "    h_p5_p4_fine= T / (N_p5_p4*2)\n",
        "    hfine=h_p5_p4_fine\n",
        "    hcoarse=h_p5_p4_coarse\n",
        "\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z1 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z1)**2-hfine)\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z2 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z2)**2-hfine)\n",
        "    #grober Pfad\n",
        "    s=s + mu *s * hcoarse +sigma * s *tf.sqrt(hcoarse)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(hcoarse)*z)**2-hcoarse)    \n",
        "    return tf.add(idx, 1), s, sfine, sigma,mu,T,K   \n",
        "\n",
        " def mc_body_ref_p6_p5(idx, p):\n",
        "    _, _xcoarse, _xfine, sigma,mu,T,K = tf.while_loop(lambda _idx, s, xfine, sigma,mu,T,K: _idx < N_p6_p5,\n",
        "                          lambda _idx, s, xfine, sigma,mu,T,K: sde_body_ref_p6_p5(_idx, s, xfine,sigma,mu,T,K,\n",
        "                                                   mc_samples_ref),\n",
        "                                                   loop_var_mc_ref_p6_p5)\n",
        "    return idx + 1, p + tf.reduce_mean(phi(_xfine,sigma,mu,T,K, 2)-phi(_xcoarse,sigma,mu,T,K, 2), axis=0)  \n",
        "                            \n",
        " def sde_body_ref_p6_p5(idx, s, sfine,sigma,mu,T,K, samples): #Milstein Scheme\n",
        "    z1 = tf.random_normal(shape=(samples, batch_size_approx, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z2 = tf.random_normal(shape=(samples, batch_size_approx, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z=(z1+z2)/tf.sqrt(2.)\n",
        "    #Milstein\n",
        "    #feiner Pfad\n",
        "    h_p6_p5_coarse= T / (N_p6_p5)\n",
        "    h_p6_p5_fine= T / (N_p6_p5*2)\n",
        "    hfine=h_p6_p5_fine\n",
        "    hcoarse=h_p6_p5_coarse\n",
        "\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z1 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z1)**2-hfine)\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z2 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z2)**2-hfine)\n",
        "    #grober Pfad\n",
        "    s=s + mu *s * hcoarse +sigma * s *tf.sqrt(hcoarse)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(hcoarse)*z)**2-hcoarse)    \n",
        "    return tf.add(idx, 1), s, sfine, sigma,mu,T,K   \n",
        "\n",
        " def mc_body_ref_p7_p6(idx, p):\n",
        "    _, _xcoarse, _xfine, sigma,mu,T,K = tf.while_loop(lambda _idx, s, xfine, sigma,mu,T,K: _idx < N_p7_p6,\n",
        "                          lambda _idx, s, xfine, sigma,mu,T,K: sde_body_ref_p7_p6(_idx, s, xfine,sigma,mu,T,K,\n",
        "                                                   mc_samples_ref),\n",
        "                                                   loop_var_mc_ref_p7_p6)\n",
        "    return idx + 1, p + tf.reduce_mean(phi(_xfine,sigma,mu,T,K, 2)-phi(_xcoarse,sigma,mu,T,K, 2), axis=0)  \n",
        "                            \n",
        " def sde_body_ref_p7_p6(idx, s, sfine,sigma,mu,T,K, samples): #Milstein Scheme\n",
        "    z1 = tf.random_normal(shape=(samples, batch_size_approx, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z2 = tf.random_normal(shape=(samples, batch_size_approx, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z=(z1+z2)/tf.sqrt(2.)\n",
        "    #Milstein\n",
        "    #feiner Pfad\n",
        "    h_p7_p6_coarse= T / (N_p7_p6)\n",
        "    h_p7_p6_fine= T / (N_p7_p6*2)\n",
        "    hfine=h_p7_p6_fine\n",
        "    hcoarse=h_p7_p6_coarse\n",
        "\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z1 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z1)**2-hfine)\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z2 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z2)**2-hfine)\n",
        "    #grober Pfad\n",
        "    s=s + mu *s * hcoarse +sigma * s *tf.sqrt(hcoarse)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(hcoarse)*z)**2-hcoarse)    \n",
        "    return tf.add(idx, 1), s, sfine, sigma,mu,T,K   \n",
        "\n",
        "\n",
        " def sde_body_ref_p0(idx, s,sigma,mu,T,K, samples): #Milstein Scheme\n",
        "    z = tf.random_normal(shape=(samples, batch_size_approx, 1),\n",
        "                         stddev=1., dtype=dtype)\n",
        "    h=T/N\n",
        "    s=s + mu *s * h +sigma * s *tf.sqrt(h)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(h)*z)**2-h)\n",
        "    return tf.add(idx, 1), s, sigma,mu,T,K\n",
        "\n",
        " def mc_body_ref_p0(idx, p):\n",
        "    _, _x, _sigma,_mu,_T,_K = tf.while_loop(lambda _idx, s, sigma,mu,T,K: _idx < N,\n",
        "                          lambda _idx, s, sigma,mu,T,K: sde_body_ref_p0(_idx, s, sigma,mu,T,K,\n",
        "                                                   mc_samples_ref),\n",
        "                                                   loop_var_mc_ref_p0)\n",
        "#    return idx + 1, p + tf.reduce_mean(phi(_xfine, 2), axis=0) -tf.reduce_mean(phi(_xcoarse, 2), axis=0) \n",
        "    return idx + 1, p + tf.reduce_mean(phi(_x,_sigma,_mu,_T,_K, 2), axis=0)  \n",
        "\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "       \n",
        " def phi(x,sigma,mu,T,K, axis=1):\n",
        "    payoff=tf.exp(-mu * T)* tf.maximum(x - K, 0.)\n",
        "    return payoff\n",
        "\n",
        "#E[P_0] ein Pfad\n",
        "#\n",
        " def sde_body_p0(idx, s,sigma,mu,T,K, samples): #Milstein Scheme\n",
        "    z = tf.random_normal(shape=(samples, batch_size, 1),\n",
        "                         stddev=1., dtype=dtype)\n",
        "    h=T/N\n",
        "    s=s + mu *s * h +sigma * s *tf.sqrt(h)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(h)*z)**2-h)\n",
        "    return tf.add(idx, 1), s, sigma,mu,T,K\n",
        "\n",
        " def mc_body_p0(idx, p):\n",
        "    _, _x, _sigma,_mu,_T,_K = tf.while_loop(lambda _idx, s, sigma,mu,T,K: _idx < N,\n",
        "                          lambda _idx, s, sigma,mu,T,K: sde_body_p0(_idx, s, sigma,mu,T,K,\n",
        "                                                   mc_samples_ref),\n",
        "                                                   loop_var_mc_p0)\n",
        "#    return idx + 1, p + tf.reduce_mean(phi(_xfine, 2), axis=0) -tf.reduce_mean(phi(_xcoarse, 2), axis=0) \n",
        "    return idx + 1, p + tf.reduce_mean(phi(_x,_sigma,_mu,_T,_K, 2), axis=0)  \n",
        "\n",
        "#Erklaerung loop_var_mc: mc body fuer u_reference mit mc_samples_ref\n",
        "#loop_var_p0_ref = (tf.constant(0),tf.ones((mc_samples_ref,batch_size_approx, d), dtype) * xi_approx)\n",
        " loop_var_mc_p0 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size, 1), dtype) * s0, tf.ones((mc_samples_ref,batch_size, 1), dtype) * sigma,tf.ones((mc_samples_ref,batch_size, 1), dtype) * mu,tf.ones((mc_samples_ref,batch_size, 1), dtype) * T,tf.ones((mc_samples_ref,batch_size, 1), dtype) * K)\n",
        " #loop_var_mc_p0 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size, d), dtype) * xi)\n",
        " #loop_var_mc_p1_p0 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size_p1_p0, 1), dtype) * s0_p1_p0, tf.ones((mc_samples_ref,batch_size_p1_p0, 1), dtype) * sigma_p1_p0,tf.ones((mc_samples_ref,batch_size_p1_p0, 1), dtype) * mu_p1_p0,tf.ones((mc_samples_ref,batch_size_p1_p0, 1), dtype) * T_p1_p0,tf.ones((mc_samples_ref,batch_size_p1_p0, 1), dtype) * K_p1_p0)\n",
        " #oop_var_mc_p1 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size_p1_p0, 1), dtype) * s0_p1_p0, tf.ones((mc_samples_ref,batch_size_p1_p0, 1), dtype) * sigma_p1_p0,tf.ones((mc_samples_ref,batch_size_p1_p0, 1), dtype) * mu_p1_p0,tf.ones((mc_samples_ref,batch_size_p1_p0, 1), dtype) * T_p1_p0,tf.ones((mc_samples_ref,batch_size_p1_p0, 1), dtype) * K_p1_p0)\n",
        " #loop_var_mc_p1 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size_p1_p0, d), dtype) * xi_p1_p0)\n",
        " \n",
        " loop_var_mc_p1_p0 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size_p1_p0, 1), dtype) * s0_p1_p0,tf.ones((mc_samples_ref,batch_size_p1_p0, 1), dtype) * s0_p1_p0, tf.ones((mc_samples_ref,batch_size_p1_p0, 1), dtype) * sigma_p1_p0,tf.ones((mc_samples_ref,batch_size_p1_p0, 1), dtype) * mu_p1_p0,tf.ones((mc_samples_ref,batch_size_p1_p0, 1), dtype) * T_p1_p0,tf.ones((mc_samples_ref,batch_size_p1_p0, 1), dtype) * K_p1_p0)\n",
        " loop_var_mc_p2_p1 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size_p2_p1, 1), dtype) * s0_p2_p1,tf.ones((mc_samples_ref,batch_size_p2_p1, 1), dtype) * s0_p2_p1, tf.ones((mc_samples_ref,batch_size_p2_p1, 1), dtype) * sigma_p2_p1,tf.ones((mc_samples_ref,batch_size_p2_p1, 1), dtype) * mu_p2_p1,tf.ones((mc_samples_ref,batch_size_p2_p1, 1), dtype) * T_p2_p1,tf.ones((mc_samples_ref,batch_size_p2_p1, 1), dtype) * K_p2_p1)\n",
        " loop_var_mc_p3_p2 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size_p3_p2, 1), dtype) * s0_p3_p2,tf.ones((mc_samples_ref,batch_size_p3_p2, 1), dtype) * s0_p3_p2, tf.ones((mc_samples_ref,batch_size_p3_p2, 1), dtype) * sigma_p3_p2,tf.ones((mc_samples_ref,batch_size_p3_p2, 1), dtype) * mu_p3_p2,tf.ones((mc_samples_ref,batch_size_p3_p2, 1), dtype) * T_p3_p2,tf.ones((mc_samples_ref,batch_size_p3_p2, 1), dtype) * K_p3_p2)\n",
        " loop_var_mc_p4_p3 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size_p4_p3, 1), dtype) * s0_p4_p3,tf.ones((mc_samples_ref,batch_size_p4_p3, 1), dtype) * s0_p4_p3, tf.ones((mc_samples_ref,batch_size_p4_p3, 1), dtype) * sigma_p4_p3,tf.ones((mc_samples_ref,batch_size_p4_p3, 1), dtype) * mu_p4_p3,tf.ones((mc_samples_ref,batch_size_p4_p3, 1), dtype) * T_p4_p3,tf.ones((mc_samples_ref,batch_size_p4_p3, 1), dtype) * K_p4_p3)\n",
        " loop_var_mc_p5_p4 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size_p5_p4, 1), dtype) * s0_p5_p4,tf.ones((mc_samples_ref,batch_size_p5_p4, 1), dtype) * s0_p5_p4, tf.ones((mc_samples_ref,batch_size_p5_p4, 1), dtype) * sigma_p5_p4,tf.ones((mc_samples_ref,batch_size_p5_p4, 1), dtype) * mu_p5_p4,tf.ones((mc_samples_ref,batch_size_p5_p4, 1), dtype) * T_p5_p4,tf.ones((mc_samples_ref,batch_size_p5_p4, 1), dtype) * K_p5_p4)\n",
        " loop_var_mc_p6_p5 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size_p6_p5, 1), dtype) * s0_p6_p5,tf.ones((mc_samples_ref,batch_size_p6_p5, 1), dtype) * s0_p6_p5, tf.ones((mc_samples_ref,batch_size_p6_p5, 1), dtype) * sigma_p6_p5,tf.ones((mc_samples_ref,batch_size_p6_p5, 1), dtype) * mu_p6_p5,tf.ones((mc_samples_ref,batch_size_p6_p5, 1), dtype) * T_p6_p5,tf.ones((mc_samples_ref,batch_size_p6_p5, 1), dtype) * K_p6_p5)\n",
        " loop_var_mc_p7_p6 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size_p7_p6, 1), dtype) * s0_p7_p6,tf.ones((mc_samples_ref,batch_size_p7_p6, 1), dtype) * s0_p7_p6, tf.ones((mc_samples_ref,batch_size_p7_p6, 1), dtype) * sigma_p7_p6,tf.ones((mc_samples_ref,batch_size_p7_p6, 1), dtype) * mu_p7_p6,tf.ones((mc_samples_ref,batch_size_p7_p6, 1), dtype) * T_p7_p6,tf.ones((mc_samples_ref,batch_size_p7_p6, 1), dtype) * K_p7_p6)\n",
        " loop_var_mc_p8_p7 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size_p8_p7, 1), dtype) * s0_p8_p7,tf.ones((mc_samples_ref,batch_size_p8_p7, 1), dtype) * s0_p8_p7, tf.ones((mc_samples_ref,batch_size_p8_p7, 1), dtype) * sigma_p8_p7,tf.ones((mc_samples_ref,batch_size_p8_p7, 1), dtype) * mu_p8_p7,tf.ones((mc_samples_ref,batch_size_p8_p7, 1), dtype) * T_p8_p7,tf.ones((mc_samples_ref,batch_size_p8_p7, 1), dtype) * K_p8_p7)\n",
        "\n",
        " #loop_var_mc_p1_p0 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size_p1_p0, d), dtype) * xi_p1_p0, tf.ones((mc_samples_ref,batch_size_p1_p0, d), dtype) *xi_p1_p0)\n",
        " #loop_var_mc_p2_p1 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size_p2_p1, d), dtype) * xi_p2_p1, tf.ones((mc_samples_ref,batch_size_p2_p1, d), dtype) *xi_p2_p1)\n",
        " #loop_var_mc_p3_p2 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size_p3_p2, d), dtype) * xi_p3_p2, tf.ones((mc_samples_ref,batch_size_p3_p2, d), dtype) *xi_p3_p2)\n",
        " #loop_var_mc_p4_p3 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size_p4_p3, d), dtype) * xi_p4_p3, tf.ones((mc_samples_ref,batch_size_p4_p3, d), dtype) *xi_p4_p3)\n",
        " #loop_var_mc_p5_p4 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size_p5_p4, d), dtype) * xi_p5_p4, tf.ones((mc_samples_ref,batch_size_p5_p4, d), dtype) *xi_p5_p4)\n",
        " #loop_var_mc_p6_p5 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size_p6_p5, d), dtype) * xi_p6_p5, tf.ones((mc_samples_ref,batch_size_p6_p5, d), dtype) *xi_p6_p5)\n",
        " #loop_var_mc_p7_p6 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size_p7_p6, d), dtype) * xi_p7_p6, tf.ones((mc_samples_ref,batch_size_p7_p6, d), dtype) *xi_p7_p6)\n",
        " #loop_var_mc_p8_p7 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size_p8_p7, d), dtype) * xi_p8_p7, tf.ones((mc_samples_ref,batch_size_p8_p7, d), dtype) *xi_p8_p7)\n",
        " \n",
        " loop_var_mc_ref_p0 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * s0_approx, tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * sigma_approx,tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * mu_approx,tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * T_approx,tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * K_approx)\n",
        " loop_var_mc_ref_p1_p0 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * s0_approx,tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * s0_approx, tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * sigma_approx,tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * mu_approx,tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * T_approx,tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * K_approx)\n",
        " loop_var_mc_ref_p2_p1 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * s0_approx,tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * s0_approx, tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * sigma_approx,tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * mu_approx,tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * T_approx,tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * K_approx)\n",
        " loop_var_mc_ref_p3_p2 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * s0_approx,tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * s0_approx, tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * sigma_approx,tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * mu_approx,tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * T_approx,tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * K_approx)\n",
        " loop_var_mc_ref_p4_p3 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * s0_approx,tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * s0_approx, tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * sigma_approx,tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * mu_approx,tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * T_approx,tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * K_approx)\n",
        " loop_var_mc_ref_p5_p4 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * s0_approx,tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * s0_approx, tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * sigma_approx,tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * mu_approx,tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * T_approx,tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * K_approx)\n",
        " loop_var_mc_ref_p6_p5 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * s0_approx,tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * s0_approx, tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * sigma_approx,tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * mu_approx,tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * T_approx,tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * K_approx)\n",
        " loop_var_mc_ref_p7_p6 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * s0_approx,tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * s0_approx, tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * sigma_approx,tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * mu_approx,tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * T_approx,tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * K_approx)\n",
        " loop_var_mc_ref_p8_p7 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * s0_approx,tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * s0_approx, tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * sigma_approx,tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * mu_approx,tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * T_approx,tf.ones((mc_samples_ref,batch_size_approx, 1), dtype) * K_approx)\n",
        "\n",
        "\n",
        "#Erklaerung: mc body fuer x_sde mit nur einem sample immer\n",
        " #loop_var = (tf.constant(0), tf.ones((1,batch_size, 1), dtype) * xi)\n",
        "\n",
        " _, x_sde,sigma,mu,T,K = tf.while_loop(lambda idx, s, sigma,mu,T,K: idx < N,\n",
        "                         lambda idx, s, sigma,mu,T,K: sde_body_p0(idx, s,sigma,mu,T,K, 1),\n",
        "                         loop_var_mc_p0)\n",
        " _, u_p0  = tf.while_loop(lambda idx, p: idx < 1, mc_body_p0,\n",
        "                      (tf.constant(0), tf.zeros((batch_size, 1), dtype)))\n",
        "#_, u_ref_p0  = tf.while_loop(lambda idx, p: idx < mc_rounds_ref_p0, mc_body_p0_ref,\n",
        "#                      (tf.constant(0), tf.zeros((batch_size_approx, d), dtype)))\n",
        "\n",
        " #_, u_p1 =  tf.while_loop(lambda idx, p: idx < 1, mc_body_p1,\n",
        " #                     (tf.constant(0), tf.zeros((batch_size_p1_p0, d), dtype)))\n",
        " u_mc_p0 = u_p0 / tf.cast(1, tf.float32)\n",
        " #u_mc_p1 = u_p1 / tf.cast(1, tf.float32)\n",
        "\n",
        " _, u_p1_p0 = tf.while_loop(lambda idx, p: idx < 1, mc_body_p1_p0,\n",
        "                      (tf.constant(0), tf.zeros((batch_size_p1_p0, 1), dtype)))\n",
        " u_mc_p1_p0 = u_p1_p0 / tf.cast(1, tf.float32)\n",
        "\n",
        "\n",
        " _, u_p2_p1 = tf.while_loop(lambda idx, p: idx < 1, mc_body_p2_p1,\n",
        "                      (tf.constant(0), tf.zeros((batch_size_p2_p1, 1), dtype)))\n",
        " u_mc_p2_p1 = u_p2_p1 / tf.cast(1, tf.float32)\n",
        "\n",
        "\n",
        " _, u_p3_p2 = tf.while_loop(lambda idx, p: idx < 1, mc_body_p3_p2,\n",
        "                      (tf.constant(0), tf.zeros((batch_size_p3_p2, 1), dtype)))\n",
        " u_mc_p3_p2 = u_p3_p2 / tf.cast(1, tf.float32)\n",
        "\n",
        " _, u_p4_p3 = tf.while_loop(lambda idx, p: idx < 1, mc_body_p4_p3,\n",
        "                      (tf.constant(0), tf.zeros((batch_size_p4_p3, 1), dtype)))\n",
        " u_mc_p4_p3 = u_p4_p3 / tf.cast(1, tf.float32)\n",
        "\n",
        " _, u_p5_p4 = tf.while_loop(lambda idx, p: idx < 1, mc_body_p5_p4,\n",
        "                      (tf.constant(0), tf.zeros((batch_size_p5_p4, 1), dtype)))\n",
        " u_mc_p5_p4 = u_p5_p4 / tf.cast(1, tf.float32)\n",
        "\n",
        " _, u_p6_p5 = tf.while_loop(lambda idx, p: idx < 1, mc_body_p6_p5,\n",
        "                      (tf.constant(0), tf.zeros((batch_size_p6_p5, 1), dtype)))\n",
        " u_mc_p6_p5 = u_p6_p5 / tf.cast(1, tf.float32)\n",
        "\n",
        " _, u_p7_p6 = tf.while_loop(lambda idx, p: idx < 1, mc_body_p7_p6,\n",
        "                      (tf.constant(0), tf.zeros((batch_size_p7_p6, 1), dtype)))\n",
        " u_mc_p7_p6 = u_p7_p6 / tf.cast(1, tf.float32)\n",
        "\n",
        " _, u_p8_p7 = tf.while_loop(lambda idx, p: idx < 1, mc_body_p8_p7,\n",
        "                      (tf.constant(0), tf.zeros((batch_size_p8_p7, 1), dtype)))\n",
        " u_mc_p8_p7 = u_p8_p7 / tf.cast(1, tf.float32)\n",
        "\n",
        " #_, u_ref_p1_p0 = tf.while_loop(lambda idx, p: idx < mc_rounds_ref_p1_p0, mc_body_ref_p1_p0,\n",
        "#                      (tf.constant(0), tf.zeros((batch_size_approx, d), dtype)))\n",
        "\n",
        "\n",
        "#Approx-Fehler\n",
        "#\n",
        "#u_reference_p0=u_ref_p0 / tf.cast(mc_rounds_ref_p0, tf.float32)# tf.multiply(xi_approx,(dist.cdf(d1)))-K*np.exp(-mu*T)*(dist.cdf(d2))\n",
        "#u_reference_p1= \n",
        "#u_reference_p1_p0= u_ref_p1_p0 / tf.cast(mc_rounds_ref_p1_p0, tf.float32)\n",
        "\n",
        "#echter Fehler:\n",
        " N_approx=10000\n",
        " N_approx_ref_p0=10\n",
        " N_approx_geradenicht=1\n",
        " _, u_p0_ref = tf.while_loop(lambda idx2, p: idx2 < N_approx_ref_p0, mc_body_ref_p0,\n",
        "                      (tf.constant(0), tf.zeros((batch_size_approx, 1), dtype)))\n",
        " u_mc_p0_ref = u_p0_ref / tf.cast(N_approx_ref_p0, tf.float32)\n",
        " _, u_p1_p0_ref = tf.while_loop(lambda idx, p: idx < N_approx_geradenicht, mc_body_ref_p1_p0,\n",
        "                      (tf.constant(0), tf.zeros((batch_size_approx, 1), dtype)))\n",
        " u_mc_p1_p0_ref = u_p1_p0_ref / tf.cast(N_approx_geradenicht, tf.float32)\n",
        " _, u_p2_p1_ref = tf.while_loop(lambda idx, p: idx < N_approx_geradenicht, mc_body_ref_p2_p1,\n",
        "                       (tf.constant(0), tf.zeros((batch_size_approx, 1), dtype)))\n",
        " u_mc_p2_p1_ref = u_p2_p1_ref / tf.cast(N_approx_geradenicht, tf.float32)\n",
        " _, u_p3_p2_ref = tf.while_loop(lambda idx, p: idx < N_approx_geradenicht, mc_body_ref_p3_p2,\n",
        "                      (tf.constant(0), tf.zeros((batch_size_approx, 1), dtype)))\n",
        " u_mc_p3_p2_ref = u_p3_p2_ref / tf.cast(N_approx_geradenicht, tf.float32)\n",
        " _, u_p4_p3_ref = tf.while_loop(lambda idx, p: idx < N_approx_geradenicht, mc_body_ref_p4_p3,\n",
        "                       (tf.constant(0), tf.zeros((batch_size_approx, 1), dtype)))\n",
        " u_mc_p4_p3_ref = u_p4_p3_ref / tf.cast(N_approx_geradenicht, tf.float32)\n",
        " _, u_p5_p4_ref = tf.while_loop(lambda idx, p: idx < N_approx_geradenicht, mc_body_ref_p5_p4,\n",
        "                      (tf.constant(0), tf.zeros((batch_size_approx, 1), dtype)))\n",
        " u_mc_p5_p4_ref = u_p5_p4_ref / tf.cast(N_approx_geradenicht, tf.float32)\n",
        " _, u_p6_p5_ref = tf.while_loop(lambda idx, p: idx < N_approx_geradenicht, mc_body_ref_p6_p5,\n",
        "                      (tf.constant(0), tf.zeros((batch_size_approx, 1), dtype)))\n",
        " u_mc_p6_p5_ref = u_p6_p5_ref / tf.cast(N_approx_geradenicht, tf.float32)\n",
        " _, u_p7_p6_ref = tf.while_loop(lambda idx, p: idx < N_approx, mc_body_ref_p7_p6,\n",
        "                      (tf.constant(0), tf.zeros((batch_size_approx, 1), dtype)))\n",
        " u_mc_p7_p6_ref = u_p7_p6_ref / tf.cast(N_approx, tf.float32)\n",
        "#\n",
        "\n",
        " u_reference_p0   =u_reference#u_mc_p0_ref#tf.multiply(s0_approx,(dist.cdf(d1)))-K_approx*tf.exp(-mu_approx*T_approx)*(dist.cdf(d2))#u_mc_p0_ref#xi_approx*0.#\n",
        " u_reference_p1_p0=xi_approx*0.#u_mc_p1_p0_ref#xi_approx*0.\n",
        " u_reference_p2_p1=xi_approx*0.#u_mc_p2_p1_ref#\n",
        " u_reference_p3_p2=xi_approx*0.#u_mc_p3_p2_ref#xi_approx*0.\n",
        " u_reference_p4_p3=xi_approx*0.#u_mc_p4_p3_ref#xi_approx*0.\n",
        " u_reference_p5_p4=xi_approx*0.#u_mc_p5_p4_ref#xi_approx*0.\n",
        " u_reference_p6_p5=xi_approx*0.#u_mc_p6_p5_ref#\n",
        " u_reference_p7_p6=xi_approx*0.#u_mc_p7_p6_ref\n",
        " u_reference_p8_p7=xi_approx*0. \n",
        "\n",
        "#delta_reference=tf.cast(dist.cdf(d1),tf.float32)\n",
        "\n",
        " file_name='output.csv'\n",
        "                        \n",
        " kolmogorov_train_and_test(xi, xi_approx, xi_p1_p0, xi_p8_p7, xi_p7_p6, xi_p6_p5, xi_p5_p4, xi_p4_p3, xi_p3_p2,xi_p2_p1, tf.squeeze(tf.reduce_mean(x_sde,0,keepdims=True), axis=0), u_mc_p0, u_mc_p8_p7, u_mc_p7_p6, u_mc_p6_p5, u_mc_p5_p4, u_mc_p4_p3, u_mc_p3_p2, u_mc_p2_p1, u_mc_p1_p0, u_reference, u_reference_p0,u_reference_p8_p7,u_reference_p7_p6,u_reference_p6_p5,u_reference_p5_p4,u_reference_p4_p3,u_reference_p3_p2,u_reference_p2_p1, u_reference_p1_p0,\n",
        "                          neurons, lr_boundaries, lr_values, train_steps,\n",
        "                          mc_rounds, mc_freq, file_name, dtype)     \n",
        "                        \n",
        "## Create & upload a file. in google drive\n",
        " uploaded = drive.CreateFile({'title': file_name})\n",
        " uploaded.SetContentFile(file_name)\n",
        " uploaded.Upload()\n",
        " print('Uploaded file with ID {}'.format(uploaded.get('id')))\n",
        "                        \n",
        "                        \n",
        "                       "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}