{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "github-single.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMSglMWV0ill8H3pO9VSnxw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielroth12/Multilevel-Monte-Carlo-learning/blob/main/github_single.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BtTim4HwVYa",
        "outputId": "f67234c2-0788-48ec-d369-e747f6f6cdd8"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "#@title\n",
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once in a notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "## Authenticate and create the PyDrive client.\n",
        "## This only needs to be done once in a notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#Netz nach kolmogorov\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt \n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import time\n",
        "from tensorflow.python.ops import init_ops\n",
        "from tensorflow.contrib.layers.python.layers import initializers\n",
        "from tensorflow.python.training.moving_averages import assign_moving_average\n",
        "def neural_net(x, xi_approx, neurons, is_training, name,\n",
        "                    mv_decay=0.9, dtype=tf.float32):\n",
        "  \n",
        "  def approx_test(): return xi_approx\n",
        "  def approx_learn(): return x\n",
        "  x = tf.cond(is_training, approx_learn,approx_test)\n",
        "\n",
        "  def _batch_normalization(_x):\n",
        "    beta = tf.get_variable('beta', [_x.get_shape()[-1]],\n",
        "                           dtype, init_ops.zeros_initializer())\n",
        "    gamma = tf.get_variable('gamma', [_x.get_shape()[-1]],\n",
        "                            dtype, init_ops.ones_initializer())\n",
        "    mv_mean = tf.get_variable('mv_mean', [_x.get_shape()[-1]],\n",
        "                              dtype, init_ops.zeros_initializer(),\n",
        "                              trainable=False)\n",
        "    mv_variance = tf.get_variable('mv_variance', [_x.get_shape()[-1]],\n",
        "                                  dtype, init_ops.ones_initializer(),\n",
        "                                  trainable=False)\n",
        "    mean, variance = tf.nn.moments(_x, [0], name='moments')\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_mean, mean,\n",
        "                                               mv_decay, True))\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_variance, variance,\n",
        "                                               mv_decay, False))\n",
        "    mean, variance = tf.cond(is_training,\n",
        "                             lambda: (mean, variance),\n",
        "                             lambda: (mv_mean, mv_variance))\n",
        "    return tf.nn.batch_normalization(_x, mean, variance,\n",
        "                                     beta, gamma, 1e-6)\n",
        "  \n",
        "  def _layer(_x, out_size, activation_fn):\n",
        "    w = tf.get_variable('weights',\n",
        "                        [_x.get_shape().as_list()[-1], out_size],\n",
        "                        dtype, initializers.xavier_initializer())\n",
        "    return activation_fn(_batch_normalization(tf.matmul(_x, w)))\n",
        "  \n",
        "  with tf.variable_scope(name):\n",
        "      x = _batch_normalization(x)\n",
        "      for i in range(len(neurons)):\n",
        "        with tf.variable_scope('layer_%i_' % (i + 1)):\n",
        "          x = _layer(x, neurons[i],\n",
        "            tf.nn.tanh if i < len(neurons)-1 else tf.identity)\n",
        "  return x\n",
        "\n",
        "\n",
        "def kolmogorov_train_and_test(xi, xi_approx, x_sde, phi, u_reference, neurons,\n",
        "          lr_boundaries, lr_values, train_steps,\n",
        "          mc_rounds, mc_freq, file_name,\n",
        "          dtype=tf.float32):\n",
        "  def _approximate_errors():\n",
        "    lr, gs = sess.run([learning_rate, global_step])\n",
        "    l1_err, l2_err, li_err = 0., 0., 0.\n",
        "    rel_l1_err, rel_l2_err, rel_li_err = 0., 0., 0.\n",
        "    for _ in range(mc_rounds):\n",
        "      plot_xi, plot_approx, plot_ref, l1, l2, li, rl1, rl2, rli, appr, ref \\\n",
        "              = sess.run([xi_approx, u_approx,u_reference,err_l_1, err_l_2, err_l_inf,\n",
        "                          rel_err_l_1, rel_err_l_2, rel_err_l_inf, approx, reference],\n",
        "                         feed_dict={is_training: False})\n",
        "      l1_err, l2_err, li_err = (l1_err + l1, l2_err + l2,\n",
        "                                  np.maximum(li_err, li))\n",
        "      rel_l1_err, rel_l2_err, rel_li_err \\\n",
        "              = (rel_l1_err + rl1, rel_l2_err + rl2,\n",
        "                 np.maximum(rel_li_err, rli))\n",
        "    l1_err, l2_err = l1_err / mc_rounds, np.sqrt(l2_err / mc_rounds)\n",
        "    rel_l1_err, rel_l2_err \\\n",
        "              = rel_l1_err / mc_rounds, np.sqrt(rel_l2_err / mc_rounds)\n",
        "    t_mc = time.time()\n",
        "    file_out.write('%i, %f, %f, %f, %f, %f, %f, '\n",
        "        '%f, %f, %f, %f, %f \\n' % (gs, l1_err, l2_err, li_err,\n",
        "        rel_l1_err, rel_l2_err, rel_li_err, lr,\n",
        "                      t1_train - t0_train, t_mc - t1_train, appr, ref)) #rel_li_argmax\n",
        "    file_out.flush()\n",
        "    \n",
        "\n",
        "  t0_train = time.time()\n",
        "  is_training = tf.placeholder(tf.bool, [])\n",
        "  u_approx = neural_net(xi, xi_approx, neurons, is_training, 'u_approx', dtype=dtype)\n",
        "  loss = tf.reduce_mean(tf.squared_difference(u_approx, phi))\n",
        "  \n",
        "  approx=tf.reduce_mean(u_approx)\n",
        "  reference=tf.reduce_mean(u_reference)\n",
        "  err = tf.abs(u_approx - u_reference)\n",
        "  err_l_1 = tf.reduce_mean(err)\n",
        "  err_l_2 = tf.reduce_mean(err ** 2)\n",
        "  err_l_inf = tf.reduce_max(err)\n",
        "  rel_err = err / tf.maximum(u_reference, 1e-4)\n",
        "  rel_err_l_1 = tf.reduce_mean(rel_err)\n",
        "  rel_err_l_2 = tf.reduce_mean(rel_err ** 2)\n",
        "  rel_err_l_inf = tf.reduce_max(rel_err)\n",
        "  std = tf.math.sqrt(tf.math.reduce_variance(phi))\n",
        "  lr=0.01#1/std\n",
        "  step_rate = 40000\n",
        "  decay = 0.1\n",
        "  global_step = tf.Variable(0, trainable=False)\n",
        "  increment_global_step = tf.assign(global_step, global_step + 1)\n",
        "  learning_rate = tf.train.exponential_decay(lr, global_step, step_rate, decay, staircase=True)\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=0.01)\n",
        "\n",
        "  update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, 'u_approx')\n",
        "  with tf.control_dependencies(update_ops):\n",
        "    train_op = optimizer.minimize(loss, global_step)\n",
        "    \n",
        "        \n",
        "  file_out = open(file_name, 'w')\n",
        "  file_out.write('step, l1_err, l2_err, li_err, l1_rel, '\n",
        "                   'l2_rel, li_rel, learning_rate, time_train, time_mc ,approx, reference \\n ')\n",
        "    \n",
        "  with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "        \n",
        "    for step in range(train_steps):\n",
        "      if step % mc_freq == 0:\n",
        "        print(step)\n",
        "        t1_train = time.time()\n",
        "        _approximate_errors()\n",
        "        t0_train = time.time()\n",
        "      sess.run(train_op, feed_dict={is_training:True})\n",
        "    t1_train = time.time()\n",
        "    _approximate_errors()\n",
        "        \n",
        "  file_out.close()\n",
        "  \n",
        "\n",
        "#Heston Modell (3.6 kolmogorov)\n",
        "for i in range(1,2):\n",
        "  print(i)\n",
        "  tf.reset_default_graph()\n",
        "  with tf.Session()  as sess:\n",
        "    dtype = tf.float32\n",
        "    batch_size =50000\n",
        "    batch_size_approx=2000000\n",
        "    N, d = 64, 5\n",
        "    neurons = [50, 50, 1]\n",
        "    train_steps = 50000\n",
        "    mc_rounds, mc_freq = 1, 10000\n",
        "    #mc_samples_ref, mc_rounds_ref = 1024, 1024\n",
        "    mc_samples_ref, mc_rounds_ref = 1, 1\n",
        "    \n",
        "    N_l=1\n",
        "    lr_boundaries = [1001,2001,3001,5001,25001]\n",
        "    lr_values = [1.0,0.1,0.01,0.001,0.0001,0.00005]\n",
        "\n",
        "    s_0_l=80.0\n",
        "    s_0_r=120.0\n",
        "    sigma_l=0.1\n",
        "    sigma_r=0.2\n",
        "    mu_l=0.02\n",
        "    mu_r=0.05\n",
        "    T_l=0.9\n",
        "    T_r=1.0\n",
        "    K_l=109.0\n",
        "    K_r=110.0\n",
        "    s_0_l_approx=80.4\n",
        "    s_0_r_approx=119.6\n",
        "    sigma_l_approx=0.11\n",
        "    sigma_r_approx=0.19\n",
        "    mu_l_approx=0.03\n",
        "    mu_r_approx=0.04\n",
        "    T_l_approx=0.91\n",
        "    T_r_approx=0.99\n",
        "    K_l_approx=109.1\n",
        "    K_r_approx=109.9\n",
        "\n",
        "    s0 = tf.random_uniform((batch_size,1), minval=s_0_l,\n",
        "                                 maxval=s_0_r, dtype=dtype)\n",
        "    sigma=tf.random_uniform((batch_size,1),\n",
        "                            minval=sigma_l,maxval=sigma_r, dtype=dtype)\n",
        "    mu=tf.random_uniform((batch_size,1),\n",
        "                            minval=mu_l,maxval=mu_r, dtype=dtype)\n",
        "    T=tf.random_uniform((batch_size,1),\n",
        "                            minval=T_l,maxval=T_r, dtype=dtype)\n",
        "    K=tf.random_uniform((batch_size,1),\n",
        "                      minval=K_l,maxval=K_r, dtype=dtype)\n",
        "    s0_approx = tf.random_uniform((batch_size_approx,  1 ), \n",
        "                            minval=s_0_l_approx,maxval=s_0_r_approx, dtype=dtype)\n",
        "    sigma_approx=tf.random_uniform((batch_size_approx,  1 ), \n",
        "                            minval=sigma_l_approx,maxval=sigma_r_approx, dtype=dtype)\n",
        "    mu_approx=tf.random_uniform((batch_size_approx,1),\n",
        "                            minval=mu_l_approx,maxval=mu_r_approx, dtype=dtype)\n",
        "    T_approx=tf.random_uniform((batch_size_approx,1),\n",
        "                            minval=T_l_approx,maxval=T_r_approx, dtype=dtype)\n",
        "    K_approx=tf.random_uniform((batch_size_approx,1),\n",
        "                            minval=K_l_approx,maxval=K_r_approx, dtype=dtype)\n",
        "    xi=tf.reshape(tf.stack([s0,sigma,mu,T,K], axis=2), (batch_size,d))\n",
        "    xi_approx=tf.reshape(tf.stack([s0_approx,sigma_approx,mu_approx,T_approx,K_approx], axis=2), (batch_size_approx,d))\n",
        "\n",
        "    tfd = tfp.distributions\n",
        "    dist = tfd.Normal(loc=tf.cast(0.,tf.float32), scale=tf.cast(1.,tf.float32))\n",
        "    d1=tf.math.divide(\n",
        "    (tf.log(tf.math.divide(s0_approx,K_approx))+(mu_approx + 0.5*sigma_approx**2)*T_approx) , (sigma_approx*tf.sqrt(T_approx)))\n",
        "    d2=tf.math.divide(\n",
        "    (tf.log(tf.math.divide(s0_approx,K_approx))+(mu_approx - 0.5*sigma_approx**2)*T_approx) , (sigma_approx*tf.sqrt(T_approx)))\n",
        "    u_reference= tf.multiply(s0_approx,(dist.cdf(d1)))-K_approx*tf.exp(-mu_approx*T_approx)*(dist.cdf(d2))\n",
        "  \n",
        "  def phi(x,sigma,mu,T,K, axis=1):\n",
        "    payoffcoarse=tf.exp(-mu * T)* tf.maximum(x - K, 0.)\n",
        "    return payoffcoarse\n",
        "  def sde_body(idx, s, sigma,mu,T,K, samples): #Milstein Scheme\n",
        "    h=T/N\n",
        "    z=tf.random_normal(shape=(samples, batch_size,1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    #Milstein\n",
        "    s=s + mu *s * h +sigma * s *tf.sqrt(h)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(h)*z)**2-h)\n",
        "    \n",
        "    return tf.add(idx, 1), s, sigma,mu,T,K#, sfine                     \n",
        "  def mc_body(idx, p):\n",
        "    _, _x, _sigma,_mu,_T,_K = tf.while_loop(lambda _idx, s, sigma,mu,T,K: _idx < N,\n",
        "                          lambda _idx, s, sigma,mu,T,K: sde_body(_idx, s, sigma,mu,T,K,\n",
        "                                                   mc_samples_ref),\n",
        "                                                   loop_var_mc)\n",
        "    return idx + 1, p + tf.reduce_mean(phi(_x,_sigma,_mu,_T,_K, 2), axis=0)\n",
        "\n",
        "  #Erklaerung loop_var_mc: mc body fuer u_reference mit mc_samples_ref\n",
        "  loop_var_mc = (tf.constant(0),tf.ones((mc_samples_ref,batch_size, 1), dtype) * s0, tf.ones((mc_samples_ref,batch_size, 1), dtype) * sigma,tf.ones((mc_samples_ref,batch_size, 1), dtype) * mu,tf.ones((mc_samples_ref,batch_size, 1), dtype) * T,tf.ones((mc_samples_ref,batch_size, 1), dtype) * K)  \n",
        "  _, u = tf.while_loop(lambda idx, p: idx < N_l, mc_body,(tf.constant(0), tf.zeros((batch_size, 1), dtype)))\n",
        "  u_mc_test = u / tf.cast(N_l, tf.float32)\n",
        "  \n",
        "  file_name='output-single.csv'\n",
        "                \n",
        "  kolmogorov_train_and_test(xi, xi_approx, xi, u_mc_test, u_reference,\n",
        "                          neurons, lr_boundaries, lr_values, train_steps,\n",
        "                          mc_rounds, mc_freq, file_name, dtype)   \n",
        "                        \n",
        "                        \n",
        "  ## Create & upload a file. in google drive\n",
        "  uploaded = drive.CreateFile({'title': file_name})\n",
        "  uploaded.SetContentFile(file_name)\n",
        "  uploaded.Upload()\n",
        "  print('Uploaded file with ID {}'.format(uploaded.get('id')))\n",
        "                        \n",
        "                        \n",
        "                       "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "0\n",
            "10000\n",
            "20000\n",
            "30000\n",
            "40000\n",
            "Uploaded file with ID 1ENwIEtVVlewDVEoi697qpBWeO2mbgJ90\n"
          ]
        }
      ]
    }
  ]
}