{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multi-individual-steps.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMLrntDRXUKqOVSPTrkPXBr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielroth12/Multilevel-Monte-Carlo-learning/blob/main/multi_individual_steps.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcEnYQ5RpLtV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "280e0c35-2369-4e06-bdb2-41dbb81d68ed"
      },
      "source": [
        "#Multilevel algorithm using 8 networks\n",
        "#For more detailed explanations of the training and model parameters\n",
        "#see Gerstner et al. \"Multilevel Monte Carlo learning.\" arXiv preprint arXiv:2102.08734 (2021).\n",
        "\n",
        "#Packages\n",
        "%tensorflow_version 1.x\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import time\n",
        "from tensorflow.python.ops import init_ops\n",
        "from tensorflow.contrib.layers.python.layers import initializers\n",
        "from tensorflow.python.training.moving_averages import assign_moving_average\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "#Basic network framework according to Beck, Christian, et al. \"Solving the Kolmogorov PDE by means of deep learning.\" Journal of Scientific Computing 88.3 (2021): 1-28.\n",
        "#The individual definition was repeated for each of the 8 networks\n",
        "def neural_net(x, xi_approx, neurons, is_training, name,\n",
        "                    mv_decay=0.9, dtype=tf.float32):\n",
        "  def approx_test(): return xi_approx\n",
        "  def approx_learn(): return x\n",
        "  x = tf.cond(is_training, approx_learn,approx_test)\n",
        "\n",
        "  def _batch_normalization(_x):\n",
        "    beta = tf.get_variable('beta', [_x.get_shape()[-1]],\n",
        "                           dtype, init_ops.zeros_initializer())\n",
        "    gamma = tf.get_variable('gamma', [_x.get_shape()[-1]],\n",
        "                            dtype, init_ops.ones_initializer())\n",
        "    mv_mean = tf.get_variable('mv_mean', [_x.get_shape()[-1]],\n",
        "                              dtype, init_ops.zeros_initializer(),\n",
        "                              trainable=False)\n",
        "    mv_variance = tf.get_variable('mv_variance', [_x.get_shape()[-1]],\n",
        "                                  dtype, init_ops.ones_initializer(),\n",
        "                                  trainable=False)\n",
        "    mean, variance = tf.nn.moments(_x, [0], name='moments')\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_mean, mean,\n",
        "                                               mv_decay, True))\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_variance, variance,\n",
        "                                               mv_decay, False))\n",
        "    mean, variance = tf.cond(is_training,\n",
        "                             lambda: (mean, variance),\n",
        "                             lambda: (mv_mean, mv_variance))\n",
        "    return tf.nn.batch_normalization(_x, mean, variance,\n",
        "                                     beta, gamma, 1e-6)\n",
        "  def _layer(_x, out_size, activation_fn):\n",
        "    w = tf.get_variable('weights',\n",
        "                        [_x.get_shape().as_list()[-1], out_size],\n",
        "                        dtype, initializers.xavier_initializer())\n",
        "    return activation_fn(_batch_normalization(tf.matmul(_x, w)))\n",
        "  \n",
        "  with tf.variable_scope(name):\n",
        "      x = _batch_normalization(x)\n",
        "      for i in range(len(neurons)):\n",
        "        with tf.variable_scope('layer_%i_' % (i + 1)):\n",
        "          x = _layer(x, neurons[i],\n",
        "            tf.nn.tanh if i < len(neurons)-1 else tf.identity)\n",
        "  return x\n",
        "def neural_net_p1_p0(x, xi_approx, neurons, is_training, name,\n",
        "                    mv_decay=0.9, dtype=tf.float32):\n",
        "  def approx_test(): return xi_approx\n",
        "  def approx_learn(): return x\n",
        "  x = tf.cond(is_training, approx_learn,approx_test)\n",
        "  def _batch_normalization(_x):\n",
        "    beta = tf.get_variable('beta2', [_x.get_shape()[-1]],\n",
        "                           dtype, init_ops.zeros_initializer())\n",
        "    gamma = tf.get_variable('gamma2', [_x.get_shape()[-1]],\n",
        "                            dtype, init_ops.ones_initializer())\n",
        "    mv_mean = tf.get_variable('mv_mean2', [_x.get_shape()[-1]],\n",
        "                              dtype, init_ops.zeros_initializer(),\n",
        "                              trainable=False)\n",
        "    mv_variance = tf.get_variable('mv_variance2', [_x.get_shape()[-1]],\n",
        "                                  dtype, init_ops.ones_initializer(),\n",
        "                                  trainable=False)\n",
        "    mean, variance = tf.nn.moments(_x, [0], name='moments2')\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_mean, mean,\n",
        "                                               mv_decay, True))\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_variance, variance,\n",
        "                                               mv_decay, False))\n",
        "    mean, variance = tf.cond(is_training,\n",
        "                             lambda: (mean, variance),\n",
        "                             lambda: (mv_mean, mv_variance))\n",
        "    return tf.nn.batch_normalization(_x, mean, variance,\n",
        "                                     beta, gamma, 1e-6)\n",
        "  def _layer(_x, out_size, activation_fn):\n",
        "    w = tf.get_variable('weights2',\n",
        "                        [_x.get_shape().as_list()[-1], out_size],\n",
        "                        dtype, initializers.xavier_initializer())\n",
        "    return activation_fn(_batch_normalization(tf.matmul(_x, w)))\n",
        "  \n",
        "  with tf.variable_scope(name):\n",
        "      x = _batch_normalization(x)\n",
        "      for i in range(len(neurons)):\n",
        "        with tf.variable_scope('layer2_%i_' % (i + 1)):\n",
        "          x = _layer(x, neurons[i],\n",
        "            tf.nn.tanh if i < len(neurons)-1 else tf.identity)\n",
        "  return x  \n",
        "def neural_net_p2_p1(x, xi_approx, neurons, is_training, name,\n",
        "                    mv_decay=0.9, dtype=tf.float32):\n",
        "  \n",
        "  \n",
        "  def approx_test(): return xi_approx\n",
        "  def approx_learn(): return x\n",
        "  x = tf.cond(is_training, approx_learn,approx_test)\n",
        "  def _batch_normalization(_x):\n",
        "    beta = tf.get_variable('beta3', [_x.get_shape()[-1]],\n",
        "                           dtype, init_ops.zeros_initializer())\n",
        "    gamma = tf.get_variable('gamma3', [_x.get_shape()[-1]],\n",
        "                            dtype, init_ops.ones_initializer())\n",
        "    mv_mean = tf.get_variable('mv_mean3', [_x.get_shape()[-1]],\n",
        "                              dtype, init_ops.zeros_initializer(),\n",
        "                              trainable=False)\n",
        "    mv_variance = tf.get_variable('mv_variance3', [_x.get_shape()[-1]],\n",
        "                                  dtype, init_ops.ones_initializer(),\n",
        "                                  trainable=False)\n",
        "    mean, variance = tf.nn.moments(_x, [0], name='moments3')\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_mean, mean,\n",
        "                                               mv_decay, True))\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_variance, variance,\n",
        "                                               mv_decay, False))\n",
        "    mean, variance = tf.cond(is_training,\n",
        "                             lambda: (mean, variance),\n",
        "                             lambda: (mv_mean, mv_variance))\n",
        "    return tf.nn.batch_normalization(_x, mean, variance,\n",
        "                                     beta, gamma, 1e-6)\n",
        "  def _layer(_x, out_size, activation_fn):\n",
        "    w = tf.get_variable('weights3',\n",
        "                        [_x.get_shape().as_list()[-1], out_size],\n",
        "                        dtype, initializers.xavier_initializer())\n",
        "    return activation_fn(_batch_normalization(tf.matmul(_x, w)))\n",
        "  \n",
        "  with tf.variable_scope(name):\n",
        "      x = _batch_normalization(x)\n",
        "      for i in range(len(neurons)):\n",
        "        with tf.variable_scope('layer3_%i_' % (i + 1)):\n",
        "          x = _layer(x, neurons[i],\n",
        "            tf.nn.tanh if i < len(neurons)-1 else tf.identity)\n",
        "  return x   \n",
        "def neural_net_p3_p2(x, xi_approx, neurons, is_training, name,\n",
        "                    mv_decay=0.9, dtype=tf.float32):\n",
        "  def approx_test(): return xi_approx\n",
        "  def approx_learn(): return x\n",
        "  x = tf.cond(is_training, approx_learn,approx_test)\n",
        "  def _batch_normalization(_x):\n",
        "    beta = tf.get_variable('beta4', [_x.get_shape()[-1]],\n",
        "                           dtype, init_ops.zeros_initializer())\n",
        "    gamma = tf.get_variable('gamma4', [_x.get_shape()[-1]],\n",
        "                            dtype, init_ops.ones_initializer())\n",
        "    mv_mean = tf.get_variable('mv_mean4', [_x.get_shape()[-1]],\n",
        "                              dtype, init_ops.zeros_initializer(),\n",
        "                              trainable=False)\n",
        "    mv_variance = tf.get_variable('mv_variance4', [_x.get_shape()[-1]],\n",
        "                                  dtype, init_ops.ones_initializer(),\n",
        "                                  trainable=False)\n",
        "    mean, variance = tf.nn.moments(_x, [0], name='moments4')\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_mean, mean,\n",
        "                                               mv_decay, True))\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_variance, variance,\n",
        "                                               mv_decay, False))\n",
        "    mean, variance = tf.cond(is_training,\n",
        "                             lambda: (mean, variance),\n",
        "                             lambda: (mv_mean, mv_variance))\n",
        "    return tf.nn.batch_normalization(_x, mean, variance,\n",
        "                                     beta, gamma, 1e-6)\n",
        "  def _layer(_x, out_size, activation_fn):\n",
        "    w = tf.get_variable('weights4',\n",
        "                        [_x.get_shape().as_list()[-1], out_size],\n",
        "                        dtype, initializers.xavier_initializer())\n",
        "    return activation_fn(_batch_normalization(tf.matmul(_x, w)))\n",
        "  \n",
        "  with tf.variable_scope(name):\n",
        "      x = _batch_normalization(x)\n",
        "      for i in range(len(neurons)):\n",
        "        with tf.variable_scope('layer4_%i_' % (i + 1)):\n",
        "          x = _layer(x, neurons[i],\n",
        "            tf.nn.tanh if i < len(neurons)-1 else tf.identity)\n",
        "  return x     \n",
        "def neural_net_p4_p3(x, xi_approx, neurons, is_training, name,\n",
        "                    mv_decay=0.9, dtype=tf.float32):\n",
        "  def approx_test(): return xi_approx\n",
        "  def approx_learn(): return x\n",
        "  x = tf.cond(is_training, approx_learn,approx_test)\n",
        "  def _batch_normalization(_x):\n",
        "    beta = tf.get_variable('beta5', [_x.get_shape()[-1]],\n",
        "                           dtype, init_ops.zeros_initializer())\n",
        "    gamma = tf.get_variable('gamma5', [_x.get_shape()[-1]],\n",
        "                            dtype, init_ops.ones_initializer())\n",
        "    mv_mean = tf.get_variable('mv_mean5', [_x.get_shape()[-1]],\n",
        "                              dtype, init_ops.zeros_initializer(),\n",
        "                              trainable=False)\n",
        "    mv_variance = tf.get_variable('mv_variance5', [_x.get_shape()[-1]],\n",
        "                                  dtype, init_ops.ones_initializer(),\n",
        "                                  trainable=False)\n",
        "    mean, variance = tf.nn.moments(_x, [0], name='moments5')\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_mean, mean,\n",
        "                                               mv_decay, True))\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_variance, variance,\n",
        "                                               mv_decay, False))\n",
        "    mean, variance = tf.cond(is_training,\n",
        "                             lambda: (mean, variance),\n",
        "                             lambda: (mv_mean, mv_variance))\n",
        "    return tf.nn.batch_normalization(_x, mean, variance,\n",
        "                                     beta, gamma, 1e-6)\n",
        "  def _layer(_x, out_size, activation_fn):\n",
        "    w = tf.get_variable('weights5',\n",
        "                        [_x.get_shape().as_list()[-1], out_size],\n",
        "                        dtype, initializers.xavier_initializer())\n",
        "    return activation_fn(_batch_normalization(tf.matmul(_x, w)))\n",
        "  \n",
        "  with tf.variable_scope(name):\n",
        "      x = _batch_normalization(x)\n",
        "      for i in range(len(neurons)):\n",
        "        with tf.variable_scope('layer5_%i_' % (i + 1)):\n",
        "          x = _layer(x, neurons[i],\n",
        "            tf.nn.tanh if i < len(neurons)-1 else tf.identity)\n",
        "  return x       \n",
        "def neural_net_p5_p4(x, xi_approx, neurons, is_training, name,\n",
        "                    mv_decay=0.9, dtype=tf.float32):\n",
        "  def approx_test(): return xi_approx\n",
        "  def approx_learn(): return x\n",
        "  x = tf.cond(is_training, approx_learn,approx_test)\n",
        "  def _batch_normalization(_x):\n",
        "    beta = tf.get_variable('beta6', [_x.get_shape()[-1]],\n",
        "                           dtype, init_ops.zeros_initializer())\n",
        "    gamma = tf.get_variable('gamma6', [_x.get_shape()[-1]],\n",
        "                            dtype, init_ops.ones_initializer())\n",
        "    mv_mean = tf.get_variable('mv_mean6', [_x.get_shape()[-1]],\n",
        "                              dtype, init_ops.zeros_initializer(),\n",
        "                              trainable=False)\n",
        "    mv_variance = tf.get_variable('mv_variance6', [_x.get_shape()[-1]],\n",
        "                                  dtype, init_ops.ones_initializer(),\n",
        "                                  trainable=False)\n",
        "    mean, variance = tf.nn.moments(_x, [0], name='moments6')\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_mean, mean,\n",
        "                                               mv_decay, True))\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_variance, variance,\n",
        "                                               mv_decay, False))\n",
        "    mean, variance = tf.cond(is_training,\n",
        "                             lambda: (mean, variance),\n",
        "                             lambda: (mv_mean, mv_variance))\n",
        "    return tf.nn.batch_normalization(_x, mean, variance,\n",
        "                                     beta, gamma, 1e-6)\n",
        "  def _layer(_x, out_size, activation_fn):\n",
        "    w = tf.get_variable('weights6',\n",
        "                        [_x.get_shape().as_list()[-1], out_size],\n",
        "                        dtype, initializers.xavier_initializer())\n",
        "    return activation_fn(_batch_normalization(tf.matmul(_x, w)))\n",
        "  \n",
        "  with tf.variable_scope(name):\n",
        "      x = _batch_normalization(x)\n",
        "      for i in range(len(neurons)):\n",
        "        with tf.variable_scope('layer6_%i_' % (i + 1)):\n",
        "          x = _layer(x, neurons[i],\n",
        "            tf.nn.tanh if i < len(neurons)-1 else tf.identity)\n",
        "  return x    \n",
        "def neural_net_p6_p5(x, xi_approx, neurons, is_training, name,\n",
        "                    mv_decay=0.9, dtype=tf.float32):\n",
        "  def approx_test(): return xi_approx\n",
        "  def approx_learn(): return x\n",
        "  x = tf.cond(is_training, approx_learn,approx_test)\n",
        "\n",
        "  def _batch_normalization(_x):\n",
        "    beta = tf.get_variable('beta7', [_x.get_shape()[-1]],\n",
        "                           dtype, init_ops.zeros_initializer())\n",
        "    gamma = tf.get_variable('gamma7', [_x.get_shape()[-1]],\n",
        "                            dtype, init_ops.ones_initializer())\n",
        "    mv_mean = tf.get_variable('mv_mean7', [_x.get_shape()[-1]],\n",
        "                              dtype, init_ops.zeros_initializer(),\n",
        "                              trainable=False)\n",
        "    mv_variance = tf.get_variable('mv_variance7', [_x.get_shape()[-1]],\n",
        "                                  dtype, init_ops.ones_initializer(),\n",
        "                                  trainable=False)\n",
        "    mean, variance = tf.nn.moments(_x, [0], name='moments7')\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_mean, mean,\n",
        "                                               mv_decay, True))\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_variance, variance,\n",
        "                                               mv_decay, False))\n",
        "    mean, variance = tf.cond(is_training,\n",
        "                             lambda: (mean, variance),\n",
        "                             lambda: (mv_mean, mv_variance))\n",
        "    return tf.nn.batch_normalization(_x, mean, variance,\n",
        "                                     beta, gamma, 1e-6)\n",
        "  \n",
        "  def _layer(_x, out_size, activation_fn):\n",
        "    w = tf.get_variable('weights7',\n",
        "                        [_x.get_shape().as_list()[-1], out_size],\n",
        "                        dtype, initializers.xavier_initializer())\n",
        "    return activation_fn(_batch_normalization(tf.matmul(_x, w)))\n",
        "  \n",
        "  with tf.variable_scope(name):\n",
        "      x = _batch_normalization(x)\n",
        "      for i in range(len(neurons)):\n",
        "        with tf.variable_scope('layer7_%i_' % (i + 1)):\n",
        "          x = _layer(x, neurons[i],\n",
        "            tf.nn.tanh if i < len(neurons)-1 else tf.identity)\n",
        "  return x         \n",
        "\n",
        "def neural_net_p7_p6(x, xi_approx, neurons, is_training, name,\n",
        "                    mv_decay=0.9, dtype=tf.float32):\n",
        "  \n",
        "  \n",
        "  def approx_test(): return xi_approx\n",
        "  def approx_learn(): return x\n",
        "  x = tf.cond(is_training, approx_learn,approx_test)\n",
        "\n",
        "  def _batch_normalization(_x):\n",
        "    beta = tf.get_variable('beta8', [_x.get_shape()[-1]],\n",
        "                           dtype, init_ops.zeros_initializer())\n",
        "    gamma = tf.get_variable('gamma8', [_x.get_shape()[-1]],\n",
        "                            dtype, init_ops.ones_initializer())\n",
        "    mv_mean = tf.get_variable('mv_mean8', [_x.get_shape()[-1]],\n",
        "                              dtype, init_ops.zeros_initializer(),\n",
        "                              trainable=False)\n",
        "    mv_variance = tf.get_variable('mv_variance8', [_x.get_shape()[-1]],\n",
        "                                  dtype, init_ops.ones_initializer(),\n",
        "                                  trainable=False)\n",
        "    mean, variance = tf.nn.moments(_x, [0], name='moments8')\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_mean, mean,\n",
        "                                               mv_decay, True))\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                         assign_moving_average(mv_variance, variance,\n",
        "                                               mv_decay, False))\n",
        "    mean, variance = tf.cond(is_training,\n",
        "                             lambda: (mean, variance),\n",
        "                             lambda: (mv_mean, mv_variance))\n",
        "    return tf.nn.batch_normalization(_x, mean, variance,\n",
        "                                     beta, gamma, 1e-6)\n",
        "  def _layer(_x, out_size, activation_fn):\n",
        "    w = tf.get_variable('weights8',\n",
        "                        [_x.get_shape().as_list()[-1], out_size],\n",
        "                        dtype, initializers.xavier_initializer())\n",
        "    return activation_fn(_batch_normalization(tf.matmul(_x, w)))\n",
        "  with tf.variable_scope(name):\n",
        "      x = _batch_normalization(x)\n",
        "      for i in range(len(neurons)):\n",
        "        with tf.variable_scope('layer8_%i_' % (i + 1)):\n",
        "          x = _layer(x, neurons[i],\n",
        "            tf.nn.tanh if i < len(neurons)-1 else tf.identity)\n",
        "  return x         \n",
        "\n",
        "#Basic network framework according to Beck, Christian, et al. \"Solving the Kolmogorov PDE by means of deep learning.\" Journal of Scientific Computing 88.3 (2021): 1-28.\n",
        "#Adjustments:\n",
        "#- exponential decay (individual parameters possible)\n",
        "#- each network trained individually but in one session (line 534)\n",
        "#- overall error calculated in line 411\n",
        "\n",
        "def train_and_test(xi, xi_approx, xi_p1_p0, xi_p7_p6, xi_p6_p5, xi_p5_p4, xi_p4_p3, xi_p3_p2, xi_p2_p1, x_sde, phi_p0, phi_p7_p6, phi_p6_p5, phi_p5_p4, phi_p4_p3, phi_p3_p2, phi_p2_p1, phi_p1_p0, u_reference, u_reference_p0,u_reference_p7_p6,u_reference_p6_p5,u_reference_p5_p4,u_reference_p4_p3,u_reference_p3_p2,u_reference_p2_p1,u_reference_p1_p0, neurons,\n",
        "          train_steps,mc_rounds, mc_freq, file_name,\n",
        "          dtype=tf.float32):\n",
        "  def _approximate_errors():\n",
        "    lr, gs, lr10,lr21,lr32,lr43,lr54,lr65 = sess.run([learning_rate, global_step,learning_rate_p1_p0,learning_rate_p2_p1,learning_rate_p3_p2,learning_rate_p4_p3,learning_rate_p5_p4,learning_rate_p6_p5])\n",
        "    li_err, li_err_p1_p0, li_err_kombination, li_err_p2_p1, li_err_p3_p2,li_err_p4_p3,li_err_p5_p4,li_err_p6_p5,li_err_p7_p6 = 0., 0., 0.,0.,0.,0.,0.,0.,0.\n",
        "    for _ in range(mc_rounds):\n",
        "      li, appr, ref, li_p7_p6, li_p6_p5, li_p5_p4, li_p4_p3, li_p3_p2, li_p2_p1,li_p1_p0, li_kombination = sess.run([err_l_inf,  approx, reference, err_l_p7_p6, err_l_p6_p5, err_l_p5_p4, err_l_p4_p3, err_l_p3_p2, err_l_p2_p1,err_l_p1_p0, err_l_kombination],\n",
        "                                              feed_dict={is_training: False})\n",
        "      li_err, li_err_p1_p0, li_err_kombination, li_err_p2_p1, li_err_p3_p2, li_err_p4_p3, li_err_p5_p4, li_err_p6_p5, li_err_p7_p6= (np.maximum(li_err, li),np.maximum(li_err_p1_p0, li_p1_p0)\n",
        "          ,np.maximum(li_kombination, li_err_kombination),np.maximum(li_err_p2_p1, li_p2_p1),np.maximum(li_err_p3_p2, li_p3_p2),np.maximum(li_err_p4_p3, li_p4_p3),np.maximum(li_err_p5_p4, li_p5_p4),np.maximum(li_err_p6_p5, li_p6_p5),np.maximum(li_err_p7_p6, li_p7_p6))\n",
        "    t_mc = time.time()\n",
        "    file_out.write('%i, %f, %f, %f, %f, %f, %f, '\n",
        "                    ' %f, %f, %f, %f, %f, %f \\n' % (gs, li_err_kombination, lr,\n",
        "                      t1_train - t0_train, t_mc - t1_train, appr, ref, lr10,lr21,lr32,lr43,lr54,lr65))\n",
        "    file_out.flush()\n",
        "    \n",
        "  t0_train = time.time()\n",
        "  is_training = tf.placeholder(tf.bool, [])\n",
        "  u_approx = neural_net(xi, xi_approx, neurons, is_training, 'u_approx', dtype=dtype)\n",
        "  u_approx_p1_p0 = neural_net_p1_p0(xi_p1_p0, xi_approx, neurons, is_training, 'u_approx_p1_p0', dtype=dtype)\n",
        "  u_approx_p2_p1 = neural_net_p2_p1(xi_p2_p1, xi_approx, neurons, is_training, 'u_approx_p2_p1', dtype=dtype)\n",
        "  u_approx_p3_p2 = neural_net_p3_p2(xi_p3_p2, xi_approx, neurons, is_training, 'u_approx_p3_p2', dtype=dtype)\n",
        "  u_approx_p4_p3 = neural_net_p4_p3(xi_p4_p3, xi_approx, neurons, is_training, 'u_approx_p4_p3', dtype=dtype)  \n",
        "  u_approx_p5_p4 = neural_net_p5_p4(xi_p5_p4, xi_approx, neurons, is_training, 'u_approx_p5_p4', dtype=dtype)  \n",
        "  u_approx_p6_p5 = neural_net_p6_p5(xi_p6_p5, xi_approx, neurons, is_training, 'u_approx_p6_p5', dtype=dtype)  \n",
        "  u_approx_p7_p6 = neural_net_p7_p6(xi_p7_p6, xi_approx, neurons, is_training, 'u_approx_p7_p6', dtype=dtype)  \n",
        "  \n",
        "  loss_p0 = tf.reduce_mean(tf.squared_difference(u_approx, phi_p0))\n",
        "  loss_p1_p0 = tf.reduce_mean(tf.squared_difference(u_approx_p1_p0, phi_p1_p0))\n",
        "  loss_p2_p1 = tf.reduce_mean(tf.squared_difference(u_approx_p2_p1, phi_p2_p1))\n",
        "  loss_p3_p2 = tf.reduce_mean(tf.squared_difference(u_approx_p3_p2, phi_p3_p2))\n",
        "  loss_p4_p3 = tf.reduce_mean(tf.squared_difference(u_approx_p4_p3, phi_p4_p3))\n",
        "  loss_p5_p4 = tf.reduce_mean(tf.squared_difference(u_approx_p5_p4, phi_p5_p4))\n",
        "  loss_p6_p5 = tf.reduce_mean(tf.squared_difference(u_approx_p6_p5, phi_p6_p5))\n",
        "  loss_p7_p6 = tf.reduce_mean(tf.squared_difference(u_approx_p7_p6, phi_p7_p6))\n",
        "  \n",
        "  approx=tf.reduce_mean(u_approx)\n",
        "  approx_p1_p0=tf.reduce_mean(u_approx_p1_p0)\n",
        "  reference=tf.reduce_mean(u_reference_p0)\n",
        "\n",
        "  err = tf.abs(u_approx - u_reference_p0)\n",
        "  err_p1_p0 = tf.abs(u_approx_p1_p0 - u_reference_p1_p0)\n",
        "  err_p2_p1 = tf.abs(u_approx_p2_p1 - u_reference_p2_p1)\n",
        "  err_p3_p2 = tf.abs(u_approx_p3_p2 - u_reference_p3_p2)\n",
        "  err_p4_p3 = tf.abs(u_approx_p4_p3 - u_reference_p4_p3)\n",
        "  err_p5_p4 = tf.abs(u_approx_p5_p4 - u_reference_p5_p4)\n",
        "  err_p6_p5 = tf.abs(u_approx_p6_p5 - u_reference_p6_p5)\n",
        "  err_p7_p6 = tf.abs(u_approx_p7_p6 - u_reference_p7_p6)\n",
        "\n",
        "  err_kombination=tf.abs(u_approx   + u_approx_p7_p6 + u_approx_p6_p5 + u_approx_p5_p4 + u_approx_p4_p3 + u_approx_p3_p2 + u_approx_p2_p1 + u_approx_p1_p0 - u_reference)\n",
        "  err_l_inf = tf.reduce_max(err)\n",
        "  err_l_p2_p1 = tf.reduce_max(err_p2_p1)\n",
        "  err_l_p3_p2 = tf.reduce_max(err_p3_p2)\n",
        "  err_l_p4_p3 = tf.reduce_max(err_p4_p3)\n",
        "  err_l_p5_p4 = tf.reduce_max(err_p5_p4)\n",
        "  err_l_p6_p5 = tf.reduce_max(err_p6_p5)\n",
        "  err_l_p7_p6 = tf.reduce_max(err_p7_p6)\n",
        "  err_l_p1_p0 = tf.reduce_max(err_p1_p0)\n",
        "  err_l_kombination = tf.reduce_max(err_kombination)\n",
        "\n",
        "  lr = 0.01\n",
        "  step_rate = 40000\n",
        "  decay = 0.1\n",
        "  global_step = tf.Variable(1, trainable=False)\n",
        "  increment_global_step = tf.assign(global_step, global_step + 1)\n",
        "  learning_rate = tf.train.exponential_decay(lr, global_step, step_rate, decay, staircase=True)\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "\n",
        "  lr_p1_p0 = lr\n",
        "  step_rate_p1_p0 = step_rate\n",
        "  decay_p1_p0 = 0.1\n",
        "  global_step_p1_p0 = tf.Variable(0, trainable=False)\n",
        "  increment_global_step_p1_p0 = tf.assign(global_step_p1_p0, global_step_p1_p0 + 1)\n",
        "  learning_rate_p1_p0 = tf.train.exponential_decay(lr_p1_p0, global_step_p1_p0, step_rate_p1_p0, decay_p1_p0, staircase=True)\n",
        "  optimizer_p1_p0 = tf.train.AdamOptimizer(learning_rate_p1_p0)\n",
        "\n",
        "  lr_p2_p1 = lr\n",
        "  step_rate_p2_p1 =step_rate\n",
        "  decay_p2_p1 = 0.1\n",
        "  global_step_p2_p1 = tf.Variable(0, trainable=False)\n",
        "  increment_global_step_p2_p1 = tf.assign(global_step_p2_p1, global_step_p2_p1 + 1)\n",
        "  learning_rate_p2_p1 = tf.train.exponential_decay(lr_p2_p1, global_step_p2_p1, step_rate_p2_p1, decay_p2_p1, staircase=True)\n",
        "  optimizer_p2_p1 = tf.train.AdamOptimizer(learning_rate_p2_p1)\n",
        "\n",
        "  lr_p3_p2 = lr\n",
        "  step_rate_p3_p2 = step_rate\n",
        "  decay_p3_p2 = 0.1\n",
        "  global_step_p3_p2 = tf.Variable(0, trainable=False)\n",
        "  increment_global_step_p3_p2 = tf.assign(global_step_p3_p2, global_step_p3_p2 + 1)\n",
        "  learning_rate_p3_p2 = tf.train.exponential_decay(lr_p3_p2, global_step_p3_p2, step_rate_p3_p2, decay_p3_p2, staircase=True)\n",
        "  optimizer_p3_p2 = tf.train.AdamOptimizer(learning_rate_p3_p2)\n",
        "\n",
        "  lr_p4_p3 = lr\n",
        "  step_rate_p4_p3 = step_rate\n",
        "  decay_p4_p3 = 0.1\n",
        "  global_step_p4_p3 = tf.Variable(0, trainable=False)\n",
        "  increment_global_step_p4_p3 = tf.assign(global_step_p4_p3, global_step_p4_p3 + 1)\n",
        "  learning_rate_p4_p3 = tf.train.exponential_decay(lr_p4_p3, global_step_p4_p3, step_rate_p4_p3, decay_p4_p3, staircase=True)\n",
        "  optimizer_p4_p3 = tf.train.AdamOptimizer(learning_rate_p4_p3)\n",
        "  \n",
        "  lr_p5_p4 = lr\n",
        "  step_rate_p5_p4 = step_rate\n",
        "  decay_p5_p4 = 0.1\n",
        "  global_step_p5_p4 = tf.Variable(0, trainable=False)\n",
        "  increment_global_step_p5_p4 = tf.assign(global_step_p5_p4, global_step_p5_p4 + 1)\n",
        "  learning_rate_p5_p4 = tf.train.exponential_decay(lr_p5_p4, global_step_p5_p4, step_rate_p5_p4, decay_p5_p4, staircase=True)\n",
        "  optimizer_p5_p4 = tf.train.AdamOptimizer(learning_rate_p5_p4)\n",
        "\n",
        "  #p6_p5\n",
        "  lr_p6_p5 = lr\n",
        "  step_rate_p6_p5 = step_rate\n",
        "  decay_p6_p5 = 0.1\n",
        "  global_step_p6_p5 = tf.Variable(0, trainable=False)\n",
        "  increment_global_step_p6_p5 = tf.assign(global_step_p6_p5, global_step_p6_p5 + 1)\n",
        "  learning_rate_p6_p5 = tf.train.exponential_decay(lr_p6_p5, global_step_p6_p5, step_rate_p6_p5, decay_p6_p5, staircase=True)\n",
        "  optimizer_p6_p5 = tf.train.AdamOptimizer(learning_rate_p6_p5)\n",
        "\n",
        "  #p7_p6\n",
        "  lr_p7_p6 = lr\n",
        "  step_rate_p7_p6 =step_rate\n",
        "  decay_p7_p6 = 0.1\n",
        "  global_step_p7_p6 = tf.Variable(0, trainable=False)\n",
        "  increment_global_step_p7_p6 = tf.assign(global_step_p7_p6, global_step_p7_p6 + 1)\n",
        "  learning_rate_p7_p6 = tf.train.exponential_decay(lr_p7_p6, global_step_p7_p6, step_rate_p7_p6, decay_p7_p6, staircase=True)\n",
        "  optimizer_p7_p6 = tf.train.AdamOptimizer(learning_rate_p7_p6)\n",
        "\n",
        "\n",
        "  update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, 'u_approx')\n",
        "  with tf.control_dependencies(update_ops):\n",
        "    train_op = optimizer.minimize(loss_p0, global_step)\n",
        "  \n",
        "  update_ops_p1_p0 = tf.get_collection(tf.GraphKeys.UPDATE_OPS, 'u_approx_p1_p0')\n",
        "  with tf.control_dependencies(update_ops_p1_p0):\n",
        "    train_op_p1_p0 = optimizer_p1_p0.minimize(loss_p1_p0, global_step_p1_p0)  \n",
        "\n",
        "  update_ops_p2_p1 = tf.get_collection(tf.GraphKeys.UPDATE_OPS, 'u_approx_p2_p1')\n",
        "  with tf.control_dependencies(update_ops_p2_p1):\n",
        "    train_op_p2_p1 = optimizer_p2_p1.minimize(loss_p2_p1, global_step_p2_p1)   \n",
        "    \n",
        "  update_ops_p3_p2 = tf.get_collection(tf.GraphKeys.UPDATE_OPS, 'u_approx_p3_p2')\n",
        "  with tf.control_dependencies(update_ops_p3_p2):\n",
        "    train_op_p3_p2 = optimizer_p3_p2.minimize(loss_p3_p2, global_step_p3_p2)   \n",
        "\n",
        "  update_ops_p4_p3 = tf.get_collection(tf.GraphKeys.UPDATE_OPS, 'u_approx_p4_p3')\n",
        "  with tf.control_dependencies(update_ops_p4_p3):\n",
        "    train_op_p4_p3 = optimizer_p4_p3.minimize(loss_p4_p3, global_step_p4_p3)    \n",
        "\n",
        "  update_ops_p5_p4 = tf.get_collection(tf.GraphKeys.UPDATE_OPS, 'u_approx_p5_p4')\n",
        "  with tf.control_dependencies(update_ops_p5_p4):\n",
        "    train_op_p5_p4 = optimizer_p5_p4.minimize(loss_p5_p4, global_step_p5_p4) \n",
        "\n",
        "  update_ops_p6_p5 = tf.get_collection(tf.GraphKeys.UPDATE_OPS, 'u_approx_p6_p5')\n",
        "  with tf.control_dependencies(update_ops_p6_p5):\n",
        "    train_op_p6_p5 = optimizer_p6_p5.minimize(loss_p6_p5, global_step_p6_p5) \n",
        "\n",
        "  update_ops_p7_p6 = tf.get_collection(tf.GraphKeys.UPDATE_OPS, 'u_approx_p7_p6')\n",
        "  with tf.control_dependencies(update_ops_p7_p6):\n",
        "    train_op_p7_p6 = optimizer_p7_p6.minimize(loss_p7_p6, global_step_p7_p6) \n",
        "        \n",
        "  file_out = open(file_name, 'w')\n",
        "  file_out.write('step,li_err, learning_rate, time_train, time_mc ,approx, reference, lr10,lr21,lr32,lr43,lr54,lr65 \\n ')\n",
        "    \n",
        "\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    for step in range(1,train_steps):\n",
        "      if step % mc_freq == 0:\n",
        "        print(step)\n",
        "        t1_train = time.time()\n",
        "        _approximate_errors()\n",
        "        t0_train = time.time()      \n",
        "      sess.run([train_op,train_op_p1_p0,train_op_p2_p1,train_op_p3_p2,train_op_p4_p3,train_op_p5_p4,train_op_p6_p5,train_op_p7_p6], feed_dict={is_training:True})\n",
        "    t1_train = time.time()\n",
        "    _approximate_errors()\n",
        "  file_out.close()\n",
        "  \n",
        "#Model and training parameter specification    \n",
        "for i in range(1,2):\n",
        " #print(i)\n",
        " tf.reset_default_graph()\n",
        " tf.random.set_random_seed(i)\n",
        " with tf.Session()  as sess:\n",
        "  dtype = tf.float32\n",
        "\n",
        "  #Set network and training parameter \n",
        "  batch_size      =1200000\n",
        "  batch_size_p1_p0=64000 \n",
        "  batch_size_p2_p1=32000\n",
        "  batch_size_p3_p2=16000\n",
        "  batch_size_p4_p3=8000\n",
        "  batch_size_p5_p4=4000\n",
        "  batch_size_p6_p5=2000\n",
        "  batch_size_p7_p6=1000\n",
        "  batch_size_approx=2000000 \n",
        "  N, d = 1, 5\n",
        "  N_p0=N\n",
        "  N_p1_p0=N \n",
        "  N_p2_p1=N*2\n",
        "  N_p3_p2=N*4\n",
        "  N_p4_p3=N*8\n",
        "  N_p5_p4=N*16\n",
        "  N_p6_p5=N*32\n",
        "  N_p7_p6=N*64\n",
        "  neurons = [50, 50, 1]\n",
        "  train_steps =150000\n",
        "  Ksteps_p7_p6=11000\n",
        "  Ksteps_p6_p5=13000\n",
        "  Ksteps_p5_p4=14000\n",
        "  Ksteps_p4_p3=15000\n",
        "  Ksteps_p3_p2=18000\n",
        "  Ksteps_p2_p1=19000\n",
        "  Ksteps_p1_p0=20000\n",
        "  Ksteps_p0=   train_steps    \n",
        "  mc_rounds, mc_freq = 100, 5000\n",
        "  mc_samples_ref_p0=1#1530596\n",
        "  mc_samples_ref_p1_p0=1#5000\n",
        "  mc_samples_ref_p2_p1=1#1000\n",
        "  mc_samples_ref_p3_p2=1#500\n",
        "  mc_samples_ref_p4_p3=1#500\n",
        "  mc_samples_ref_p5_p4=1#100\n",
        "  mc_samples_ref_p6_p5=1#100\n",
        "  mc_samples_ref, mc_rounds_ref_p0, mc_rounds_ref_p1_p0 = 1, 1000000,1000000\n",
        "\n",
        "  #Define training and test interval\n",
        "  s_0_l=80.0\n",
        "  s_0_r=120.0\n",
        "  sigma_l=0.1\n",
        "  sigma_r=0.2\n",
        "  mu_l=0.02\n",
        "  mu_r=0.05\n",
        "  T_l=0.9\n",
        "  T_r=1.0\n",
        "  K_l=109.0\n",
        "  K_r=110.0\n",
        "  s_0_l_approx=80.4\n",
        "  s_0_r_approx=119.6\n",
        "  sigma_l_approx=0.11\n",
        "  sigma_r_approx=0.19\n",
        "  mu_l_approx=0.03\n",
        "  mu_r_approx=0.04\n",
        "  T_l_approx=0.91\n",
        "  T_r_approx=0.99\n",
        "  K_l_approx=109.1\n",
        "  K_r_approx=109.9\n",
        "  s0 = tf.random_uniform((batch_size,1), minval=s_0_l,\n",
        "                                 maxval=s_0_r, dtype=dtype)\n",
        "  sigma=tf.random_uniform((batch_size,1),\n",
        "                            minval=sigma_l,maxval=sigma_r, dtype=dtype)\n",
        "  mu=tf.random_uniform((batch_size,1),\n",
        "                            minval=mu_l,maxval=mu_r, dtype=dtype)\n",
        "  T=tf.random_uniform((batch_size,1),\n",
        "                            minval=T_l,maxval=T_r, dtype=dtype)\n",
        "  K=tf.random_uniform((batch_size,1),\n",
        "                      minval=K_l,maxval=K_r, dtype=dtype)\n",
        "  s0_p1_p0 = tf.stack((tf.random_uniform((batch_size_p1_p0,1), minval=s_0_l,\n",
        "                                 maxval=s_0_r, dtype=dtype)))\n",
        "  sigma_p1_p0=tf.random_uniform((batch_size_p1_p0,1),\n",
        "                            minval=sigma_l,maxval=sigma_r, dtype=dtype)\n",
        "  mu_p1_p0=tf.random_uniform((batch_size_p1_p0,1),\n",
        "                            minval=mu_l,maxval=mu_r, dtype=dtype)\n",
        "  T_p1_p0=tf.random_uniform((batch_size_p1_p0,1),\n",
        "                            minval=T_l,maxval=T_r, dtype=dtype)\n",
        "  K_p1_p0=tf.random_uniform((batch_size_p1_p0,1),\n",
        "                      minval=K_l,maxval=K_r, dtype=dtype)\n",
        "  s0_p2_p1 = tf.stack((tf.random_uniform((batch_size_p2_p1,1), minval=s_0_l,\n",
        "                                 maxval=s_0_r, dtype=dtype)))\n",
        "  sigma_p2_p1=tf.random_uniform((batch_size_p2_p1,1),\n",
        "                            minval=sigma_l,maxval=sigma_r, dtype=dtype)\n",
        "  mu_p2_p1=tf.random_uniform((batch_size_p2_p1,1),\n",
        "                            minval=mu_l,maxval=mu_r, dtype=dtype)\n",
        "  T_p2_p1=tf.random_uniform((batch_size_p2_p1,1),\n",
        "                            minval=T_l,maxval=T_r, dtype=dtype)\n",
        "  K_p2_p1=tf.random_uniform((batch_size_p2_p1,1),\n",
        "                      minval=K_l,maxval=K_r, dtype=dtype)\n",
        "  s0_p3_p2 = tf.stack((tf.random_uniform((batch_size_p3_p2,1), minval=s_0_l,\n",
        "                                 maxval=s_0_r, dtype=dtype)))\n",
        "  sigma_p3_p2=tf.random_uniform((batch_size_p3_p2,1),\n",
        "                            minval=sigma_l,maxval=sigma_r, dtype=dtype)\n",
        "  mu_p3_p2=tf.random_uniform((batch_size_p3_p2,1),\n",
        "                            minval=mu_l,maxval=mu_r, dtype=dtype)\n",
        "  T_p3_p2=tf.random_uniform((batch_size_p3_p2,1),\n",
        "                            minval=T_l,maxval=T_r, dtype=dtype)\n",
        "  K_p3_p2=tf.random_uniform((batch_size_p3_p2,1),\n",
        "                      minval=K_l,maxval=K_r, dtype=dtype)\n",
        "  s0_p4_p3 = tf.stack((tf.random_uniform((batch_size_p4_p3,1), minval=s_0_l,\n",
        "                                 maxval=s_0_r, dtype=dtype)))\n",
        "  sigma_p4_p3=tf.random_uniform((batch_size_p4_p3,1),\n",
        "                            minval=sigma_l,maxval=sigma_r, dtype=dtype)\n",
        "  mu_p4_p3=tf.random_uniform((batch_size_p4_p3,1),\n",
        "                            minval=mu_l,maxval=mu_r, dtype=dtype)\n",
        "  T_p4_p3=tf.random_uniform((batch_size_p4_p3,1),\n",
        "                            minval=T_l,maxval=T_r, dtype=dtype)\n",
        "  K_p4_p3=tf.random_uniform((batch_size_p4_p3,1),\n",
        "                      minval=K_l,maxval=K_r, dtype=dtype)\n",
        "  s0_p5_p4 = tf.stack((tf.random_uniform((batch_size_p5_p4,1), minval=s_0_l,\n",
        "                                 maxval=s_0_r, dtype=dtype)))\n",
        "  sigma_p5_p4=tf.random_uniform((batch_size_p5_p4,1),\n",
        "                            minval=sigma_l,maxval=sigma_r, dtype=dtype)\n",
        "  mu_p5_p4=tf.random_uniform((batch_size_p5_p4,1),\n",
        "                            minval=mu_l,maxval=mu_r, dtype=dtype)\n",
        "  T_p5_p4=tf.random_uniform((batch_size_p5_p4,1),\n",
        "                            minval=T_l,maxval=T_r, dtype=dtype)\n",
        "  K_p5_p4=tf.random_uniform((batch_size_p5_p4,1),\n",
        "                      minval=K_l,maxval=K_r, dtype=dtype)\n",
        "  s0_p6_p5 = tf.stack((tf.random_uniform((batch_size_p6_p5,1), minval=s_0_l,\n",
        "                                 maxval=s_0_r, dtype=dtype)))\n",
        "  sigma_p6_p5=tf.random_uniform((batch_size_p6_p5,1),\n",
        "                            minval=sigma_l,maxval=sigma_r, dtype=dtype)\n",
        "  mu_p6_p5=tf.random_uniform((batch_size_p6_p5,1),\n",
        "                            minval=mu_l,maxval=mu_r, dtype=dtype)\n",
        "  T_p6_p5=tf.random_uniform((batch_size_p6_p5,1),\n",
        "                            minval=T_l,maxval=T_r, dtype=dtype)\n",
        "  K_p6_p5=tf.random_uniform((batch_size_p6_p5,1),\n",
        "                      minval=K_l,maxval=K_r, dtype=dtype)\n",
        "  s0_p7_p6 = tf.stack((tf.random_uniform((batch_size_p7_p6 ,1), minval=s_0_l,\n",
        "                                 maxval=s_0_r, dtype=dtype)))\n",
        "  sigma_p7_p6 =tf.random_uniform((batch_size_p7_p6 ,1),\n",
        "                            minval=sigma_l,maxval=sigma_r, dtype=dtype)\n",
        "  mu_p7_p6 =tf.random_uniform((batch_size_p7_p6 ,1),\n",
        "                            minval=mu_l,maxval=mu_r, dtype=dtype)\n",
        "  T_p7_p6 =tf.random_uniform((batch_size_p7_p6 ,1),\n",
        "                            minval=T_l,maxval=T_r, dtype=dtype)\n",
        "  K_p7_p6 =tf.random_uniform((batch_size_p7_p6 ,1),\n",
        "                      minval=K_l,maxval=K_r, dtype=dtype)\n",
        "  xi=tf.reshape(tf.stack([s0,sigma,mu,T,K], axis=2), (batch_size,d))\n",
        "  xi_p1_p0=tf.reshape(tf.stack([s0_p1_p0,sigma_p1_p0,mu_p1_p0,T_p1_p0,K_p1_p0], axis=2), (batch_size_p1_p0,d))\n",
        "  xi_p2_p1=tf.reshape(tf.stack([s0_p2_p1,sigma_p2_p1,mu_p2_p1,T_p2_p1,K_p2_p1], axis=2), (batch_size_p2_p1,d))\n",
        "  xi_p3_p2=tf.reshape(tf.stack([s0_p3_p2,sigma_p3_p2,mu_p3_p2,T_p3_p2,K_p3_p2], axis=2), (batch_size_p3_p2,d))\n",
        "  xi_p4_p3=tf.reshape(tf.stack([s0_p4_p3,sigma_p4_p3,mu_p4_p3,T_p4_p3,K_p4_p3], axis=2), (batch_size_p4_p3,d))\n",
        "  xi_p5_p4=tf.reshape(tf.stack([s0_p5_p4,sigma_p5_p4,mu_p5_p4,T_p5_p4,K_p5_p4], axis=2), (batch_size_p5_p4,d))\n",
        "  xi_p6_p5=tf.reshape(tf.stack([s0_p6_p5,sigma_p6_p5,mu_p6_p5,T_p6_p5,K_p6_p5], axis=2), (batch_size_p6_p5,d))\n",
        "  xi_p7_p6=tf.reshape(tf.stack([s0_p7_p6,sigma_p7_p6,mu_p7_p6,T_p7_p6,K_p7_p6], axis=2), (batch_size_p7_p6,d))\n",
        "  s0_approx = tf.random_uniform((batch_size_approx,  1 ), \n",
        "                            minval=s_0_l_approx,maxval=s_0_r_approx, dtype=dtype)\n",
        "  sigma_approx=tf.random_uniform((batch_size_approx,  1 ), \n",
        "                            minval=sigma_l_approx,maxval=sigma_r_approx, dtype=dtype)\n",
        "  mu_approx=tf.random_uniform((batch_size_approx,1),\n",
        "                            minval=mu_l_approx,maxval=mu_r_approx, dtype=dtype)\n",
        "  T_approx=tf.random_uniform((batch_size_approx,1),\n",
        "                            minval=T_l_approx,maxval=T_r_approx, dtype=dtype)\n",
        "  K_approx=tf.random_uniform((batch_size_approx,1),\n",
        "                            minval=K_l_approx,maxval=K_r_approx, dtype=dtype)\n",
        "  xi_approx=tf.reshape(tf.stack([s0_approx,sigma_approx,mu_approx,T_approx,K_approx], axis=2), (batch_size_approx,d))\n",
        "  #Closed solution as reference\n",
        "  tfd = tfp.distributions\n",
        "  dist = tfd.Normal(loc=tf.cast(0.,tf.float32), scale=tf.cast(1.,tf.float32))\n",
        "  d1=tf.math.divide(\n",
        "  (tf.log(tf.math.divide(s0_approx,K_approx))+(mu_approx + 0.5*sigma_approx**2)*T_approx) , (sigma_approx*tf.sqrt(T_approx)))\n",
        "  d2=tf.math.divide(\n",
        "  (tf.log(tf.math.divide(s0_approx,K_approx))+(mu_approx - 0.5*sigma_approx**2)*T_approx) , (sigma_approx*tf.sqrt(T_approx)))\n",
        "  u_reference= tf.multiply(s0_approx,(dist.cdf(d1)))-K_approx*tf.exp(-mu_approx*T_approx)*(dist.cdf(d2))\n",
        "\n",
        " #Multilevel Monte Carlo path simulation\n",
        " def sde_body_p1(idx, s,sigma,mu,T,K, samples): \n",
        "    z = tf.random_normal(shape=(samples, batch_size_p1_p0, 1),\n",
        "                         stddev=1., dtype=dtype)\n",
        "    s=s + mu *s * h_p1_p0 +sigma * s *tf.sqrt(h_p1_p0)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(h_p1_p0)*z)**2-h_p1_p0)  \n",
        "    return tf.add(idx, 1), s, sigma,mu,T,K\n",
        " def mc_body_p1(idx, p):\n",
        "    _, _x, sigma,mu,T,K = tf.while_loop(lambda _idx, s, sigma,mu,T,K: _idx < N,\n",
        "                          lambda _idx, s, sigma,mu,T,K: sde_body_p1(_idx, s,sigma,mu,T,K,\n",
        "                                                   mc_samples_ref),\n",
        "                                                   loop_var_mc_p1)\n",
        "\n",
        "    return idx + 1, p + tf.reduce_mean(phi(_x,sigma,mu,T,K, 2), axis=0)\n",
        " def sde_body_p1_p0(idx, s, sfine,sigma,mu,T,K, samples): \n",
        "    z1 = tf.random_normal(shape=(samples, batch_size_p1_p0, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z2 = tf.random_normal(shape=(samples, batch_size_p1_p0, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z=(z1+z2)/tf.sqrt(2.)\n",
        "    h_p1_p0_coarse= T / (N_p1_p0)\n",
        "    h_p1_p0_fine= T / (N_p1_p0*2)\n",
        "    hfine=h_p1_p0_fine\n",
        "    hcoarse=h_p1_p0_coarse\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z1 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z1)**2-hfine)\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z2 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z2)**2-hfine)\n",
        "    s=s + mu *s * hcoarse +sigma * s *tf.sqrt(hcoarse)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(hcoarse)*z)**2-hcoarse)    \n",
        "    return tf.add(idx, 1), s, sfine, sigma,mu,T,K   \n",
        " def mc_body_p1_p0(idx, p):\n",
        "    _, _xcoarse, _xfine, sigma,mu,T,K = tf.while_loop(lambda _idx, s, xfine, sigma,mu,T,K: _idx < N_p1_p0,\n",
        "                          lambda _idx, s, xfine, sigma,mu,T,K: sde_body_p1_p0(_idx, s, xfine,sigma,mu,T,K,\n",
        "                                                   mc_samples_ref),\n",
        "                                                   loop_var_mc_p1_p0)\n",
        "    return idx + 1, p + tf.reduce_mean(phi(_xfine,sigma,mu,T,K, 2)-phi(_xcoarse,sigma,mu,T,K, 2), axis=0) \n",
        " def sde_body_p2_p1(idx, s, sfine,sigma,mu,T,K, samples): \n",
        "    z1 = tf.random_normal(shape=(samples, batch_size_p2_p1, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z2 = tf.random_normal(shape=(samples, batch_size_p2_p1, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z=(z1+z2)/tf.sqrt(2.)\n",
        "    h_p2_p1_coarse=T / (N_p2_p1)\n",
        "    h_p2_p1_fine=T / (N_p2_p1*2)\n",
        "    hfine=h_p2_p1_fine\n",
        "    hcoarse=h_p2_p1_coarse\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z1 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z1)**2-hfine)\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z2 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z2)**2-hfine)\n",
        "    s=s + mu *s * hcoarse +sigma * s *tf.sqrt(hcoarse)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(hcoarse)*z)**2-hcoarse)    \n",
        "    return tf.add(idx, 1), s, sfine , sigma,mu,T,K\n",
        " def mc_body_p2_p1(idx, p):\n",
        "    _, _xcoarse, _xfine, sigma,mu,T,K = tf.while_loop(lambda _idx, s, xfine, sigma,mu,T,K: _idx < N_p2_p1,\n",
        "                          lambda _idx, s, xfine, sigma,mu,T,K: sde_body_p2_p1(_idx, s, xfine,sigma,mu,T,K,\n",
        "                                                   mc_samples_ref),\n",
        "                                                   loop_var_mc_p2_p1)\n",
        "    return idx + 1, p + tf.reduce_mean(phi(_xfine,sigma,mu,T,K, 2), axis=0) -tf.reduce_mean(phi(_xcoarse,sigma,mu,T,K, 2), axis=0) \n",
        " def sde_body_p3_p2(idx, s, sfine,sigma,mu,T,K, samples): \n",
        "    z1 = tf.random_normal(shape=(samples, batch_size_p3_p2, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z2 = tf.random_normal(shape=(samples, batch_size_p3_p2, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z=(z1+z2)/tf.sqrt(2.)\n",
        "    h_p3_p2_coarse=T / (N_p3_p2)\n",
        "    h_p3_p2_fine=T / (N_p3_p2*2)\n",
        "    hfine=h_p3_p2_fine\n",
        "    hcoarse=h_p3_p2_coarse\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z1 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z1)**2-hfine)\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z2 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z2)**2-hfine)\n",
        "    s=s + mu *s * hcoarse +sigma * s *tf.sqrt(hcoarse)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(hcoarse)*z)**2-hcoarse)    \n",
        "    return tf.add(idx, 1), s, sfine  , sigma,mu,T,K\n",
        " def sde_body_p4_p3(idx, s, sfine,sigma,mu,T,K, samples): \n",
        "    z1 = tf.random_normal(shape=(samples, batch_size_p4_p3, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z2 = tf.random_normal(shape=(samples, batch_size_p4_p3, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z=(z1+z2)/tf.sqrt(2.)\n",
        "    h_p4_p3_coarse=T / (N_p4_p3)\n",
        "    h_p4_p3_fine=T / (N_p4_p3*2)\n",
        "    hfine=h_p4_p3_fine\n",
        "    hcoarse=h_p4_p3_coarse\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z1 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z1)**2-hfine)\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z2 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z2)**2-hfine)\n",
        "    s=s + mu *s * hcoarse +sigma * s *tf.sqrt(hcoarse)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(hcoarse)*z)**2-hcoarse)    \n",
        "    return tf.add(idx, 1), s, sfine  , sigma,mu,T,K\n",
        " def sde_body_p5_p4(idx, s, sfine,sigma,mu,T,K, samples):\n",
        "    z1 = tf.random_normal(shape=(samples, batch_size_p5_p4, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z2 = tf.random_normal(shape=(samples, batch_size_p5_p4, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z=(z1+z2)/tf.sqrt(2.)\n",
        "    h_p5_p4_coarse=T / (N_p5_p4)\n",
        "    h_p5_p4_fine=T / (N_p5_p4*2)\n",
        "    hfine=h_p5_p4_fine\n",
        "    hcoarse=h_p5_p4_coarse\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z1 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z1)**2-hfine)\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z2 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z2)**2-hfine)\n",
        "    s=s + mu *s * hcoarse +sigma * s *tf.sqrt(hcoarse)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(hcoarse)*z)**2-hcoarse)    \n",
        "    return tf.add(idx, 1), s, sfine  , sigma,mu,T,K\n",
        " def sde_body_p6_p5(idx, s, sfine,sigma,mu,T,K, samples): \n",
        "    z1 = tf.random_normal(shape=(samples, batch_size_p6_p5, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z2 = tf.random_normal(shape=(samples, batch_size_p6_p5, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z=(z1+z2)/tf.sqrt(2.)\n",
        "    h_p6_p5_coarse=T / (N_p6_p5)\n",
        "    h_p6_p5_fine=T / (N_p6_p5*2)\n",
        "    hfine=h_p6_p5_fine\n",
        "    hcoarse=h_p6_p5_coarse\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z1 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z1)**2-hfine)\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z2 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z2)**2-hfine)\n",
        "    s=s + mu *s * hcoarse +sigma * s *tf.sqrt(hcoarse)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(hcoarse)*z)**2-hcoarse)    \n",
        "    return tf.add(idx, 1), s, sfine  , sigma,mu,T,K\n",
        " def sde_body_p7_p6(idx, s, sfine,sigma,mu,T,K, samples): \n",
        "    z1 = tf.random_normal(shape=(samples, batch_size_p7_p6, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z2 = tf.random_normal(shape=(samples, batch_size_p7_p6, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z=(z1+z2)/tf.sqrt(2.)\n",
        "    h_p7_p6_coarse=T / (N_p7_p6)\n",
        "    h_p7_p6_fine=T / (N_p7_p6*2)\n",
        "    hfine=h_p7_p6_fine\n",
        "    hcoarse=h_p7_p6_coarse\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z1 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z1)**2-hfine)\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z2 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z2)**2-hfine)\n",
        "    s=s + mu *s * hcoarse +sigma * s *tf.sqrt(hcoarse)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(hcoarse)*z)**2-hcoarse)    \n",
        "    return tf.add(idx, 1), s, sfine  , sigma,mu,T,K\n",
        " def mc_body_p3_p2(idx, p):\n",
        "    _, _xcoarse, _xfine, sigma,mu,T,K = tf.while_loop(lambda _idx, s, xfine, sigma,mu,T,K: _idx < N_p3_p2,\n",
        "                          lambda _idx, s, xfine, sigma,mu,T,K: sde_body_p3_p2(_idx, s, xfine,sigma,mu,T,K,\n",
        "                                                   mc_samples_ref),\n",
        "                                                   loop_var_mc_p3_p2)\n",
        "    return idx + 1, p + tf.reduce_mean(phi(_xfine,sigma,mu,T,K, 2), axis=0) -tf.reduce_mean(phi(_xcoarse,sigma,mu,T,K, 2), axis=0)  \n",
        " def mc_body_p4_p3(idx, p):\n",
        "    _, _xcoarse, _xfine, sigma,mu,T,K = tf.while_loop(lambda _idx, s, xfine, sigma,mu,T,K: _idx < N_p4_p3,\n",
        "                          lambda _idx, s, xfine, sigma,mu,T,K: sde_body_p4_p3(_idx, s, xfine,sigma,mu,T,K,\n",
        "                                                   mc_samples_ref),\n",
        "                                                   loop_var_mc_p4_p3)\n",
        "    return idx + 1, p + tf.reduce_mean(phi(_xfine,sigma,mu,T,K, 2), axis=0) -tf.reduce_mean(phi(_xcoarse,sigma,mu,T,K, 2), axis=0) \n",
        " def mc_body_p5_p4(idx, p):\n",
        "    _, _xcoarse, _xfine, sigma,mu,T,K = tf.while_loop(lambda _idx, s, xfine, sigma,mu,T,K: _idx < N_p5_p4,\n",
        "                          lambda _idx, s, xfine, sigma,mu,T,K: sde_body_p5_p4(_idx, s, xfine,sigma,mu,T,K,\n",
        "                                                   mc_samples_ref),\n",
        "                                                   loop_var_mc_p5_p4)\n",
        "    return idx + 1, p + tf.reduce_mean(phi(_xfine,sigma,mu,T,K, 2), axis=0) -tf.reduce_mean(phi(_xcoarse,sigma,mu,T,K, 2), axis=0) \n",
        " def mc_body_p6_p5(idx, p):\n",
        "    _, _xcoarse, _xfine, sigma,mu,T,K = tf.while_loop(lambda _idx, s, xfine, sigma,mu,T,K: _idx < N_p6_p5,\n",
        "                          lambda _idx, s, xfine, sigma,mu,T,K: sde_body_p6_p5(_idx, s, xfine,sigma,mu,T,K,\n",
        "                                                   mc_samples_ref),\n",
        "                                                   loop_var_mc_p6_p5)\n",
        "    return idx + 1, p + tf.reduce_mean(phi(_xfine,sigma,mu,T,K, 2), axis=0) -tf.reduce_mean(phi(_xcoarse,sigma,mu,T,K, 2), axis=0) \n",
        " def mc_body_p7_p6(idx, p):\n",
        "    _, _xcoarse, _xfine, sigma,mu,T,K = tf.while_loop(lambda _idx, s, xfine, sigma,mu,T,K: _idx < N_p7_p6,\n",
        "                          lambda _idx, s, xfine, sigma,mu,T,K: sde_body_p7_p6(_idx, s, xfine,sigma,mu,T,K,\n",
        "                                                   mc_samples_ref),\n",
        "                                                   loop_var_mc_p7_p6)\n",
        "    return idx + 1, p + tf.reduce_mean(phi(_xfine,sigma,mu,T,K, 2), axis=0) -tf.reduce_mean(phi(_xcoarse,sigma,mu,T,K, 2), axis=0) \n",
        " def mc_body_p8_p7(idx, p):\n",
        "    _, _xcoarse, _xfine, sigma,mu,T,K = tf.while_loop(lambda _idx, s, xfine, sigma,mu,T,K: _idx < N_p8_p7,\n",
        "                          lambda _idx, s, xfine, sigma,mu,T,K: sde_body_p8_p7(_idx, s, xfine,sigma,mu,T,K,\n",
        "                                                   mc_samples_ref),\n",
        "                                                   loop_var_mc_p8_p7)\n",
        "    return idx + 1, p + tf.reduce_mean(phi(_xfine,sigma,mu,T,K, 2), axis=0) -tf.reduce_mean(phi(_xcoarse,sigma,mu,T,K, 2), axis=0) \n",
        " def mc_body_ref_p0(idx2, p):\n",
        "    _, _xcoarse, _xfine, sigma,mu,T,K = tf.while_loop(lambda _idx, s, xfine, sigma,mu,T,K: _idx < N_p0,\n",
        "                          lambda _idx, s, xfine, sigma,mu,T,K: sde_body_ref_p0(_idx, s, xfine,sigma,mu,T,K,\n",
        "                                                   mc_samples_ref_p0),\n",
        "                                                   loop_var_mc_ref_p0)\n",
        "    return idx2 + 1, p + tf.reduce_mean(phi(_xcoarse,sigma,mu,T,K, 2), axis=0) \n",
        " def sde_body_ref_p0(idx, s, sfine,sigma,mu,T,K, samples): \n",
        "    z = tf.random_normal(shape=(samples, batch_size_approx, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "\n",
        "    hcoarse=h_p1_p0_coarse\n",
        "    s=s + mu *s * hcoarse +sigma * s *tf.sqrt(hcoarse)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(hcoarse)*z)**2-hcoarse)    \n",
        "    return tf.add(idx, 1), s, sfine , sigma,mu,T,K \n",
        " def mc_body_ref_p1_p0(idx, p):\n",
        "    _, _xcoarse, _xfine, sigma,mu,T,K = tf.while_loop(lambda _idx, s, xfine, sigma,mu,T,K: _idx < N_p1_p0,\n",
        "                          lambda _idx, s, xfine, sigma,mu,T,K: sde_body_ref_p1_p0(_idx, s, xfine,sigma,mu,T,K,\n",
        "                                                   mc_samples_ref_p1_p0),\n",
        "                                                   loop_var_mc_ref_p1_p0)\n",
        "    return idx + 1, p + tf.reduce_mean(phi(_xfine,sigma,mu,T,K, 2)-phi(_xcoarse,sigma,mu,T,K, 2), axis=0) \n",
        " def sde_body_ref_p1_p0(idx, s, sfine,sigma,mu,T,K, samples): \n",
        "    z1 = tf.random_normal(shape=(samples, batch_size_approx, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z2 = tf.random_normal(shape=(samples, batch_size_approx, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z=(z1+z2)/tf.sqrt(2.)\n",
        "    hfine=h_p1_p0_fine\n",
        "    hcoarse=h_p1_p0_coarse\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z1 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z1)**2-hfine)\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z2 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z2)**2-hfine)\n",
        "    s=s + mu *s * hcoarse +sigma * s *tf.sqrt(hcoarse)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(hcoarse)*z)**2-hcoarse)    \n",
        "    return tf.add(idx, 1), s, sfine , sigma,mu,T,K     \n",
        " def mc_body_ref_p2_p1(idx, p):\n",
        "    _, _xcoarse, _xfine, sigma,mu,T,K = tf.while_loop(lambda _idx, s, xfine, sigma,mu,T,K: _idx < N_p2_p1,\n",
        "                          lambda _idx, s, xfine, sigma,mu,T,K: sde_body_ref_p2_p1(_idx, s, xfine,sigma,mu,T,K,\n",
        "                                                   mc_samples_ref_p2_p1),\n",
        "                                                   loop_var_mc_ref_p2_p1)\n",
        "    return idx + 1, p + tf.reduce_mean(phi(_xfine, sigma,mu,T,K,2)-phi(_xcoarse,sigma,mu,T,K, 2), axis=0) \n",
        " def sde_body_ref_p2_p1(idx, s, sfine,sigma,mu,T,K, samples): \n",
        "    z1 = tf.random_normal(shape=(samples, batch_size_approx, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z2 = tf.random_normal(shape=(samples, batch_size_approx, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z=(z1+z2)/tf.sqrt(2.)\n",
        "    hfine=h_p2_p1_fine\n",
        "    hcoarse=h_p2_p1_coarse\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z1 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z1)**2-hfine)\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z2 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z2)**2-hfine)\n",
        "    s=s + mu *s * hcoarse +sigma * s *tf.sqrt(hcoarse)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(hcoarse)*z)**2-hcoarse)    \n",
        "    return tf.add(idx, 1), s, sfine  , sigma,mu,T,K                           \n",
        " def mc_body_ref_p3_p2(idx, p):\n",
        "    _, _xcoarse, _xfine, sigma,mu,T,K = tf.while_loop(lambda _idx, s, xfine, sigma,mu,T,K: _idx < N_p3_p2,\n",
        "                          lambda _idx, s, xfine, sigma,mu,T,K: sde_body_ref_p3_p2(_idx, s, xfine,sigma,mu,T,K,\n",
        "                                                   mc_samples_ref_p3_p2),\n",
        "                                                   loop_var_mc_ref_p3_p2)\n",
        "    return idx + 1, p + tf.reduce_mean(phi(_xfine,sigma,mu,T,K, 2)-phi(_xcoarse,sigma,mu,T,K, 2), axis=0) \n",
        " def sde_body_ref_p3_p2(idx, s, sfine,sigma,mu,T,K, samples):\n",
        "    z1 = tf.random_normal(shape=(samples, batch_size_approx, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z2 = tf.random_normal(shape=(samples, batch_size_approx, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z=(z1+z2)/tf.sqrt(2.)\n",
        "    hfine=h_p3_p2_fine\n",
        "    hcoarse=h_p3_p2_coarse\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z1 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z1)**2-hfine)\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z2 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z2)**2-hfine)\n",
        "    s=s + mu *s * hcoarse +sigma * s *tf.sqrt(hcoarse)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(hcoarse)*z)**2-hcoarse)    \n",
        "    return tf.add(idx, 1), s, sfine  , sigma,mu,T,K                           \n",
        " def mc_body_ref_p4_p3(idx, p):\n",
        "    _, _xcoarse, _xfine, sigma,mu,T,K = tf.while_loop(lambda _idx, s, xfine, sigma,mu,T,K: _idx < N_p4_p3,\n",
        "                          lambda _idx, s, xfine, sigma,mu,T,K: sde_body_ref_p4_p3(_idx, s, xfine,sigma,mu,T,K,\n",
        "                                                   mc_samples_ref_p4_p3),\n",
        "                                                   loop_var_mc_ref_p4_p3)\n",
        "    return idx + 1, p + tf.reduce_mean(phi(_xfine,sigma,mu,T,K, 2)-phi(_xcoarse,sigma,mu,T,K, 2), axis=0) \n",
        " def sde_body_ref_p4_p3(idx, s, sfine,sigma,mu,T,K, samples):\n",
        "    z1 = tf.random_normal(shape=(samples, batch_size_approx, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z2 = tf.random_normal(shape=(samples, batch_size_approx, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z=(z1+z2)/tf.sqrt(2.)\n",
        "    hfine=h_p4_p3_fine\n",
        "    hcoarse=h_p4_p3_coarse\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z1 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z1)**2-hfine)\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z2 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z2)**2-hfine)\n",
        "    s=s + mu *s * hcoarse +sigma * s *tf.sqrt(hcoarse)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(hcoarse)*z)**2-hcoarse)    \n",
        "    return tf.add(idx, 1), s, sfine  , sigma,mu,T,K                           \n",
        " def mc_body_ref_p5_p4(idx, p):\n",
        "    _, _xcoarse, _xfine, sigma,mu,T,K = tf.while_loop(lambda _idx, s, xfine, sigma,mu,T,K: _idx < N_p5_p4,\n",
        "                          lambda _idx, s, xfine, sigma,mu,T,K: sde_body_ref_p5_p4(_idx, s, xfine,sigma,mu,T,K,\n",
        "                                                   mc_samples_ref_p5_p4),\n",
        "                                                   loop_var_mc_ref_p5_p4)\n",
        "    return idx + 1, p + tf.reduce_mean(phi(_xfine,sigma,mu,T,K, 2)-phi(_xcoarse,sigma,mu,T,K, 2), axis=0) \n",
        " def sde_body_ref_p5_p4(idx, s, sfine,sigma,mu,T,K, samples): \n",
        "    z1 = tf.random_normal(shape=(samples, batch_size_approx, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z2 = tf.random_normal(shape=(samples, batch_size_approx, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z=(z1+z2)/tf.sqrt(2.)\n",
        "    hfine=h_p5_p4_fine\n",
        "    hcoarse=h_p5_p4_coarse\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z1 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z1)**2-hfine)\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z2 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z2)**2-hfine)\n",
        "    s=s + mu *s * hcoarse +sigma * s *tf.sqrt(hcoarse)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(hcoarse)*z)**2-hcoarse)    \n",
        "    return tf.add(idx, 1), s, sfine    , sigma,mu,T,K                        \n",
        " def mc_body_ref_p6_p5(idx, p):\n",
        "    _, _xcoarse, _xfine, sigma,mu,T,K  = tf.while_loop(lambda _idx, s, xfine, sigma,mu,T,K: _idx < N_p6_p5,\n",
        "                          lambda _idx, s, xfine, sigma,mu,T,K: sde_body_ref_p6_p5(_idx, s, xfine,sigma,mu,T,K,\n",
        "                                                   mc_samples_ref_p6_p5),\n",
        "                                                   loop_var_mc_ref_p6_p5)\n",
        "    return idx + 1, p + tf.reduce_mean(phi(_xfine,sigma,mu,T,K, 2)-phi(_xcoarse,sigma,mu,T,K, 2), axis=0) \n",
        " def sde_body_ref_p6_p5(idx, s, sfine,sigma,mu,T,K, samples): \n",
        "    z1 = tf.random_normal(shape=(samples, batch_size_approx, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z2 = tf.random_normal(shape=(samples, batch_size_approx, 1),\n",
        "                          stddev=1., dtype=dtype)\n",
        "    z=(z1+z2)/tf.sqrt(2.)\n",
        "    hfine=h_p6_p5_fine\n",
        "    hcoarse=h_p6_p5_coarse\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z1 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z1)**2-hfine)\n",
        "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z2 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z2)**2-hfine)\n",
        "    s=s + mu *s * hcoarse +sigma * s *tf.sqrt(hcoarse)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(hcoarse)*z)**2-hcoarse)    \n",
        "    return tf.add(idx, 1), s, sfine  , sigma,mu,T,K    \n",
        " def phi(x,sigma,mu,T,K, axis=1):\n",
        "    payoff=tf.exp(-mu * T)* tf.maximum(x - K, 0.)\n",
        "    return payoff\n",
        " def sde_body_p0(idx, s,sigma,mu,T,K, samples): \n",
        "    z = tf.random_normal(shape=(samples, batch_size, 1),\n",
        "                         stddev=1., dtype=dtype)\n",
        "    h=T/N\n",
        "    s=s + mu *s * h +sigma * s *tf.sqrt(h)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(h)*z)**2-h)\n",
        "    return tf.add(idx, 1), s, sigma,mu,T,K\n",
        " def mc_body_p0(idx, p):\n",
        "    _, _x, _sigma,_mu,_T,_K = tf.while_loop(lambda _idx, s, sigma,mu,T,K: _idx < N,\n",
        "                          lambda _idx, s, sigma,mu,T,K: sde_body_p0(_idx, s, sigma,mu,T,K,\n",
        "                                                   mc_samples_ref),\n",
        "                                                   loop_var_mc_p0)\n",
        "    return idx + 1, p + tf.reduce_mean(phi(_x,_sigma,_mu,_T,_K, 2), axis=0)  \n",
        " loop_var_mc_p0 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size, 1), dtype) * s0, tf.ones((mc_samples_ref,batch_size, 1), dtype) * sigma,tf.ones((mc_samples_ref,batch_size, 1), dtype) * mu,tf.ones((mc_samples_ref,batch_size, 1), dtype) * T,tf.ones((mc_samples_ref,batch_size, 1), dtype) * K)\n",
        " loop_var_mc_p1_p0 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size_p1_p0, 1), dtype) * s0_p1_p0,tf.ones((mc_samples_ref,batch_size_p1_p0, 1), dtype) * s0_p1_p0, tf.ones((mc_samples_ref,batch_size_p1_p0, 1), dtype) * sigma_p1_p0,tf.ones((mc_samples_ref,batch_size_p1_p0, 1), dtype) * mu_p1_p0,tf.ones((mc_samples_ref,batch_size_p1_p0, 1), dtype) * T_p1_p0,tf.ones((mc_samples_ref,batch_size_p1_p0, 1), dtype) * K_p1_p0)\n",
        " loop_var_mc_p2_p1 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size_p2_p1, 1), dtype) * s0_p2_p1,tf.ones((mc_samples_ref,batch_size_p2_p1, 1), dtype) * s0_p2_p1, tf.ones((mc_samples_ref,batch_size_p2_p1, 1), dtype) * sigma_p2_p1,tf.ones((mc_samples_ref,batch_size_p2_p1, 1), dtype) * mu_p2_p1,tf.ones((mc_samples_ref,batch_size_p2_p1, 1), dtype) * T_p2_p1,tf.ones((mc_samples_ref,batch_size_p2_p1, 1), dtype) * K_p2_p1)\n",
        " loop_var_mc_p3_p2 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size_p3_p2, 1), dtype) * s0_p3_p2,tf.ones((mc_samples_ref,batch_size_p3_p2, 1), dtype) * s0_p3_p2, tf.ones((mc_samples_ref,batch_size_p3_p2, 1), dtype) * sigma_p3_p2,tf.ones((mc_samples_ref,batch_size_p3_p2, 1), dtype) * mu_p3_p2,tf.ones((mc_samples_ref,batch_size_p3_p2, 1), dtype) * T_p3_p2,tf.ones((mc_samples_ref,batch_size_p3_p2, 1), dtype) * K_p3_p2)\n",
        " loop_var_mc_p4_p3 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size_p4_p3, 1), dtype) * s0_p4_p3,tf.ones((mc_samples_ref,batch_size_p4_p3, 1), dtype) * s0_p4_p3, tf.ones((mc_samples_ref,batch_size_p4_p3, 1), dtype) * sigma_p4_p3,tf.ones((mc_samples_ref,batch_size_p4_p3, 1), dtype) * mu_p4_p3,tf.ones((mc_samples_ref,batch_size_p4_p3, 1), dtype) * T_p4_p3,tf.ones((mc_samples_ref,batch_size_p4_p3, 1), dtype) * K_p4_p3)\n",
        " loop_var_mc_p5_p4 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size_p5_p4, 1), dtype) * s0_p5_p4,tf.ones((mc_samples_ref,batch_size_p5_p4, 1), dtype) * s0_p5_p4, tf.ones((mc_samples_ref,batch_size_p5_p4, 1), dtype) * sigma_p5_p4,tf.ones((mc_samples_ref,batch_size_p5_p4, 1), dtype) * mu_p5_p4,tf.ones((mc_samples_ref,batch_size_p5_p4, 1), dtype) * T_p5_p4,tf.ones((mc_samples_ref,batch_size_p5_p4, 1), dtype) * K_p5_p4)\n",
        " loop_var_mc_p6_p5 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size_p6_p5, 1), dtype) * s0_p6_p5,tf.ones((mc_samples_ref,batch_size_p6_p5, 1), dtype) * s0_p6_p5, tf.ones((mc_samples_ref,batch_size_p6_p5, 1), dtype) * sigma_p6_p5,tf.ones((mc_samples_ref,batch_size_p6_p5, 1), dtype) * mu_p6_p5,tf.ones((mc_samples_ref,batch_size_p6_p5, 1), dtype) * T_p6_p5,tf.ones((mc_samples_ref,batch_size_p6_p5, 1), dtype) * K_p6_p5)\n",
        " loop_var_mc_p7_p6 = (tf.constant(0),tf.ones((mc_samples_ref,batch_size_p7_p6, 1), dtype) * s0_p7_p6,tf.ones((mc_samples_ref,batch_size_p7_p6, 1), dtype) * s0_p7_p6, tf.ones((mc_samples_ref,batch_size_p7_p6, 1), dtype) * sigma_p7_p6,tf.ones((mc_samples_ref,batch_size_p7_p6, 1), dtype) * mu_p7_p6,tf.ones((mc_samples_ref,batch_size_p7_p6, 1), dtype) * T_p7_p6,tf.ones((mc_samples_ref,batch_size_p7_p6, 1), dtype) * K_p7_p6)\n",
        " _, x_sde,sigma,mu,T,K = tf.while_loop(lambda idx, s, sigma,mu,T,K: idx < N,\n",
        "                         lambda idx, s, sigma,mu,T,K: sde_body_p0(idx, s,sigma,mu,T,K, 1),\n",
        "                         loop_var_mc_p0)\n",
        " _, u_p0  = tf.while_loop(lambda idx, p: idx < 1, mc_body_p0,\n",
        "                      (tf.constant(0), tf.zeros((batch_size, 1), dtype)))\n",
        " u_mc_p0 = u_p0 / tf.cast(1, tf.float32)\n",
        " _, u_p1_p0 = tf.while_loop(lambda idx, p: idx < 1, mc_body_p1_p0,\n",
        "                      (tf.constant(0), tf.zeros((batch_size_p1_p0, 1), dtype)))\n",
        " u_mc_p1_p0 = u_p1_p0 / tf.cast(1, tf.float32)\n",
        " _, u_p2_p1 = tf.while_loop(lambda idx, p: idx < 1, mc_body_p2_p1,\n",
        "                      (tf.constant(0), tf.zeros((batch_size_p2_p1, 1), dtype)))\n",
        " u_mc_p2_p1 = u_p2_p1 / tf.cast(1, tf.float32)\n",
        " _, u_p3_p2 = tf.while_loop(lambda idx, p: idx < 1, mc_body_p3_p2,\n",
        "                      (tf.constant(0), tf.zeros((batch_size_p3_p2, 1), dtype)))\n",
        " u_mc_p3_p2 = u_p3_p2 / tf.cast(1, tf.float32)\n",
        " _, u_p4_p3 = tf.while_loop(lambda idx, p: idx < 1, mc_body_p4_p3,\n",
        "                      (tf.constant(0), tf.zeros((batch_size_p4_p3, 1), dtype)))\n",
        " u_mc_p4_p3 = u_p4_p3 / tf.cast(1, tf.float32)\n",
        " _, u_p5_p4 = tf.while_loop(lambda idx, p: idx < 1, mc_body_p5_p4,\n",
        "                      (tf.constant(0), tf.zeros((batch_size_p5_p4, 1), dtype)))\n",
        " u_mc_p5_p4 = u_p5_p4 / tf.cast(1, tf.float32)\n",
        " _, u_p6_p5 = tf.while_loop(lambda idx, p: idx < 1, mc_body_p6_p5,\n",
        "                      (tf.constant(0), tf.zeros((batch_size_p6_p5, 1), dtype)))\n",
        " u_mc_p6_p5 = u_p6_p5 / tf.cast(1, tf.float32)\n",
        " _, u_p7_p6 = tf.while_loop(lambda idx, p: idx < 1, mc_body_p7_p6,\n",
        "                      (tf.constant(0), tf.zeros((batch_size_p7_p6, 1), dtype)))\n",
        " u_mc_p7_p6 = u_p7_p6 / tf.cast(1, tf.float32)\n",
        " N_approx=1\n",
        " N_approx_ref_p0=1\n",
        " u_reference_p0   =tf.multiply(s0_approx,(dist.cdf(d1)))-K_approx*tf.exp(-mu_approx*T_approx)*(dist.cdf(d2))\n",
        " u_reference_p1_p0=xi_approx*0.\n",
        " u_reference_p2_p1=xi_approx*0.\n",
        " u_reference_p3_p2=xi_approx*0.\n",
        " u_reference_p4_p3=xi_approx*0.\n",
        " u_reference_p5_p4=xi_approx*0.\n",
        " u_reference_p6_p5=xi_approx*0.\n",
        " u_reference_p7_p6=xi_approx*0.\n",
        "\n",
        " #Start training and testing                         \n",
        " train_and_test(xi, xi_approx, xi_p1_p0, xi_p7_p6, xi_p6_p5, xi_p5_p4, xi_p4_p3, xi_p3_p2,xi_p2_p1, tf.squeeze(tf.reduce_mean(x_sde,0,keepdims=True), axis=0), u_mc_p0, u_mc_p7_p6, u_mc_p6_p5, u_mc_p5_p4, u_mc_p4_p3, u_mc_p3_p2, u_mc_p2_p1, u_mc_p1_p0, u_reference, u_reference_p0,u_reference_p7_p6,u_reference_p6_p5,u_reference_p5_p4,u_reference_p4_p3,u_reference_p3_p2,u_reference_p2_p1, u_reference_p1_p0,\n",
        "                          neurons, train_steps,\n",
        "                          mc_rounds, mc_freq, 'multi-individual-trainsteps.csv', dtype)                      "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_probability/python/internal/special_math.py:154: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "15\n"
          ]
        }
      ]
    }
  ]
}